# Table des matières

- [La cosmologie quantique](#La-cosmologie-quantique)
- [Matière noire](#Matière-noire)
- [Energie sombre](#Energie-sombre)
- [L'inflation cosmique](#Linflation-cosmique)
- [Le problème de la hiérarchie](#Le-problème-de-la-hiérarchie)
- [Le problème du lithium cosmique](#Le-problème-du-lithium-cosmique)
- [Le principe cosmologique](#Le-principe-cosmologique)
- [La tension de Hubble](#La-tension-de-Hubble)

# La cosmologie quantique

*Sources*

- [Quantum cosmology: a review](https://arxiv.org/abs/1501.04899) - Bojowald (2015)
- [Loop Quantum Cosmology: A brief review](https://arxiv.org/abs/1612.01236) - Agulo et Singh (2016)
- [Conceptual Problems in Quantum Gravity and Quantum Cosmology](https://www.hindawi.com/journals/isrn/2013/509316/) - Kiefer (2013)

---

# Matière noire

*Sources*

- [A History of Dark Matter](https://arxiv.org/pdf/1605.04909.pdf) - Berton et Hooper (2016)
- [Dark matter and cosmic structure](https://arxiv.org/abs/1210.0544) - Frenk and White (2012)
- [La matière noire, une sombre affaire](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p4.pdf) - Combes (2016)

---

The Swiss-American astronomer Fritz Zwicky is arguably the most famous and widely
cited pioneer in the field of dark matter. In 1933, he studied the redshifts of various galaxy
clusters, as published by Edwin Hubble and Milton Humason in 1931 [162], and noticed
a large scatter in the apparent velocities of eight galaxies within the Coma Cluster, with
differences that exceeded 2000 km/s [346]. The fact that Coma exhibited a large velocity
dispersion with respect to other clusters had already been noticed by Hubble and Humason,
but Zwicky went a step further, applying the virial theorem to the cluster in order to estimate
its mass

Zwicky started by estimating the total mass of Coma to be the product of the number
of observed galaxies, 800, and the average mass of a galaxy, which he took to be 109 solar
masses, as suggested by Hubble. He then adopted an estimate for the physical size of the
system, which he took to be around 106 light-years, in order to determine the potential energy
of the system. From there, he calculated the average kinetic energy and finally a velocity
dispersion. He found that 800 galaxies of 109 solar masses in a sphere of 106 light-years
should exhibit a velocity dispersion of 80 km/s. In contrast, the observed average velocity
dispersion along the line-of-sight was approximately 1000 km/s. From this comparison, he
concluded:
If this would be confirmed, we would get the surprising result that dark matter
is present in much greater amount than luminous matter.

The overall situation was that of a community that was struggling to find a unified
solution to a variety of problems. The dark matter hypothesis was not commonly accepted,
nor was it disregarded. Instead, there was a consensus that more information would be
needed in order to understand these systems.

n the 1960s, Kent Ford developed an image tube spectrograph that Vera Rubin and he
used to perform spectroscopic observations of the Andromeda Galaxy. The observations of
the M31 rotation curve Rubin and Ford published in 1970 [267] represented a step forward in
terms of quality.

t was also in 1970 that the first explicit statements began to appear arguing that ad-
ditional mass was needed in the outer parts of some galaxies, based on comparisons of the
rotation curves predicted from photometry and those measured from 21 cm observations

Morton Roberts was among the first to recognize the implications of the observed flatness
of galactic rotation curves. Together with R. Whitehurst, he published in 1972 a rotation
curve of M31 that extended to 120 arcminutes from its center [335]. In 1973, together with
Arnold Rots, he extended the analysis to M81 and M101, and argued that these spiral
galaxies each exhibited flat rotation curves in their outer parts [260] (see Fig. 3). The
authors’ interpretation of these data was unambiguous:
The three galaxies rotation curves decline slowly, if at all, at large radii,
implying a significant mass density at these large distances. It is unreasonable
to expect the last measured point to refer to the ‘edge’ of the galaxy, and we
must conclude that spiral galaxies must be larger than indicated by the usual
photometric measurements [...]. The present data also require that the mass to
luminosity ratio vary with radius increasing in distance from the center.

In 1978 ( ?) Rubin, Ford and Norbert Thonnard published optical rotation curves for ten high-luminosity spiral galaxies and found that they were flat out to the outermost
measured radius [268]. This work has become one of the most well-known and widely cited
in the literature, despite the fact that the optical measurements did not extend to radii as
large as those probed by radio observations, thus leaving open the possibility that galaxies
may not have dark matter halos, as pointed out, for example, by Agris J. Kalnajs in 1983
(see the discussion at the end of Ref. [150]) and by Stephen Kent in 1986 [175]. Rubin, Ford
and Thonnard themselves acknowledged the credit that was due to the preceding analyses:
Roberts and his collaborators deserve credit for first calling attention to flat
rotation curves. [...] These results take on added importance in conjunction with
the suggestion of Einasto, Kaasik, and Saar (1974) and Ostriker, Peebles and
Yahil (1974) that galaxies contain massive halos extending to large r

By the late 1980s, the hypothesis that the missing mass
consists of one or more yet-unknown subatomic particle species had gained enough support
to become established as the leading paradigm for dark matter.




### L'amas de la balle (Bullet cluster)

The combination of X-ray and gravitational lensing data for the`bullet cluster’
1E0657-56 (a merger of two large clusters) confirms that dark matter is very
different in nature from the bulk of normal, baryonic matter (X-ray emitting gas)

X-ray emitting gas
feels ram pressure
whereas dark matter
(and galaxies) pass
through (at least
relatively) unhindered.

Consistent with weakly
interacting, cold dark
matter (CDM) paradigm

# Energie sombre

*Sources*

- [Dark Energy: A Short Review](https://arxiv.org/abs/1401.0046) - Mortonson et al (2013)
- [Dark Energy Versus Modified Gravity](https://www.annualreviews.org/doi/abs/10.1146/annurev-nucl-102115-044553) - Joyce et al (2016)
- [Dark energy: A brief review](https://link.springer.com/article/10.1007/s11467-013-0300-5) - Li et al (2013)
- [Changement de rythme dans l’expansion de l’Univers : Un premier rôle pour le côté obscur](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p12.pdf) - Palanque Delabrouille (2016)
- [L'histoire de l'expansion de l'Univers dévoilée](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2022/01/refdp202271p22.pdf) - Burtin (2022)
- [Avancées de la recherche Cisaillement gravitationnel et sondage de l’Univers](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2006/01/refdp20061p5.pdf) - Mellier (2006)
- [La constante cosmologique : la plus grande erreur d’Einstein](https://books.openedition.org/cdf/9443?lang=fr) - Combes
- [Listening for the Size of the Universe](https://astro.ucla.edu/~wright/BAO-cosmology.html) - Edward Wright (2014)
- [Astronomy Jargon 101: Baryon Acoustic Oscillations](https://www.universetoday.com/153273/astronomy-jargon-101-baryon-acoustic-oscillations/) - Paul Sutter
- [BOSS Measures the Universe to One-Percent Accuracy](https://newscenter.lbl.gov/2014/01/08/boss-one-percent/) - Berkeley Lab (2014)
- [DESI lance un programme de 5 ans pour mieux comprendre l’univers](https://insidetheperimeter.ca/fr/desi-launches-five-year-quest-to-understand-the-universe/) - PI (2021)
- [La distribution des galaxies, une fenêtre sur l'Univers primordial](https://www.pourlascience.fr/sd/cosmologie/la-distribution-des-galaxies-une-fenetre-sur-l-univers-primordial-11827.php) - Pour la Science (2014)
- [What are baryonic acoustic oscillations?](https://sci.esa.int/web/euclid/-/what-are-baryonic-acoustic-oscillations-) - ESA
- [What the hell are Baryon Acoustic Oscillations?](https://medium.com/starts-with-a-bang/what-the-hell-are-baryon-acoustic-oscillations-cfee6d726538) - Ethan Siegel
- [Dark Energy Spectroscopic Instrument (DESI) Creates Largest 3-D Map of the Cosmos](https://www.ifae.es/news/2022/01/13/dark-energy-spectroscopic-instrument-desi-creates-largest-3-d-map-of-the-cosmos/) - IFAE (2022)
- [Listening to the sound of the universe](https://web.physics.ucsb.edu/~jatila/CMB-sounds/CMB/) - Jatila van der Veen
- [Dark matter in galaxy clusters](https://www.slac.stanford.edu/econf/C070730/talks/allen_080607.pdf) - Steve Allen
- [PROBES OF COSMIC ACCELERATION](https://ned.ipac.caltech.edu/level5/March08/Frieman/Frieman7.html) - Frieman (2008)
- [Probing Dark Energy with Galaxy Clusters](https://www.nasa.gov/mission_pages/chandra/probing-dark-energy-with-galaxy-clusters.html) - NASA (2016)
- [Probing dark energy via galaxy cluster outskirts](https://academic.oup.com/mnras/article/457/3/3266/2588928) - Morandi et Sun (2016)
- [La masse des amas déduite des rayons X](https://media4.obspm.fr/public/ressources_lu/pages_structures/massesx.html) - Observatoire de Paris
- [LES AMAS DE GALAXIES EN LUMIERE VISIBLE ET EN RAYONS X](http://www.astrosurf.com/thizy/rochelle2009/doc/2009oct27E%20Amas%20de%20Galaxies%20(Florence%20Durret).pdf) - Florence Durret (2009)
- [Studies of dark energy with x-ray observatories](https://www.pnas.org/doi/10.1073/pnas.0914905107) - Vikhlinin (2010)
- [Dark energy two decades after: Observables, probes, consistency tests](https://arxiv.org/abs/1709.01091) - Huterer et Chafer (2017)
- [Découverte de l'accélération de l'expansion de l'Univers](https://cosmology.education/decouverte-acceleration-expansion/distance-luminosite/#!) - Lucas Gautheron
- [Cosmology with cosmic shear observations: a review]( https://iopscience.iop.org/article/10.1088/0034-4885/78/8/086901) – Kilbinger (2015)
[Cisaillement cosmique](https://www.college-de-france.fr/agenda/seminaire/amas-de-galaxies-et-grandes-structures-de-univers/cisaillement-cosmique) – Collège de France
- [How Does DARK ENERGY Change the CMB Temperature? | The Integrated Sachs-Wolfe Effect](https://youtu.be/F3XOz1dR9YA) - Chris Pattison
- [Faster walk on the dark side](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/2006/faster.html) - NASA
- [L’effet Sachs-Wolfe intégré ou la traversée des grandes structures](http://public.planck.fr/outils/astrophysique/effet-sachs-wolfe-integre-isw) - Planck HFI
- [RayGal : une nouvelle simulation du cosmos pour percer les secrets de l'énergie noire](https://www.futura-sciences.com/sciences/actualites/cosmologie-raygal-nouvelle-simulation-cosmos-percer-secrets-energie-noire-94917/) - Futura Sciences (2022)
- [Comment Estimer la masse des amas de galaxies?](http://physique.unice.fr/sem6/2017-2018/PagesWeb/PT/Galaxie/les_amas/comment.html) - Observatoire de la côte d'Azur
- [Gravitational Lensing in the Canary Islands](https://astrobites.org/2012/11/25/gravitational-lensing-in-the-canary-islands/) - Astrobites
- [Cosmological constant](http://www.scholarpedia.org/article/Cosmological_constant) - Scholarpedia
- [Ask Ethan: Is Einstein’s Cosmological Constant The Same As Dark Energy?](https://www.forbes.com/sites/startswithabang/2020/12/25/ask-ethan-is-einsteins-cosmological-constant-the-same-as-dark-energy/?sh=33a76ee84636) - Ethan Siegel
- [La plus grosse erreur de toute l’histoire de la physique](https://scienceetonnante.com/2012/05/14/la-plus-grosse-erreur-de-toute-lhistoire-de-la-physique/) - Science Etonnante

---

-	En 1917, alors que les observations astronomiques sont principalement limitées aux étoiles de notre Galaxie, Albert Einstein tente d’appliquer sa toute jeune théorie de la relativité générale à un modèle d’univers qu’il juge statique. Pour contrer les effets de la gravitation, il introduit un nouveau terme dans ses équations : la ***constante cosmologique*** (symbolisée par la lettre grecque Λ). 
-	En 1922, le mathématicien russe Alexandre Friedmann démontre que la solution d’univers statique d’Einstein est aussi stable qu’un stylo posé sur sa pointe, et propose de le remplacer par un modèle d’univers en expansion. Le modèle de Friedman sera corroboré par les observations de la fuite des galaxies par Edwin Hubble à la fin des années 20. Il est désormais à la base du modèle cosmologique standard.
> À partir des années 30, Einstein va rapidement renoncer à son modèle d’univers statique. Il confiera plus tard à son collègue George Gamow que l’introduction de la constante cosmologique dans ses équations était « la plus grosse erreur de sa vie » (biggest blunder).
- La constante cosmologique va gagner une seconde vie à la fin des années 90. À cette époque, la majorité des cosmologistes sont persuadés que l’expansion de l’univers est en train de ralentir. Mais en 1998, deux équipes indépendantes d’astronomes des collaborations High-Z supernova Team et Supernova Cosmology Project découvrent que des supernovae thermonucléaires lointaines (0<z<1) sont moins brillantes que ce qu’elles devraient être dans un univers en cours de décélération (ce à quoi on s'attendrait dans un univers constitué uniquement de matière et de rayonnement). 
- La manière la plus simple d’interpréter ces observations est de considérer que l’univers se trouve dans une période d’expansion accélérée depuis environ 6 milliards d’années, qui fait suite à une période de décélération (justifiée par des supernovae thermonucléaires à z>1 plus brillantes que prévues cette fois).
- Cette découverte a valu le prix Nobel de physique en 2011 à trois chercheurs associés aux deux collaborations : Saul Perlmutter, Brian Schmidt et Adam Riess.
- De nos jours, de nombreuses observations indépendantes vont dans le sens d’une période d’expansion cosmique accélérée dans l’histoire récente de l’univers.
- Selon les données du télescope spatial Planck publiées en 2013, l’énergie sombre représenterait 68,3% de la densité énergétique totale de l’univers.
- On ne connaît pas la nature du phénomène qui cause cette expansion accélérée. 

## Comment modéliser la cause de l’expansion accélérée

- Une manière d’interpréter les observations dans le cadre du modèle cosmologique standard est d’invoquer l’existence d’une mystérieuse ***énergie sombre*** qui remplirait uniformément l’univers et qui serait dotée d’une « pression négative » qui la ferait se comporter comme une force répulsive.
> On modélise l’énergie sombre comme un fluide parfait caractérisé par son équation d’état, qui relie sa pression à sa densité d’énergie : P = wρ. (w est un nombre sans dimension)
- La modélisation la plus simple de cette hypothétique énergie sombre fait intervenir la fameuse constante cosmologique, qui pourrait être assimilée de façon élégante à une sorte d’énergie du vide dont la densité reste constante à mesure que l’univers s’étend.
> Si l’énergie sombre est une constante cosmologique, w = -1.
- Dans le langage de la théorie quantique des champs, il est possible d’assimiler cette énergie noire à l’***énergie du vide*** (vacuum energy), qui émerge à cause du ***principe d’incertitude d’Heisenberg***.
> La physique quantique ne permet pas l’existence d’états dans le vide où l’énergie est strictement nulle. L’énergie du vide est l’énergie de l’état fondamental des champs quantiques dans un espace vide de matière. Cet état est constamment fluctuant, et permet l’émergence et l’annihilation permanente de paires de particules virtuelles dans le vide quantique.
- Le problème, c’est que lorsqu’on mesure la densité de l’énergie sombre aux échelles cosmologiques, et qu’on calcule la densité de l’énergie du vide à l’aide des équations de la théorie quantique des champs, les deux valeurs sont incroyablement différentes : la densité du vide quantique excède celle de l’énergie sombre d’un facteur 10 puissance 120 ! Ce problème n’est pas encore résolu à ce jour.
> Certains appellent ce résultat « la plus grosse erreur de toute l’histoire de la physique ».
- La constante cosmologique n’est qu’une des modélisations possibles de l’énergie sombre. De nombreuses théories de gravitation quantique produisent ainsi des candidats à l’énergie sombre qui possèdent d’autres propriétés (par exemple différentes valeurs de w).
- Une autre forme hypothétique que pourrait prendre l’énergie sombre est la ***quintessence***, un champ scalaire dont la contribution varierait dans le temps, contrairement à une constante cosmologique (ie, son équation d’état évolue au fil du temps).
> Une forme particulière de quintessence est ***l’énergie fantôme***, proposé par le physicien Robert Caldwell et dont le paramètre w < -1. 
- Toutes ces propositions d’énergie sombre partent du principe que la relativité générale est la bonne théorie pour décrire l’évolution à grande échelle du cosmos. Mais il existe aussi l’option que la relativité générale doit être révisée aux échelles les plus vastes. C’est l’approche dite de ***gravité modifiée*** (ou de ***gravité sombre*** - dark gravity).
- Les modèles de gravité modifiée produisent des scénarios d’évolution des grandes structures cosmiques différents de ceux du modèle standard de la cosmologie.
- Malheureusement, il n’existe à ce jour aucun modèle de gravité modifiée qui permette d’expliquer l’accélération de l’expansion cosmique observée.

## Comment sonder les propriétés de l'énergie noire à l'aide de nos observations

- Il y a essentiellement deux classes de méthodes pour étudier les signatures observables de l'énergie sombre : les méthodes dites "géométriques", qui se concentrent sur le taux d'expansion de l'univers (à l'aide de règles ou de chandelles standards), et les méthodes se focalisant sur la croissance des grandes structures cosmiques.

### Les anisotropies du CMB

-	Le CMB a été émis à une époque où la contribution de l’énergie sombre dans l’évolution cosmique était négligeable.
-	Par contre, l’analyse statistique des anisotropies de température du CMB (à travers son spectre de puissance) permet de contraindre les paramètres du modèle standard de la cosmologie avec une précision inférieure au pourcent. En effet, l’énergie sombre affecte la distance (comobile) qui nous sépare de l’époque de la recombinaison, et donc les échelles angulaires auxquelles les fluctuations de températures du CMB sont observées. Ces échelles se traduisent dans la position horizontale des pics dans le spectre de puissance.
- L'énergie sombre laisse une autre trace, plus subtile, dans les anisotropies de température du CMB, générée cette fois-ci au cours du voyage des photons du CMB depuis leur source jusqu'à nous.
> * Les grandes structures cosmiques comme les amas de galaxies courbent l'espace-temps autour d'eux et génèrent des puits de potentiel gravitationnel. Lorsqu'un photon du CMB traverse un amas de galaxies, il tombe dans le puits de potentiel. Et à la manière d'une bille qui est accélérée lorsqu'elle roule dans une cuvette, le photon gagne de l'énergie. Autrement dit, il bleuit (sa longueur d'onde est décalée vers le bleu). Et parce qu'émerger du puits gravitationnel est coûteux en énergie, le photon perd de l'énergie lorsqu'il quitte l'amas de galaxies. Autrement dit, il rougit. En règle générale, comme l’énergie se conserve, les deux effets se compensent, et le photon à la sortie de l’amas possède la même longueur d’onde qu’avant son arrivée. Ce phénomène a été théorisé en 1967 par les astronomes Rainer Kurt Sachs et Arthur Michael Wolfe et porte aujourd'hui le nom d'***effet Sachs-Wolfe***. Dans un univers dominé par l'énergie sombre (z < 1), la situation est légèrement différente. Lorsque le photon pénètre dans l’amas de galaxies, il rougit comme avant. Cependant, la structure est si vaste (il faut des centaines de millions d’années à un photon pour la traverser) qu’elle subit les effets de l’accélération de l’expansion cosmique. Ces effets se traduisent par une inflation de l’amas de galaxies, et donc par un « aplatissement » du puits de potentiel gravitationnel. Autrement dit, la forme de la cuvette évolue dans le temps. Elle est moins profonde qu’avant. Dans ce cas de figure, le photon bleui qui émerge de l’amas est moins rougi que dans la situation précédente. Lorsqu’il arrive dans nos détecteurs, le photon sera donc légèrement décalé vers le bleu par rapport à sa longueur d’onde de départ. On appelle ce phénomène l’***effet Sachs-Wolfe intégré*** (ou effet ISW, pour Integrated Sachs-Wolfe).
> * À noter que la traversée d’un vide cosmique produit aussi un effet Sachs-Wolfe intégré, mais qu’il fonctionne dans l’autre sens. Le photon rougit en arrivant dans le vide, et bleuit à sa sortie.
- Lorsqu’on cartographie la distribution de la matière dans une région du ciel et qu’on corrèle cette carte avec la carte des anisotropies du CMB dans la même direction du ciel (on fait ce qu’on appelle une ***corrélation croisée***), on peut donc en tirer des indices sur la nature et les propriétés de l’énergie sombre (à travers son effet sur la croissance des grands structures).

*Illustration de l'effet Sachs-Wolfe intégré. Crédit : NASA's Cosmic Times [Source](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/2006/faster.html)*

![ISW](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/2006/images/sachs_wolfe_illustration.gif)

### Les supernovae thermonucléaires

- Les ***supernovae thermonucléaires*** (aussi appelées ***supernovae de type Ia***, ou SNIa) sont des phénomènes résultant de l’explosion de naines blanches situées dans des couples stellaires.
> Dans de tels systèmes, la naine blanche (constituée majoritairement de carbone et d’oxygène) accumule de la matière provenant de sa partenaire à sa surface. Cette accumulation a lieu jusqu’à ce que la naine blanche atteigne une masse critique de 1,4 masse solaire (la ***masse de Chandrasekhar***). À ce moment-là, on pense que des réactions de fusion du carbone se déclenchent dans son cœur et s’emballent en quelques secondes à peine, provoquant une explosion thermonucléaire qui oblitère complètement cette dernière.
- La durée de ce type d’explosion est de l’ordre d’un mois, durée pendant laquelle la luminosité de la SNIa augmente puis diminue jusqu’à disparaitre dans la nuit. À son pic de luminosité, une SNIa peut être aussi brillante qu’une galaxie tout entière.
- Les SNIa sont très utiles pour les cosmologistes puisqu’elles constituent des ***chandelles standards***, c’est-à-dire des objets (idéaux) dont la luminosité intrinsèque est toujours la même, et qui servent donc à estimer les distances dans l’univers.
> Si on connaît la luminosité intrinsèque d’un objet, il suffit de mesurer sa luminosité apparente pour en déduire la distance (de luminosité) grâce à la ***loi en carré inverse***. 
- Une fois que l’on a mesuré la distance de luminosité d’une SNIa, on peut obtenir son redshift pour un modèle cosmologique donné à l’aide de la ***relation distance-redshift***. Observer la luminosité apparente d’un échantillon de SNIa permet donc d’obtenir des informations sur l’histoire de l’expansion de l’univers.
- En pratique, les SNIa ne sont pas des chandelles standard parfaites : à leur pic, la luminosité d’un échantillon de ces explosions présente des différences de l’ordre de 10-15% (après correction de la durée de l’explosion et de sa couleur). Ces différences limitent la précision à laquelle les distances cosmiques sont inférées. 
- On recense aujourd’hui des milliers de SNIa, dont près d’un millier sont utilisées à des fins cosmologiques.
> Ce sont des événements relativement rares. En moyenne, dans une galaxie typique, il se produit une SNIa tous les 100 ans environs.

*Courbe de luminosité d'un échantillon de SN Ia. Crédit : Huterer et Schafer (2018). [Source](https://astrobites.org/2019/02/18/type-ia-supernovae-could-use-some-more-color/)*

![SNIa](https://astrobites.org/wp-content/uploads/2020/01/Screen-Shot-2020-01-03-at-3.30.49-PM-1080x469.png)

### Les oscillations acoustiques des baryons

- La méthode la plus précise actuellement utilisée pour mesurer l'histoire de l'expansion de l'univers fait appel à d'étranges motifs imprimés dans le ciel que l'on peut détecter quand on observe la distribution des galaxies dans l'univers avec une grande précision. Quand on les projette sur la sphère céleste, ces motifs ressemblent à des petits cercles entourés d'immenses anneaux, similaires à ceux créés par un caillou jeté dans un étang. Ces anneaux sont difficiles à détecter et se chevauchent, mais le plus surprenant c'est qu'ils ont tous la même taille caractéristique (que les astronomes appellent ***échelle acoustique***).
- Ces motifs sont dus à un phénomène physique assez insolite : les  ***oscillations acoustiques des baryons*** (ou BAO, pour Baryonic Acoustic Oscillations)
- La signature des BAO dans la répartition des galaxies a été mise en évidence pour la première fois en 2005 par deux équipes indépendantes d'astronomes associés aux relevés SDSS (Sloan Digital Sky Survey, au Nouveau Mexique) et 2dFGRS (2dF Galaxy Redshift Survey, en Australie).
> Ils ont montré que les anneaux ont une taille caractéristique de 500 millions d'années-lumière (150 Mpc) dans l'univers actuel. Autrement dit, si on observe une galaxie quelque part dans l’univers, on a (un peu) plus de chance de trouver une autre galaxie située à 500 millions d’années-lumière d’elle qu’une galaxie à 400 ou à 600 millions d’années-lumière. 
- Pour comprendre l'origine de ces motifs, il faut revenir aux conditions de l'univers primordial.
> Pendant les premières centaines de milliers d'années qui ont suivi le Big Bang, l'univers était un plasma chaud composé de photons, de protons, d'électrons et de matière noire qui interagissaient sans cesse. Ce plasma avait beau être extrêmement homogène, de petites surdensités de matière noire apparaissaient quand même en permanence. Attirée par la gravité de ces petits amas de matière noire, la matière ordinaire (dite "baryonique") tentait de s'accumuler, mais le rayonnement ambiant avait tendance à s'opposer à cette accumulation (à noter que la matière noire n'interagit pas avec la lumière, donc elle peut former des amas sans problème). Ces deux processus antagonistes ont généré des ondes de pressions qui se déplacent à la vitesse du son (170 000 km/s) dans ce plasma ultra-dense. Ces ondes de pression sont littéralement des ondes sonores de différentes intensités qui se déplacent dans toutes les directions. Dans cette cacophonie, certaines longueurs d'onde sont plus amplifiées que d'autres. La "note" la plus amplifiée possède une longueur d'onde d'un million d'années-lumière environ, ce qui représente une fréquence 48 octaves plus basses que la note la plus grave sur un piano ! Mais lorsque le cosmos atteint l’âge de 380 000 ans, la température du rayonnement primordial passe sous la barre des 3000 K (2700°C environ), et les électrons peuvent enfin se lier aux protons pour former les premiers atomes. Le plasma primordial est devenu un gaz d’atomes électriquement neutres. La lumière cesse alors d’interagir avec la matière et peut se balader dans l’espace à l’infini. Le cosmos, initialement opaque, devient alors transparent. C’est ***l’époque de la recombinaison***. À ce moment, la cacophonie s’arrête. Les ondes sonores sont soudainement figées à une certaine distance caractéristique des surdensités de matière noire. C’est comme si on avait soudainement gelé la surface de l’étang. Ces surdensités sont les graines à partir desquelles se formeront les premières galaxies et autres grandes structures de l’univers. Les empreintes des ondes sonores en forme de coquilles sphériques, quant à elles, vont se mettre à enfler au fil de l’expansion cosmique, à la matière d’un cercle dessiné à la surface d’un ballon de baudruche que l’on gonfle. 13,8 milliards d’années plus tard, on pourra les observer sous la forme d’un léger surnombre de galaxies au niveau  de ces coquilles, et d’une très légère tendance de ces galaxies à être alignées le long de leur surface.
- Le phénomène des BAO lie de manière quantitative les anisotropies observées dans le CMB et les motifs d'anneaux observés dans la distribution spatiale des galaxies
> En effet, les BAO sont responsables des pics observés dans le ***spectre de puissance*** (à définir) du CMB. Ces derniers correspondent aux longueurs d'onde sonores les plus amplifiées dans le plasma primordial.
- En mesurant la taille de ces anneaux à différentes époques de l’univers, à l’aide de relevés astronomiques très fins, et en liant ces anneaux aux conditions initiales fournies par les anisotropies du CMB, on peut donc reconstruire l’histoire de l’expansion cosmique avec une grande précision.
- L'empreinte des BAO sert de ***règle standard*** aux astronomes, c’est-à-dire que ces anneaux sont des objets dont on connaît la taille réelle. Quand on mesure leur diamètre apparent dans le ciel, comme on connaît leur taille on peut donc déterminer leur distance.
- Le grand relevé astronomique BOSS (Baryon Oscillation Spectroscopic Survey), qui fait partie du 3e relevé SDSS, et dont les données ont été acquises entre 2009 et 2014, constitue à ce jour la mesure la plus précise des anneaux produits par les BAO. À travers l’étude spectroscopique de plus d’un million de galaxies, les astronomes ont pu mesurer ces « règles standard » avec une précision d’1% sur une période qui couvre les 6 derniers milliards d’années de l’évolution cosmique (il y a une vingtaine d’années à peine, cette précision n’était que de 50% !).
- Un instrument fondamental pour détecter la signature des BAO dans les années à venir est DESI (Dark Energy Spectroscopic Instrument, "instrument spectroscopique pour l'énergie noire"), installé sur le télescope Mayall de 4 mètres de l'Observatoire National de Kitt Peak (Arizona) et capable de mesurer 5000 spectres de galaxies simultanément
> En 2021, une collaboration internationale de 50 institutions dirigée par le Berkeley Lab a débuté un programme d'observation de 5 ans avec DESI qui devrait l'amener à cartographier la position de plus de 30 millions de galaxies dans le ciel de l'hémisphère Nord, dont près de 2,4 millions de quasars. Couvrant les 11 derniers milliards d’années de l’histoire de l’univers, il pourra atteindre des profondeurs que BOSS ne pouvait pas atteindre, et permettra l'étude la plus fine de l'histoire de l'expansion cosmique jamais produite.      	      	      

*(Illustration des BAO. Crédit: Gabriela Secara, Institut Périmètre [Source](https://insidetheperimeter.ca/fr/desi-launches-five-year-quest-to-understand-the-universe/))*

![BAO](https://insidetheperimeter.ca/wp-content/uploads/2021/05/test-DESI_Baryon_Acoustic_Oscillation.png)

### L'effet de lentille gravitationnelle faible

-	Lorsque des rayons de lumière provenant de galaxies très lointaines traversent l’espace-temps déformé aux abords d’une grosse concentration de matière (dans ce contexte, un amas de galaxies) située entre les sources et l’observateur, on observe des images distordues des galaxies lointaines, comme si ces dernières étaient vues à travers la lentille d’une loupe. Ces distorsions sont dues à l’***effet de lentille gravitationnelle***, un phénomène prédit par la théorie de la relativité générale d’Albert Einstein.
>	La mesure de la déflexion de rayons lumineux des étoiles observées à proximité du Soleil lors d’une éclipse en 1919 constitue l’une des premières prédictions réussies de la relativité générale.
-	Dans certaines situations, l’effet de lentille gravitationnelle peut produire de multiples images de la source en arrière-plan. On parle alors de ***lentillage fort***. Celui-ci requiert un alignement particulier entre la source, la concentration de matière et l’observateur, ainsi que des distances relatives particulières entre les 3 parties. 
>	Les lentilles gravitationnelles fortes peuvent générer d’étranges mirages gravitationnels, tels que les croix ou les anneaux d’Einstein.
-	Mais dans le contexte de la traque de l’énergie sombre, l’effet qui nous intéresse particulièrement est le ***lentillage faible***, beaucoup plus fréquent, mais plus difficile à observer. Ce dernier crée un aplatissement ainsi qu’un alignement systématique des images des galaxies en arrière-plan qui forment un motif particulier.
-	On appelle ***cisaillement cosmique*** (cosmic shear) le signal statistique causé par le lentillage faible des amas de galaxies. Pour l’estimer, on mesure la forme d’un grand échantillon de galaxies lointaines dans une région du ciel donné, et on calcule la déformation moyenne de ces galaxies produite par l’effet de lentilles gravitationnelles.
>	Le cisaillement cosmique a été mesuré pour la première fois en 2000 par une équipe internationale d’astronomes incluant des chercheurs de l’IAP, grâce à l’étude des données du Canada France Hawaii Telescope (CFHT).
-	L’analyse statistique des déformations causées par l’effet de lentillage faible permet de reconstruire la distribution de matière noire dans les amas de galaxies et de suivre l’évolution de ces structures dans le temps (en observant le lentillage faible de ces galaxies lointaines à différents redshifts), ce qui permet de sonder l’influence de l’énergie sombre sur le taux de croissance des structures cosmiques.
>	* On peut comparer la cartographie de la distribution de la matière noire par cette méthode à la reconstruction des lignes de champ magnétique invisible avec de la limaille de fer
> * Lorsque l'on cartographie le cisaillement cosmique, on peut recenser les fluctuations de densité de différentes tailles qui se jouent à grande échelle dans l'univers et en déduire leur spectre de puissance. Et l'étude de ce spectre de puissance nous informe sur la nature et les propriétés de l'énergie sombre. C'est une méthode analogue à celle de l'analyse du spectre de puissance du CMB, mais qui nous informe cette fois du devenir des fluctuations de densité initiales.
-	De nombreux relevés astronomiques font de la mesure du cisaillement cosmique une priorité dans les années à venir. C’est notamment le cas du Dark Energy Survey (DES), du Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) ou encore du relevé que réalisera l'observatoire Vera Rubin (anciennement appelé LSST), qui a pour mission de mesurer plusieurs milliards (!) de formes de galaxies dans le ciel de l'hémisphère Sud. 

*Illustration de l'effet de lentilles gravitationnelles faible. Crédit : Michael Sachs. [Source](https://en.wikipedia.org/wiki/Weak_gravitational_lensing#/media/File:Gravitational-lensing-3d.png)*

![weak-lensing](https://upload.wikimedia.org/wikipedia/commons/b/b9/Gravitational-lensing-3d.png)

### Les amas de galaxies

- Les amas de galaxies sont les plus grandes structures liées par gravité dans l'Univers.
> Ils ont des tailles de l'ordre du million d'années-lumière (Mpc) et des masses de l'ordre du million de milliards de masses solaires (10^15 Msol)
- Les galaxies ne constituent que quelques pourcents de la masse totale d'un amas. Celles-ci baignent en effet dans un gaz très chaud (comprimé à 10-100 millions de degrés sous l'action de la gravité) et très peu dense, et ce gaz représente 80-90% de la masse baryonique d'un amas. De plus, la masse totale de l'amas est largement dominée par la matière noire qui la compose (combien ?)
- Quand on observe les amas de galaxies en rayons X, ceux-ci apparaissent comme des sources diffuses et étendues. Ce qu'on voit dans ces longueurs d'onde, c'est la contribution du gaz intergalactique surchauffé. 
> On ne peut pas observer les rayons X depuis des observatoires au sol, puisque ces longueurs d'onde sont absorbées par l'atmosphère terrestre. On doit donc avoir recours à des observatoires spatiaux comme Chandra ou XMM-Newton pour ce genre d'études.
- Pour les amas dits "relaxés" (ie en équilibre dynamique), la distribution spatiale de ce gaz est très fortement corrélée avec la distribution de la matière noire qui compose en grande partie
> Les amas relaxés constituent une minorité dans la population des amas de galaxies. 
- La formation des amas de galaxies dépend à la fois de la composition de l'univers (et donc des propriétés de la matière noire : Ω_m), de l'histoire de la formation des structures cosmiques (encapsulé dans σ8, l'amplitude des fluctuations de matière) et de l'histoire de l'expansion de l'univers (et donc des propriétés de l'énergie sombre : Ω_Λ, ω)
> En recensant les gros amas de galaxies "relaxés" dans l'univers, on peut comparer les nombres d'amas observés en fonction de leur distance (redshift) et les abondances prédites par différents scénarios d'évolution et de composition cosmique (issues de simulations cosmologiques), et ainsi inférer les propriétés de l'énergie sombre qui collent le mieux aux observations (Ω_Λ, ω) 
- À partir de l'observation d'un amas en rayons X, on peut calculer sa masse baryonique (car la luminosité X est proportionnelle au carré de la densité de gaz qui émet ces rayons X) et sa température. Et à partir d'un profil de densité théorique, si l'on fait l'hypothèse que le gaz est en équilibre hydrostatique dans le puits de potentiel de l'amas (ie l'amas ne présente pas de sous-structure et n'est pas en train de fusionner avec d'autres amas), alors on peut calculer la masse totale de l'amas en fonction du rayon, puis en intégrant sur le profil la masse totale de l'amas. On peut alors obtenir la ***fraction de gaz*** des amas (notée f_gas(z)), définie comme le rapport de la masse de gaz (mesurée via l'émission de rayons X) sur la masse totale de l'amas (galaxies + gaz intergalactique + matière noire)
> * Une autre manière d'obtenir la masse totale d'un amas est d'utiliser l'effet de lentilles gravitationnelle.
- Au milieu des années 90, les astronomes se sont rendus compte qu'ils pouvaientaussi utiliser les observations en rayons X des amas comme indicateurs de distance de ces amas, ce qui leur ont permis de tester les prédictions du modèle ΛCDM et en particulier de contraindre les propriétés de l'énergie sombre
> * Il faut pour cela supposer que les amas de galaxies sont des objets si vastes et si massifs que leur composition matérielle, notamment le rapport entre matière baryonique et matière noire est représentative de la composition matérielle de l'univers tout entier.
> * La fraction de gaz est corrélée avec la distance de l'amas, et cette distance dépend elle-même de la quantité de matière noire et d'énergie sombre dans l'univers. Comme la fraction de gaz doit être à peu près la même sur tous les amas de galaxies, en mesurant la fraction de gaz dans plein d'amas on doit pouvoir trouver les paramètres cosmologiques qui donnent les distances d'amas "correctes".
- Les données actuelles issues de l'observation des amas de galaxies fournissent une confirmation indépendante que l'expansion de l'univers est bien accélérée, et elles montrent que les propriétés de l'énergie sombre sont très proches d'une constante cosmologique
- Le télescope spatial Euclid de l'ESA, qui sera lancé en 2023, a pour mission de produire dans les années à venir l'un des catalogues d'amas de galaxies les plus fournis et les plus profonds de tous les temps, ce qui permettra aux astronomes de mieux contraindre les propriétés de l'énergie sombre.  

*Emission de rayons X du gaz chaud de 4 amas de galaxies, photographiée par le télescope spatial Chandra. Crédit : X-ray: NASA/CXC/Univ. of Alabama/A. Morandi et al; Optical: SDSS, NASA/STScI. [Source](https://www.nasa.gov/mission_pages/chandra/probing-dark-energy-with-galaxy-clusters.html)*

![xraychandra](https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/clusters.jpg)

### La détermination directe du taux d'expansion cosmique

Direct Determination of H0: The value of H0 sets the current value of the critical density
ρc = 3H2
0 /8πGN, and combination with CMB measurements provides a long lever arm
for constraining the evolution of dark energy. The challenge in direct H0 measurements is
establishing distances to galaxies that are far enough away that their peculiar velocities
are small compared to the expansion velocity v = H0d. This can be done by building
a ladder of distance indicators tied to stellar parallax on its lowest rung, or by using
gravitational lens time delays or geometrical measurements of maser data to circumvent
this ladder

Direct measurements of the Hubble constant offer use-
ful complementary information that helps break degen-
eracy between dark energy and other cosmological pa-
rameters. This is because precise CMB measurements
effectively fix high-redshift parameters including the
physical matter density Ωmh2; independent measure-
ments of H0 (i.e. h) therefore help determine Ωm which
is degenerate with the dark energy equation of state.
Current & 3σ tension between the most precise direct
measurements of H0 from the Cepheid distance lad-
der [355, 356] and the indirect ΛCDM determination
from the CMB [74] is partially, but not fully, relieved
by allowing phantom dark energy (w < −1) or extra
relativistic degrees of freedom

A still more ambitious period begins late in this decade and continues through the
2020s, with experiments that include the Dark Energy Spectroscopic Instrument (DESI),
the Subaru Prime Focus Spectrograph (PFS), the Large Synoptic Survey Telescope
(LSST), and the space missions Euclid and WFIRST (Wide Field Infrared Survey
Telescope). DESI and PFS both aim for major improvements in the precision of BAO,
RSD, and other measurements of galaxy clustering in the redshift range 0.8 < z < 2,
where large comoving volume allows much smaller cosmic variance errors than low
redshift surveys like BOSS. LSST will be the ultimate ground-based optical weak lensing
experiment, measuring several billion galaxy shapes over 20,000 deg2 of the southern
hemisphere sky, and it will detect and monitor many thousands of SNe per year. Euclid
and WFIRST also have weak lensing as a primary science goal, taking advantage of the
high angular resolution and extremely stable image quality achievable from space. Both
missions plan large spectroscopic galaxy surveys, which will provide better sampling at
high redshifts than DESI or PFS because of the lower infrared sky background above
the atmosphere. WFIRST is also designed to carry out what should be the ultimate
supernova cosmology experiment, with deep, high resolution, near-IR observations and
the stable calibration achievable with a space platform.

Performance forecasts necessarily become more uncertain the further ahead we
look, but collectively these experiments are likely to achieve 1–2 order of magnitude
improvements over the precision of current expansion and growth measurements, while
simultaneously extending their redshift range, improving control of systematics, and
enabling much tighter cross-checks of results from entirely independent methods. The
critical clue to the origin of cosmic acceleration could also come from a surprising
direction, such as laboratory or solar system tests that challenge GR, time variation
of fundamental “constants,” or anomalous behavior of gravity in some astronomical
environments. Experimental advances along these multiple axes could confirm today’s
relatively simple, but frustratingly incomplete, “standard model” of cosmology, or they
could force yet another radical revision in our understanding of energy, or gravity, or the
spacetime structure of the Universe.


## Que sait-on de l'énergie sombre aujourd'hui ?

- L'énergie sombre ne domine le contenu énergétique de l'univers que depuis relativement récemment (z ~ 0.5)
- Elle affecte les distances dans l'univers, ainsi que la croissance et le nombre des grandes structures cosmiques 
- Les différentes contraintes observationnelles actuelles tendent vers w ~ -1 (à 5% près).
- Les prochains relevés astronomiques vont permettre d'étudier la variation temporelle (hypothétique) de la contribution de l'énergie sombre


# L'inflation cosmique

# Le problème de la hiérarchie

*Sources*

- [The Hierarchy Problem: why the Higgs has a snowball’s chance in hell](https://www.quantumdiaries.org/2012/07/01/the-hierarchy-problem-why-the-higgs-has-a-snowballs-chance-in-hell/) - Quantum Diaries
- [The Hierarchy Problem](https://profmattstrassler.com/articles-and-posts/particle-physics-basics/the-hierarchy-problem/) - Matt Strassler 
- [The Higgs, The Hierarchy Problem, and the LHC](https://www2.physics.ox.ac.uk/sites/default/files/2014-11-24/higgs_lhc_jmr_nov14_pdf_93873.pdf) - John March-Russell
- [The Mystery of the Higgs Boson's Mass](https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5idXp6c3Byb3V0LmNvbS8xMTYyNjEzLnJzcw/episode/QnV6enNwcm91dC04OTE4NTUw) - Why this universe
- [Naturalness after the Higgs](https://cerncourier.com/a/naturalness-after-the-higgs/) - CERN
- [A Deepening Crisis Forces Physicists to Rethink Structure of Nature’s Laws](https://www.quantamagazine.org/crisis-in-particle-physics-forces-a-rethink-of-what-is-natural-20220301/) - Quanta Magazine
- [The Dawn of the Post-Naturalness Era](https://arxiv.org/abs/1710.07663) - Giudice (2017)
- [A Lecture on the Hierarchy Problem and Gravity](https://cds.cern.ch/record/2120792/files/CERN-2013-003-p145.pdf) - Dvali (2013) 
- [The Higgs & the Hierarchy Problem](https://www.youtube.com/watch?v=iywSF7BGhyU) - Anna Barth
- [Big Mysteries: The Higgs Mass](https://www.youtube.com/watch?v=IjCypYnBYwQ) - Fermilab
- [Naturally Speaking: The Naturalness Criterion and Physics at the LHC](https://arxiv.org/abs/0801.2562) - Giudice (2008)
- [Histoire de la cosmologie](https://cosmology.education/booklet/booklet.pdf) - Lucas Gautheron
- [Naturalness: A Snowmass White Paper](https://arxiv.org/abs/2205.05708) - Craig (2022)
- ["Particle Physics: The Higgs Boson and Beyond" by Andreas Hoecker (CERN)](https://youtu.be/XX4pL7pwl7w) - SLAC (2012)
- ["The Once and Future Higgs" by Prof. Nathaniel Craig (University of California, Santa Barbara)](https://youtu.be/GduJt1V2eNk) - TIFR (2021)

---

- Le modèle standard est constitué de 24 particules fondamentales.
- Le boson de Higgs joue un rôle fondamental dans le modèle standard
- Le boson de Higgs brise la symétrie électrofaible et donne sa masse aux bosons et aux fermions
- Les particules interagissent avec le champ de Higgs (qui remplit tout l'espace-temps), ce qui réduit leur vitesse. Et plus elles interagissent fortement, plus leur masse est grande (c'est proportionnel).
- Découvert en 2012 par le LHC, un accélérateur de particules de 27km de circonférence
- On peut sonder la composition de la matière de plus en plus profondément à mesure que l'on fait des collisions de plus en plus énergétiques
- Dans le modèle standard, le higgs est "le seul scalaire fondamental (tous les autres scalaires sont des états composites)"
- Un des problèmes les plus importants de la physique contemporaine
- coincidence spectaculaire
- Boson de Higgs prédit en 1954
- En une phrase : la masse observée du boson de Higgs est de 125 GeV (125 fois la masse du proton ?). Pourtant, des corrections quantiques issues des interactions avec d'autres particules du modèle standard prédisent une masse 17 ordres de grandeur plus grande (au niveau de la masse de Planck). C'est le problème de la hiérarchie électrofaible.
- The Higgs boson plays a key role in the Standard Model: it is related to the unification of the electromagnetic and weak forces, explains the origin of elementary particle masses
- Higgs—the last piece of the Standard Model
- Hierarchy problem. This is often ‘explained’ by saying that quantum corrections want to make the Higgs much heavier than we need it to be… say, 125-ish GeV. 
- The Higgs has a snowball’s chance in hell of having a mass in that ballpark.
- If you put a glass of water in a really hot place—you expect it to also become really hot, maybe even to off into steam.  It would be really surprising if we put an ice cube in a hot oven and 10 minutes later it had not melted. This is because the ambient thermal energy is expected to be transferred to the ice cube by the energetic air molecules bouncing off it. Sure, it is possible that the air molecules just happen to bounce in a way that doesn’t impart much thermal energy—but that would be ridiculously improbable, as we learn in thermodynamics.
- The Higgs is very similar: we expect its mass to be around 125 GeV (not too far from W and Z masses), but ambient quantum energy wants to make its mass much larger through interactions with virtual particles. While it is possible that the Higgs stays light without any additional help, it’s ridiculously improbable, as we learn from quantum physics.
- the Standard Model really, really wants the Higgs to be around the 100 GeV scale. This is because it needs something to “unitarize longitudinal vector boson scattering.” It needs to have some Higgs-like state accessible at low energies to explain why certain observed particle interactions are well behaved.
- The Hierarchy problem has been the main motivation for new physics at the TeV scale for over two decades. 
- it is possible that the Higgs mass is 125 GeV due to some miraculous almost-cancellation that set it to be in just the right ballpark to unitarize longitudinal vector boson scattering. But such miracles are rare in physics without any a priori explanation. The electron mass is an excellent example. There are some apparent (and somewhat controversial) counter-examples: the cosmological constant problem is a much more severe ‘fine-tuning’ problem which may be explained anthropically rather than through more fundamental principles.
-  What are the possible ways to solve the Hierarchy problem?
- There are two main directions that most people consider:
> * Supersymmetry. Recall in our electron analogy that the solution to the ‘electron mass hierarchy problem’ was that quantum mechanics doubled the number of particles: in addition to the electron, there was also a positron. The virtual electron–positron contributions solved the problem by smearing out the electric charge. Supersymmetry is an analogous idea where once again the set of particles is doubled, and in doing so the loop contributions of one particle to the Higgs are cancelled by the loop contributions of its super-partner. Supersymmetry has deep connections to an extension of space-time symmetry since it relates matter particles to force particles.
> * Compositeness/extra dimensions. The other solution is that maybe our description of physics breaks down much sooner than the Planck scale. In particular, maybe at the TeV scale the Higgs no longer behaves like a scalar particles, but rather as a bound state of two fermions. This is precisely what happens with the mesons: even though the pion is a scalar, there is no pion ‘hierarchy problem’ because as you probe smaller distances, you realize the pion is actually a bound state of two quarks and it starts behaving as such. One of the beautiful developments of theoretical physics in the 1990s and early 2000s was the realization that this is precisely what is being described by theories of extra dimensions through the so-called holographic principle.
- An important feature of nature that puzzles scientists like myself is known as the hierarchy, meaning the vast discrepancy between aspects of the weak nuclear force and gravity. There are several different ways to describe this hierarchy, each emphasizing a different feature of it. Here is one:
- The mass of the smallest possible black hole defines what is known as the Planck Mass. 
-     The masses of the W and Z particles, the force carriers of the weak nuclear force, are about 10,000,000,000,000,000 times smaller than the Planck Mass. Thus there is a huge hierarchy in the mass scales of weak nuclear forces and gravity.
- When faced with such a large number as 10,000,000,000,000,000, ten quadrillion, the question that physicists are naturally led to ask is: where did that number come from? It might have some sort of interesting explanation.
- But while trying to figure out a possible explanation, physicists in the 1970s realized there was actually a serious problem, even a paradox, behind this number. The issue, now called the hierarchy problem, has to do with the size of the non-zero Higgs field, which in turn determines the mass of the W and Z particles.
- The non-zero Higgs field has a size of about 250 GeV, and that gives us the W and Z particles with masses of about 100 GeV. But it turns out that quantum mechanics would lead us to expect that this size of a Higgs field is unstable, something like (warning: imperfect analogy ahead) a vase balanced precariously on the edge of a table. With the physics we know about so far, the tendency of quantum mechanics to jostle — those quantum fluctuations I’ve mentioned elsewhere — would seem to imply that there are two natural values for the Higgs field — in analogy to the two natural places for the vase, firmly placed on the table or smashed on the floor. Naively, the Higgs field should either be zero, or it should be as big as the Planck Energy, 10,000,000,000,000,000 times larger than it is observed to be. Why is it at a value that is non-zero and tiny, a value that seems, at least naively, so unnatural? This is the hierarchy problem.
- Many theoretical physicists have devoted significant fractions of their careers to trying to solve this problem. Some have argued that new particles and new forces are needed (and their theories go by names such as supersymmetry, technicolor , little Higgs, etc.) Some have argued that our understanding of gravity is mistaken and that there are new unknown dimensions (“extra dimensions”) of space that will become apparent to our experiments at the Large Hadron collider in the near future. Others have argued that there is nothing to explain, because of a selection effect: the universe is far larger and far more diverse than the part that we can see, and we live in an apparently unnatural part of the universe mainly because the rest of it is uninhabitable — much the way that although rocky planets are rare in the universe, we live on one because it’s the only place we could have evolved and survived. There may be other solutions to this problem that have not yet been invented.
- Many of these solutions — certainly all the ones with new particles and forces or with new dimensions — predict that new phenomena should be visible at the Large Hadron Collider. 
- By the way, you will often read the hierarchy problem stated as a problem with the Higgs particle mass.  This is incorrect.  The problem is with how big the non-zero Higgs field is.  (For experts — quantum mechanics corrects not the Higgs particle mass but the Higgs mass-squared parameter, changing the Higgs field potential energy and thus the field’s value, making it zero or immense.  That’s a disaster because the W and Z masses are known.  The Higgs mass is unknown, and therefore it could be very large — if the W and Z masses were very large too.  So it is the W and Z masses — and the size of the non-zero Higgs field — that are the problem, both logically and scientifically.)
- Either new particles are keeping the Higgs boson light, or the universe is oddly fine-tuned for our existence. 
- When Victor Weisskopf sat down in the early 1930s to compute the energy of a solitary electron, he had no way of knowing that he’d ultimately discover what is now known as the electroweak hierarchy problem. Revisiting a familiar puzzle from classical electrodynamics – that the energy stored in an electron’s own electric field diverges as the radius of the electron is taken to zero (equivalently, as the energy cutoff of the theory is taken to infinity) – in Dirac’s recently proposed theory of relativistic quantum mechanics, he made a remarkable discovery: the contribution from a new particle in Dirac’s theory, the positron, cancelled the divergence from the electron itself and left a quantum correction to the self-energy that was only logarithmically sensitive to the cutoff. 
-  the coupling between the Higgs boson and other particles of the Standard Model (SM) leads to yet another divergent self-energy, for which the logic of naturalness implied new physics at around the TeV scale. Thus the electroweak hierarchy problem was born – not as a new puzzle unique to the Higgs, but rather the latest application of Weisskopf’s wildly successful logic (albeit one for which the answer is not yet known). 
- History suggested two possibilities. As a scalar, the Higgs could only benefit from the sort of cancellation observed among fermions if there is a symmetry relating bosons and fermions, namely supersymmetry. Alternatively, it could be a light product of compositeness, just as the pions and kaons are light bound states of the strong interactions. These solutions to the hierarchy problem came to dominate expectations for physics beyond the SM, with a sharp target – the TeV scale – motivating successive generations of collider experiments. Indeed, when the physics case for the LHC was first developed in the mid-1980s, it was thought that new particles associated with supersymmetry or compositeness would be much easier to discover than the Higgs itself. But while the Higgs was discovered, no signs of supersymmetry or compositeness were to be found.
- In the meantime, other naturalness problems were brewing. The vacuum energy – Einstein’s infamous cosmological constant – suffers a divergence of its own, and even the finite contributions from the SM are many orders of magnitude larger than the observed value. Although natural expectations for the cosmological constant fail, an entirely different set of logic seems to succeed in its place. To observe a small cosmological constant requires observers, and observers can presumably arise only if gravitationally-bound structures are able to form. As Steven Weinberg and others observed in the 1980s, such anthropic reasoning leads to a prediction that is remarkably close to the value ultimately measured in 1998. To have predictive power, this requires a multitude of possible universes across which the cosmological constant varies; only the ones with sufficiently small values of the cosmological constant produce observers to bear witness.
- An analogous argument might apply to the electroweak hierarchy problem: the nuclear binding energy is no longer sufficient to stabilise the neutron within typical nuclei if the Higgs vacuum expectation value (VEV) is increased well above its observed value. If the Higgs VEV varies across a landscape of possible universes while its couplings to fermions are kept fixed, only universes with sufficiently small values of the Higgs VEV would lead to complex atoms and, presumably, observers. Although anthropic reasoning for the hierarchy problem requires stronger assumptions than for the cosmological-constant problem, its compatibility with null results at the LHC is enough to raise questions about the robustness of natural reasoning. 
- The success of the relaxion hypothesis in solving the hierarchy problem hinges on an array of other questions involving gravity. Whether the relaxion potential can remain sufficiently smooth over the vast trans-Planckian distances in field space required to set the value of the weak scale is an open question, one that is intimately connected to the fate of global symmetries in a theory of quantum gravity (itself the target of active study in what is known as the Swampland programme).
- the recognition that cosmology might play a role in solving the hierarchy problem has given rise to a plethora of new ideas. For instance, in Raffaele D’Agnolo and Daniele Teresi’s recent paradigm of “sliding naturalness”, the Higgs is coupled to a new scalar whose potential features two minima. In the true minimum, the cosmological constant is large and negative, and the universe would crunch away into oblivion if it ended up in this vacuum. In the second, local minimum, the cosmological constant is safely positive (and can be made compatible with the small observed value of the cosmological constant by Weinberg’s anthropic selection). The Higgs couples to this scalar in such a way that a large value of the Higgs VEV destabilises the “safe” minimum. During the inflationary epoch, only universes with suitably small values of the Higgs VEV can grow and expand, while those with large values of the Higgs VEV crunch away. A second scalar coupled analogously to the Higgs can explain why the VEV is small but non-zero. 
- Alternatively, in the paradigm of “Nnaturalness” proposed by Nima Arkani-Hamed and others, the multitude of SMs over which the Higgs mass varies occur in one universe, rather than many. The fact that the universe is predominantly composed of one copy of the SM with a small Higgs mass can be explained if inflation ends and reheats the universe through the decay of a single particle. If this particle is sufficiently light, it will preferentially reheat the copy of the SM with the smallest non-zero value of the Higgs VEV, even if it couples symmetrically to each copy. The sub-dominant energy density deposited in other copies of the SM leaves its mark in the form of dark radiation susceptible to detection by the Simons Observatory or upcoming CMB-S4 facility. 
- Finally, Gian Giudice, Matthew Mccullough and Tevong You have recently shown that inflation can help to understand the electroweak hierarchy problem by analogy with self-organised criticality. Just as adding individual grains of sand to a sandpile induces avalanches over diverse length scales – a hallmark of critical behaviour, obtained without tuning parameters – so too can inflation drive scalar fields close to critical points in their potential. This may help to understand why the observed Higgs mass lies so close to the boundary between the unbroken and broken phases of electroweak symmetry without fine tuning.
- Underlying Weisskopf’s natural reasoning is a long-standing assumption about relativistic theories of quantum mechanics: physics at short distances (the ultraviolet, or UV) is decoupled from physics at long distances (the infrared, or IR), making it challenging to apply a theory involving a large energy scale to a much smaller one without fine tuning. This suggests that loopholes may be found in theories that mix the UV and the IR, as is known to occur in quantum gravity. 
- While the connection between this type of UV/IR mixing and the mass of the Higgs remains tenuous, there are encouraging signs of progress.
- We have come a long way since Weisskopf first set out to understand the self-energy of the electron. The electroweak hierarchy problem is not the first of its kind, but rather the one that remains unresolved. The absence of supersymmetry or compositeness at the TeV scale beckons us to search for new solutions to the hierarchy problem, rather than turning our backs on it. In the decade since the discovery of the Higgs, this search has given rise to a plethora of novel approaches, building new bridges between particle physics, cosmology and gravity along the way. Despite the many differences among these new approaches, they share a common tendency to leave imprints on the Higgs boson. And so, as ever, we must look to experiment to show the way. 
-  The hierarchy problem, as the puzzle is called, asks why the Higgs boson is so lightweight — a hundred million billion times less massive than the highest energy scales that exist in nature. The Higgs mass seems unnaturally dialed down relative to these higher energies, as if huge numbers in the underlying equation that determines its value all miraculously cancel out.
-  The extra particles would have explained the tiny Higgs mass, restoring what physicists call “naturalness” to their equations. But after the LHC became the third and biggest collider to search in vain for them, it seemed that the very logic about what’s natural in nature might be wrong.
-  At first, the community despaired. “You could feel the pessimism,” said Isabel Garcia Garcia, a particle theorist at the Kavli Institute for Theoretical Physics at the University of California, Santa Barbara, who was a graduate student at the time. Not only had the $10 billion proton smasher failed to answer a 40-year-old question, but the very beliefs and strategies that had long guided particle physics could no longer be trusted. People wondered more loudly than before whether the universe is simply unnatural, the product of fine-tuned mathematical cancellations. Perhaps there’s a multiverse of universes, all with randomly dialed Higgs masses and other parameters, and we find ourselves here only because our universe’s peculiar properties foster the formation of atoms, stars and planets and therefore life. This “anthropic argument,” though possibly right, is frustratingly untestable.
-  Some of those who remained set to work scrutinizing decades-old assumptions. They started thinking anew about the striking features of nature that seem unnaturally fine-tuned — both the Higgs boson’s small mass, and a seemingly unrelated case, one that concerns the unnaturally low energy of space itself.
-  Their introspection is bearing fruit. Researchers are increasingly zeroing in on what they see as a weakness in the conventional reasoning about naturalness. It rests on a seemingly benign assumption, one that has been baked into scientific outlooks since ancient Greece: Big stuff consists of smaller, more fundamental stuff — an idea known as reductionism. “The reductionist paradigm … is hard-wired into the naturalness problems,” said Nima Arkani-Hamed, a theorist at the Institute for Advanced Study in Princeton, New Jersey.
- Now a growing number of particle physicists think naturalness problems and the null results at the Large Hadron Collider might be tied to reductionism’s breakdown. “Could it be that this changes the rules of the game?” Arkani-Hamed said. In a slew of recent papers, researchers have thrown reductionism to the wind. They’re exploring novel ways in which big and small distance scales might conspire, producing values of parameters that look unnaturally fine-tuned from a reductionist perspective.
- The Large Hadron Collider did make one critical discovery: In 2012, it finally struck upon the Higgs boson, the keystone of the 50-year-old set of equations known as the Standard Model of particle physics, which describes the 17 known elementary particles.
- The discovery of the Higgs confirmed a riveting story that’s written in the Standard Model equations. Moments after the Big Bang, an entity that permeates space called the Higgs field suddenly became infused with energy. This Higgs field crackles with Higgs bosons, particles that possess mass because of the field’s energy. As electrons, quarks and other particles move through space, they interact with Higgs bosons, and in this way they acquire mass as well.
- After the Standard Model was completed in 1975, its architects almost immediately noticed a problem.
- When the Higgs gives other particles mass, they give it right back; the particle masses shake out together. Physicists can write an equation for the Higgs boson’s mass that includes terms from each particle it interacts with. All the massive Standard Model particles contribute terms to the equation, but these aren’t the only contributions. The Higgs should also mathematically mingle with heavier particles, up to and including phenomena at the Planck scale, an energy level associated with the quantum nature of gravity, black holes and the Big Bang. Planck-scale phenomena should contribute terms to the Higgs mass that are huge — roughly a hundred million billion times larger than the actual Higgs mass. Naively, you would expect the Higgs boson to be as heavy as they are, thereby beefing up other elementary particles as well. Particles would be too heavy to form atoms, and the universe would be empty.
- For the Higgs to depend on enormous energies yet end up so light, you have to assume that some of the Planckian contributions to its mass are negative while others are positive, and that they’re all dialed to just the right amounts to exactly cancel out. Unless there’s some reason for this cancellation, it seems ludicrous — about as unlikely as air currents and table vibrations counteracting each other to keep a pencil balanced on its tip. This kind of fine-tuned cancellation physicists deem “unnatural.”
- Within a few years, physicists found a tidy solution: supersymmetry, a hypothesized doubling of nature’s elementary particles. Supersymmetry says that every boson (one of two types of particle) has a partner fermion (the other type), and vice versa. Bosons and fermions contribute positive and negative terms to the Higgs mass, respectively. So if these terms always come in pairs, they’ll always cancel.
- The search for supersymmetric partner particles began at the Large Electron-Positron Collider in the 1990s. Researchers assumed the particles were just a tad heavier than their Standard Model partners, requiring more raw energy to materialize, so they accelerated particles to nearly light speed, smashed them together, and looked for heavy apparitions among the debris.
- The fabric of space, even when devoid of matter, seems as if it should sizzle with energy — the net activity of all the quantum fields coursing through it. When particle physicists add up all the presumptive contributions to the energy of space, they find that, as with the Higgs mass, injections of energy coming from Planck-scale phenomena should blow it up. Albert Einstein showed that the energy of space, which he dubbed the cosmological constant, has a gravitationally repulsive effect; it causes space to expand faster and faster. If space were infused with a Planckian density of energy, the universe would have ripped itself apart moments after the Big Bang. But this hasn’t happened.
- Instead, cosmologists observe that space’s expansion is accelerating only slowly, indicating that the cosmological constant is small. Measurements in 1998 pegged its value as a million million million million million times lower than the Planck energy. Again, it seems all those enormous energy injections and extractions in the equation for the cosmological constant perfectly cancel out, leaving space eerily placid.
- In hindsight, the two naturalness problems seem more like symptoms of a deeper issue. “It’s useful to think about how these problems come about,” said Garcia Garcia in a Zoom call from Santa Barbara this winter. “The hierarchy problem and the cosmological constant problem are problems that arise in part because of the tools we’re using to try to answer questions — the way we’re trying to understand certain features of our universe.”
- 

# Le problème du lithium cosmique

*Sources*


- [Populating the periodic table: Nucleosynthesis of the elements](https://science.sciencemag.org/content/363/6426/474) - Johnson (2019)
- [The Primordial Lithium Problem](https://www.annualreviews.org/doi/10.1146/annurev-nucl-102010-130445) - Fields (2011)
- [The Cosmological Lithium Problem Revisited](https://arxiv.org/abs/1603.03864) - Bertulani et al (2016)
- [Etoile de population II](https://fr.wikipedia.org/wiki/%C3%89toile_de_population_II) - WIkipédia
- [Big-Bang Nucleosynthesis and the Baryon Density of the Universe](https://arxiv.org/pdf/astro-ph/9407006.pdf) - Copi et al (1995)
- [Big Bang Nucleosynthesis](https://pdg.lbl.gov/2019/reviews/rpp2019-rev-bbang-nucleosynthesis.pdf)  - Fields (2019)
- [Big Bang Nucleosynthesis (BBN)](https://www.astronomy.ohio-state.edu/weinberg.21/A5682/notes8.pdf) - Weinberg
- [The cosmological lithium problem](https://www.sciencedaily.com/releases/2018/10/181009102501.htm) - Science Daily (2018)

---

- La ***nucléosynthèse primordiale*** désigne le processus de formation des noyaux les plus légers dans les conditions ardentes de l'univers primordial, entre 1 s et 3 minutes (~180 s) environ après le Big Bang.
> À partir de 1 seconde après le Big Bang, la température du cosmos passe sous la barre des 10 milliards de degrés. Une séquence d'événements est alors initiée, qui mène à la synthèse de quelques éléments légers, comme ***le deutérium, l'hélium-3, l'hélium-4 et le lithium-7***.
- La théorie de la nucléosynthèse primordiale standard (Big Bang Nucleosynthesis) repose sur le modèle standard de la physique des particules ainsi que du modèle standard de la cosmologie (ΛCDM), qui modélise un univershomogène et isotrope en expansion selon les règles de la relativité générale contenant de la matière noire et de l'énergie sombre.
- Dans la théorie de la nucléosynthèse primordiale standard, les abondances des éléments légers sont encapsulées dans un unique paramètre, la ***densité de baryons cosmique***, noté η (les baryons dans ce contexte sont les protons et les neutrons), qui est normalisée par rapport à la densité de photons du fond diffus cosmologique (baryon-photon ratio). Ce rapport est de l'ordre de 10^-9, ce qui veut dire que pour chaque baryon de l'univers, il y a environ 1 milliard de photons du CMB.
> La densité de photon du CMB est de 413 photons/cm3.
- Le ***problème du lithium cosmique*** désigne l'énorme différence entre nos théories et nos observations concernant l'abondance de lithium 7 dans l'univers.
> On observe 3 à 4 fois moins de lithium-7 dans l'univers que ce qui est prédit par la théorie. 
- Par contre, les abondances d'hélium et de deutérium sont reproduites avec beaucoup de succès
> Ces abondances constituent même l'un des quatre piliers observationnels du modèle du Big Bang chaud.
- Le problème du lithium est apparu lorsque les astronomes ont commencé à étudier les propriétés du rayonnement de fond diffus cosmologique à l'aide du satellite WMAP, et le désaccord entre théorie et observation est devenu de plus en plus plus important au fil des nouvelles observations (à l'époque de la première fournée de données de WMAP, il n'y avait qu'un facteur 2-3).

### Comment mesure t-on les abondances des éléments légers dans l'univers ?

- En analysant la carte du fond diffus cosmologique, on peut prédire la densité de baryons cosmiques avec une grande précision et ainsi tester la théorie de la nucléosynthèse primordiale.
> Les récentes données du satellite Planck donnent une prédiction de η = 6x10^-10, qui est en très bon accord avec les abondances observées de deutérium et d'hélium-4 de z=1000 à z=0.
- Les abondance en deutérium (un isotope de l'hydrogène constitué d'1 proton et 1 neutron) sont mesurées dans des nuages d'hydrogène très lointains (z~3) et très pauvres en éléments lourds qui sont observés sur la ligne de visée de quasars encore plus lointains. On ne peut pas mesurer les abondances en deutérium dans les étoiles, car il est entièrement détruit dans ces systèmes.
> À l'heure actuelle, ce sont les abondances qui sont (de loin) les mieux reproduites par la théorie.
- Les abondances en hélium-3 sont mesurées dans le milieu interstellaire dans la Voie Lactée, faute de pouvoir les mesurer dans des galaxies lointaines. Comme notre galaxie est riche en éléments lourds, on ne peut pas utiliser pour le moment les abondances en hélium-3 pour contraindre la période de nucléosynthèse primordiale.
- Les abondances en hélium-4 sont mesurées dans des régions de formation d'étoiles (appelées ***régions HII***) de galaxies voisines pauvres en éléments lourds (metal-poor)
- Les abondances en lithium sont principalement mesurées dans l'atmosphère (photosphère) de très vieilles étoiles (âgées de 11 à 13,5 milliards d'années) pauvres en éléments lourds présentes dans le halo stellaire de notre Galaxie. On connaît environ une centaine de ces étoiles dites de ***population II***. Les abondances observées sont relativement faibles, notamment parce que le lithium de l'atmosphère de ces étoiles est en permanence emporté dans les profondeurs par les mouvements convectifs de leur enveloppe, où il est détruit par la chaleur intense.
> Ce qu'on mesure en pratique ce sont les raies d'absorption dans le spectre de ces étoiles qui correspondent à la signature du lithium

### Quelles sont les solutions possibles au problème du lithium cosmique ?

- Soit nos prédictions théoriques (cosmologie+physique des particules) sont correctes, mais ce sont les observations astrophysiques qui sont incomplètes
> Grâce au LSST, on aura bientôt accès à une population bien plus grande d'étoiles pauvres en éléments lourds dans des galaxies proches et lointaines qui permettra d'avoir des statistiques plus fiables sur les abondances de lithium.
- Il pourrait aussi exister des processus encore inconnus au niveau de la physique nucléaire qui pourrait altérer nos prédictions sur les abondances du lithium
> Ces processus pourraient en particulier amplifier la destruction du béryllium-7. On peut vérifier cette hypothèse à l'aide d'expériences de physique nucléaire comme celles menées par la collaboration n_TOF (neutron-Time Of Flight) au CERN depuis 2018.
- Il pourrait enfin exister des solutions au-delà du modèle standard (comme la supersymmétrie par exemple) qui pourrait impliquer de nouveaux processus au niveau cosmologique ou de la physique des particules
> Ces solutions sont testées dans les accélérateurs de particules comme le LHC et du côté des expériences de détection de matière noire.

# Le principe cosmologique

- [Probing cosmic isotropy with a new X-ray galaxy cluster sample through the LX–T scaling relation](https://www.aanda.org/articles/aa/full_html/2020/04/aa36602-19/aa36602-19.html) - Migkas et al (2020)
- [Observation d'une anisotropie de l'Univers !](https://www.ca-se-passe-la-haut.fr/2020/04/observation-dune-anisotropie-de-lunivers.html) - Ca se passe là-haut
- [The CMB Dipole: Eppur Si Muove](https://arxiv.org/abs/2111.12186) - Sullivan et Scott (2021)
- [Cosmic Microwave Background Dipole](https://astronomy.swin.edu.au/cosmos/c/Cosmic+Microwave+Background+Dipole) - SAO Encyclopedia of astronomy
- [The Motion of the Local Group with Respect to the 15,000 Kilometer per Second Abell Cluster Inertial Frame](https://adsabs.harvard.edu/full/1994ApJ...425..418L) - Lauer et Postman (1994)
- [Testing the Cosmological Principle in the radio sky](https://arxiv.org/pdf/1905.12378.pdf) - Bengaly et al (2021)
- [Is the Observable Universe Consistent with the Cosmological Principle?](https://arxiv.org/abs/2207.05765) - Aluri et al (2022)
- [Cosmologists Parry Attacks on the Vaunted Cosmological Principle](https://www.quantamagazine.org/giant-arc-of-galaxies-puts-basic-cosmology-under-scrutiny-20211213/) - Quanta (2021)
- [Un intrigant anneau géant fait de sursauts gamma](https://www.futura-sciences.com/sciences/actualites/sursaut-gamma-intrigant-anneau-geant-fait-sursauts-gamma-59317/) - Futura Sciences (2015)
- [The dipole repeller](https://www.nature.com/articles/s41550-016-0036) - Hoffman et al (2017)
- [Testing the Cosmological Principle](https://indico.cern.ch/event/1036660/attachments/2241767/3801102/TestingCosmoPrinciple.pdf) - Subir Sarkar (2021)

---

- L'homogénéité (à grande échelle, ie au-delà de 70 h-1 Mpc) et l'isotropie (statistiquement parlant) de l'univers est à la base du modèle standard de la cosmologie. Autrement dit, il n'y a pas d'observateur privilégié ou de direction privilégiée dans l'univers. Où que l'on soit dans l'univers, et quelque soit la direction où l'on regarde, on devrait observer la même chose à grande échelle. Cette idée constitue le ***principe cosmologique***.
> The cosmological principle grew out of the Copernican principle, Nicolaus Copernicus’ 1543 realization that Earth is not the fixed center of creation. Not only is Earth not special, but nothing anywhere is special. 
- The universe is clearly not homogeneous on the human scale. Teleport a person one light-year from here and you’ll ruin their day. But drop the Hubble Space Telescope halfway across the universe, and it will return familiar-looking galaxy-filled images. 
- Theorists reconstruct the cosmos’s past and predict its future using a standard theoretical model based largely on general relativity, Albert Einstein’s theory of gravity. Einstein’s theory describes the interplay between matter and space-time — the bendy fabric of the universe. But Einstein’s treatment involves 10 interlinked equations and 20 variables, a system of equations that is generally too complicated to solve.

Cosmologists lean on the cosmological principle to restrict their focus to a universe acting as a smooth and symmetric fluid. By ignoring bumps of matter like galaxies and requiring the universe to expand in the same way along all three axes, the cosmological principle deletes parts of the equations and links some of the variables, dramatically simplifying the system of equations. Theorists can then predict the velocity and acceleration of the cosmos’s expansion with just two equations — the Friedmann equations, derived from Einstein’s by Alexander Friedmann, a Russian cosmologist, in 1922. It’s a bit like computing the volume of the Earth: You could fret over every mountain and ravine, or you could assume the planet is a sphere and call it a day.

### Le dipôle cosmologique

- Lorsque l'on cartographie le fond diffus cosmologique, on remarque qu'il n'est pas directement isotrope. La plus grande anisotropie de température qu'il présente est un dipole. On l'appelle le ***dipôle cosmologique*** (CMB dipole). Son amplitude (ΔT/T\~10-3) est plus grande que les autres fluctuations de température (ΔT/T\~10-5). 
> Afin d'étudier les anisotropies de l'ordre de 10-5, on doit donc soustraire la contribution du dipôle cosmologique.
- Son existence a été révélée pour la première fois par les mesures du satellite COBE dans les années 90.

*Dipôle cosmologique révélé par COBE ([Source](https://apod.nasa.gov/apod/ap010128.html))*

![COBE_dipole](https://apod.nasa.gov/apod/image/0101/dipole_cobe.jpg)

- On interprète généralement ce dipôle comme le résultat du mouvement combiné de la Terre autour du Soleil, du Soleil autour de la Voie Lactée, de la Voie Lactée dans le Groupe Local, et du Groupe Local vers un ***Grand Attracteur*** dans le référentiel du CMB (modèle proposé par Lynden Bell et al en 1988), qui génère un ***effet Doppler*** : on observe le CMB avec un décalage spectral vers le bleu dans la direction de notre mouvement et un décalage vers le rouge dans la direction opposée. 
> * Notre système solaire se déplace à environ 370 km/s par rapport au référentiel dans lequel le CMB est isotrope, et le Groupe Local se déplace à environ 630 (+-20) km/s (2,2 millions de km/h) par rapport au CMB dans la direction du super-amas de Shapley (à 600 millions d'années-lumière d'ici), à cause du Grand Attracteur (une région contenant une demi-douzaine d'amas de galaxies qui se trouve au coeur du super-amas Laniakea, à 150 millions d'années-lumière d'ici)
> * Ce Grand Attracteur est difficile à observer dans le domaine du visible parce qu'il est situé directement derrière le plan Galactique.
- Depuis 2017, on sait aussi qu'il existe une région relativement vide de matière (noire et baryonique) située dans la direction opposée du Grand Attracteur. Cette région, qu'on a appelé ***répulseur du dipôle*** (Dipole Repeller), agit de manière effective comme un répulseur sur le mouvement des galaxies avoisinantes. La force attractive causée par le Grand Attracteur (sur-densité de matière dans la direction de notre mouvement) ET la force "répulsive" causée par le répulseur du dipôle (sous-densité de matière dans la direction opposée) contribuent de manière équivalente au mouvement du Groupe Local dans l'espace, et donc au dipôle cosmologique.

*Le mouvement du Groupe Local dans son contexte cosmique (Source : Hoffman et al 2017)*

![dipole-repeller](https://user-images.githubusercontent.com/4954089/203095826-6f110b58-2853-4938-80f3-af813e76cfe7.png)

- Lorsque ce mouvement global est corrigé, le CMB est remarquablement isotrope.
- Cependant, l'isotropie du CMB (à l'échelle de 10-5) n'implique pas forcément l'isotropie de l'univers. Pour tester l'isotropie de l'univers, on a besoin d'autres mesures indépendantes, par exemple en étudiant la distribution statistique de matière dans l'univers. 

### Des structures plus grandes que l'échelle d'homogénéité

- Depuis les années 80, des relevés astronomiques révèlent l'existence d'une liste croissante de grandes structures dont la taille dépasse l'échelle maximale à partir de laquelle l'univers est sensé être homogène. L'estimation haute de cette échelle d'homogénéité est de ~370 Mpc (1,2 milliards d'années-lumière).
- La plupart de ces structures sont des ***amas de quasars*** (en anglais : large quasar group ou LQG). Ces collections de quasars font partie des plus grandes structures cosmiques connues. On pense qu'ils pourraient être les précurseurs des filaments galactiques que l'on observe dans l'univers proche.
- ***Liste des plus grandes structures cosmiques par ordre croissant de taille :***
> * Le ***grand mur de Sloan*** est une structure mesurant plus d'1,37 milliards d'années-lumière, découverte en 2003 dans les données du SDSS
> * Le ***Clowes–Campusano Large Quasar Group*** (CCLQG) est un amas de quasars constitué de 34 quasars et mesurant environ 2 milliards d’années-lumière de diamètre (~630 Mpc), découvert en 1991.
> * ***L'amas de quasars U1.11*** est une collection de 38 quasars dont le diamètre est estimé à 2,5 milliards d'années-lumière (~780 Mpc). Il a été découvert en 2011 à proximité du CCLQG dans les données du SDSS.
> * ***L'arc géant*** est une structure en forme de sourire mesurant 3,3 milliards d'années-lumière (1/28e du diamètre de l'univers observable), découverte en 2021 dans les données du SDSS. Sa taille apparente dans le ciel est aussi étendue que 20 pleines lunes. Elle se trouve à 9,2 milliards d'années-lumière d'ici (z~0.8).
> * Le ***Huge-LQG*** (Huge Large Quasar Group, « Immense grand amas de quasars ») est un amas composé de 73 quasars mesurant environ 4 milliards d'années-lumière de diamètre (~1240 Mpc), découvert en 2013 dans les données du SDSS
> * ***L'anneau géant de sursauts gamma*** (Giant GRB ring) est un grand anneau constitué par 9 sursauts gamma (hypernovae ou collisions entre étoiles à neutrons) qui s'étalerait sur environ 5,6 milliards d'années-lumière. Il a été découvert en 2015. Son diamètre apparent dans le ciel est équivalent à 70 pleines lunes.
> * Le ***Grand Mur d'Hercule-Couronne boréale*** est un filament cosmique mesurant 10 milliards d'années-lumière, découvert en 2013 dans les données d'un relevé de sursauts gamma. Il s'agit de la plus vaste et la plus massive structure cosmique connue de l'univers observable. Mais certaines études mettent en doute son existence.
- Le modèle standard de la cosmologie n'interdit pas complètement l'existence de telles structures, mais il les rend extrêmement rares.

### Conclusion

- Avec les données actuelles, on ne peut pas encore conclure que le principe cosmologique n'est pas valide. Mais dans les décennies à venir, 

- The isotropy of the late Universe and consequently of the X-ray galaxy cluster scaling relations is an assumption greatly used in astronomy. However, within the last decade, many studies have reported deviations from isotropy when using various cosmological probes; a definitive conclusion has yet to be made. New, effective and independent methods to robustly test the cosmic isotropy are of crucial importance.

referred axes noted in other astronomical surveys spanning the
electromagnetic spectrum were also found to be aligned with the CMB kinematic dipole, pointing towards
the Virgo cluster [73, 182]. The standard narrative says that these are coincidences
-  
Previous work has shown no statistically significant violation of isotropy in the obser-
vational data of Type Ia Supernova distances [4, 5, 6] and of gamma-ray bursts [7, 8, 9].
More stringent tests require the far higher number densities delivered by large galaxy sur-
veys. The simplest way to test consistency with the CMB is to measure the dipole of a
(sufficiently wide) galaxy survey, which should be aligned with the direction of the CMB
dipole [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. For currently available data sets,
the matter dipole direction is not inconsistent with the CMB, but the amplitude is too large,
probably arising from the quality of current data sets. Forecasts predict that future all-sky
radio continuum surveys with the SKA should achieve the accuracy necessary to make a
stringent test of consistency with the CMB dipole [23, 24]
- ensions
have emerged within the ΛCDM model, most notably a statistically significant discrepancy in the
value of the Hubble constant, H0. Since the notion of cosmic expansion determined by a single
parameter is intimately tied to the CP, implications of the H0 tension may extend beyond ΛCDM
to the CP itself. This review surveys current observational hints for deviations from the expecta-
tions of the CP, highlighting synergies and disagreements that warrant further study. Setting aside
the debate about individual large structures, potential deviations from the CP include variations of
cosmological parameters on the sky, discrepancies in the cosmic dipoles, and mysterious alignments
in quasar polarizations and galaxy spins.
- If the Universe is not FLRW, but we view it through the prism of FLRW, cos-
mological tensions are inevitable. Interestingly, a host of fascinating observational tensions exist, including
the H0 tension [45, 51–58], the S8 tension [178–181], potentially a curvature tension [119, 296], and an
AISW tension 
- But our peculiar velocity might not fully explain the perceived lopsidedness of the CMB; the distortion could also include the effect of the whole universe drifting. If this is the case, gauging our motion against distant galaxies will give a different result than if we measure our speed against the CMB, since those galaxies will be moving too. 
-  we document a series of intriguing alignments that are surprising in a statistically homo-
geneous and isotropic universe. One curious aspect of some of these alignments is their observation over
large – potentially gigaparsecs – scales and axes that overlap with the CMB dipole direction. If not due to
experimental systematics or interstellar physics, one exciting possibility is that these features are cosmo-
logical in origin. 

# La tension de Hubble

*Sources*

- [La crise cosmique de la constante de Hubble](https://www.pourlascience.fr/sd/cosmologie/la-crise-cosmique-de-la-constante-de-hubble-19032.php) - Pour la Science (2020)
- [Exploring the Hubble tension](https://cerncourier.com/a/exploring-the-hubble-tension/) - CERN Courrier (2021)
- [La tension de Hubble : la cosmologie en crise ?](https://scienceetonnante.com/2022/02/04/la-tension-de-hubble/) - Science Etonnante (2022)
- [Tensions dans le modèle cosmologique : l’espoir d’une nouvelle génération de télescopes](https://www.pourlascience.fr/sd/cosmologie/tensions-dans-le-modele-cosmologique-l-espoir-d-une-nouvelle-generation-de-telescopes-23923.php) - Pour  la Science (2022)

---



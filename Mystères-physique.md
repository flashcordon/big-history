# Table des matières

- [La cosmologie quantique](#La-cosmologie-quantique)
- [Matière noire](#Matière-noire)
- [Energie sombre](#Energie-sombre)
- [L'inflation cosmique](#Linflation-cosmique)
- [Le problème de la hiérarchie](#Le-problème-de-la-hiérarchie)
- [Le problème du lithium cosmique](#Le-problème-du-lithium-cosmique)
- [Le principe cosmologique](#Le-principe-cosmologique)
- [La tension de Hubble](#La-tension-de-Hubble)

# La cosmologie quantique

*Sources*

- [Quantum cosmology: a review](https://arxiv.org/abs/1501.04899) - Bojowald (2015)
- [Loop Quantum Cosmology: A brief review](https://arxiv.org/abs/1612.01236) - Agulo et Singh (2016)
- [Conceptual Problems in Quantum Gravity and Quantum Cosmology](https://www.hindawi.com/journals/isrn/2013/509316/) - Kiefer (2013)

---

# Matière noire

*Sources*

- [A History of Dark Matter](https://arxiv.org/pdf/1605.04909.pdf) - Berton et Hooper (2016)
- [Dark matter and cosmic structure](https://arxiv.org/abs/1210.0544) - Frenk and White (2012)
- [La matière noire, une sombre affaire](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p4.pdf) - Combes (2016)

---

# Energie sombre

*Sources*

- [Dark Energy: A Short Review](https://arxiv.org/abs/1401.0046) - Mortonson et al (2013)
- [Dark Energy Versus Modified Gravity](https://www.annualreviews.org/doi/abs/10.1146/annurev-nucl-102115-044553) - Joyce et al (2016)
- [Dark energy: A brief review](https://link.springer.com/article/10.1007/s11467-013-0300-5) - Li et al (2013)
- [Changement de rythme dans l’expansion de l’Univers : Un premier rôle pour le côté obscur](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p12.pdf) - Palanque Delabrouille (2016)
- [L'histoire de l'expansion de l'Univers dévoilée](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2022/01/refdp202271p22.pdf) - Burtin (2022)
- [Avancées de la recherche Cisaillement gravitationnel et sondage de l’Univers](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2006/01/refdp20061p5.pdf) - Mellier (2006)
- [La constante cosmologique : la plus grande erreur d’Einstein](https://books.openedition.org/cdf/9443?lang=fr) - Combes
- [Listening for the Size of the Universe](https://astro.ucla.edu/~wright/BAO-cosmology.html) - Edward Wright (2014)
- [Astronomy Jargon 101: Baryon Acoustic Oscillations](https://www.universetoday.com/153273/astronomy-jargon-101-baryon-acoustic-oscillations/) - Paul Sutter
- [BOSS Measures the Universe to One-Percent Accuracy](https://newscenter.lbl.gov/2014/01/08/boss-one-percent/) - Berkeley Lab (2014)
- [DESI lance un programme de 5 ans pour mieux comprendre l’univers](https://insidetheperimeter.ca/fr/desi-launches-five-year-quest-to-understand-the-universe/) - PI (2021)
- [La distribution des galaxies, une fenêtre sur l'Univers primordial](https://www.pourlascience.fr/sd/cosmologie/la-distribution-des-galaxies-une-fenetre-sur-l-univers-primordial-11827.php) - Pour la Science (2014)
- [What are baryonic acoustic oscillations?](https://sci.esa.int/web/euclid/-/what-are-baryonic-acoustic-oscillations-) - ESA
- [What the hell are Baryon Acoustic Oscillations?](https://medium.com/starts-with-a-bang/what-the-hell-are-baryon-acoustic-oscillations-cfee6d726538) - Ethan Siegel
- [Dark Energy Spectroscopic Instrument (DESI) Creates Largest 3-D Map of the Cosmos](https://www.ifae.es/news/2022/01/13/dark-energy-spectroscopic-instrument-desi-creates-largest-3-d-map-of-the-cosmos/) - IFAE (2022)
- [Listening to the sound of the universe](https://web.physics.ucsb.edu/~jatila/CMB-sounds/CMB/) - Jatila van der Veen

---

- In the first modern cosmological model, Einstein [1] modified his field equation of
General Relativity (GR), introducing a “cosmological term” that enabled a solution with
time-independent, spatially homogeneous matter density ρm and constant positive space
curvature. Although Einstein did not frame it this way, one can view the “cosmological
constant” Λ as representing a constant energy density of the vacuum [2], whose repulsive
gravitational effect balances the attractive gravity of matter and thereby allows a static
solution. After the development of dynamic cosmological models [3,4] and the discovery
of cosmic expansion [5], the cosmological term appeared unnecessary, and Einstein and
de Sitter [6] advocated adopting an expanding, homogeneous and isotropic, spatially flat,
matter-dominated universe as the default cosmology until observations dictated otherwise.
- By the mid-1990s, Big Bang cosmology was convincingly established, but the
Einstein-de Sitter model was showing numerous cracks, under the combined onslaught
of data from the cosmic microwave background (CMB), large scale galaxy clustering,
and direct estimates of the matter density, the expansion rate (H0), and the age of the
Universe. Introducing a cosmological constant offered a potential resolution of many of
these tensions. In the late 1990s, supernova surveys by two independent teams provided
direct evidence for accelerating cosmic expansion [8,9], establishing the cosmological
constant model (with Ωm ≈ 0.3, ΩΛ ≈ 0.7) as the preferred alternative to the Ωm = 1
scenario. Shortly thereafter, CMB evidence for a spatially flat universe [10,11], and
thus for Ωtot ≈ 1, cemented the case for cosmic acceleration by firmly eliminating the
free-expansion alternative with Ωm ≪ 1 and ΩΛ = 0. Today, the accelerating universe is
well established by multiple lines of independent evidence from a tight web of precise
cosmological measurements.
-  acceleration could arise from a more general form of “dark
energy” that has negative pressure, typically specified in terms of the equation-of-state-
parameter w = p/ρ (= −1 for a cosmological constant). Furthermore, the conclusion that
acceleration requires a new energy component beyond matter and radiation relies on the
assumption that GR is the correct description of gravity on cosmological scales. The title
of this article follows the common but inexact usage of “dark energy” as a catch-all term
for the origin of cosmic acceleration, regardless of whether it arises from a new form of
energy or a modification of GR. 

A cosmological constant is the mathematically simplest, and perhaps the physically
simplest, theoretical explanation for the accelerating universe. The problem is explaining
its unnaturally small magnitude,

An alternative
(which still requires finding a way to make the cosmological constant zero or at least
negligibly small) is that the accelerating cosmic expansion is driven by a new form of
energy such as a scalar field [13] with potential V (φ). The energy density and pressure of
the field φ(x) take the same forms as for inflationary scalar fields, given in Eq. (19.52) of
the Big Bang Cosmology article. In the limit that 1
2 ̇φ2 ≪ |V (φ)|, the scalar field acts like
a cosmological constant, with pφ ≈ −ρφ. In this scenario, today’s cosmic acceleration is
closely akin to the epoch of inflation, but with radically different energy and timescale.

Instead of introducing a new energy component, one can attempt to modify gravity
in a way that leads to accelerated expansion [14]. One option is to replace the Ricci
scalar R with a function R + f (R) in the gravitational action [15]. Other changes can
be more radical, such as introducing extra dimensions and allowing gravitons to “leak”
off the brane that represents the observable universe (the “DGP” model [16]) . The DGP
example has inspired a more general class of “galileon” and massive gravity models.
Constructing viable modified gravity models is challenging, in part because it is easy to
introduce theoretical inconsistencies (such as “ghost” fields with negative kinetic energy)
but above all because GR is a theory with many high-precision empirical successes on
solar system scales

one can search for signatures of modified gravity by comparing the
history of cosmic structure growth to the history of cosmic expansion. Within GR, these
two are linked by a consistency relation, as described below (Eq. (1.2)). Modifying
gravity can change the predicted rate of structure growth, and it can make the growth
rate dependent on scale or environment. In some circumstances, modifying gravity alters
the combinations of potentials responsible for gravitational lensing and the dynamics of
non-relativistic tracers (such as galaxies or stars) in different ways (see Sec. 19.4.7 in this
Review), leading to order unity mismatches between the masses of objects inferred from
lensing and those inferred from dynamics in unscreened environments.
At present there are no fully realized and empirically viable modified gravity theories
that explain the observed level of cosmic acceleration.

The DGP model is empirically ruled out by
several tests, including the expansion history, the integrated Sachs-Wolfe effect, and
redshift-space distortion measurements of the structure growth rate [19]. The elimination
of these models should be considered an important success of the program to empirically
test theories of cosmic acceleration. However, it is worth recalling that there was no
fully realized gravitational explanation for the precession of Mercury’s orbit prior to the
completion of GR in 1915, and the fact that no complete and viable modified gravity
theory exists today does not mean that one will not arise in the future.

The main line of empirical attack on dark energy is to measure the history of cosmic
expansion and the history of matter clustering with the greatest achievable precision
over a wide range of redshift. Within GR, the expansion rate H(z) is governed by
the Friedmann equation

In modified gravity models the growth rate of gravitational clustering may differ from
the GR prediction. A general strategy to test modified gravity, therefore, is to measure
both the expansion history and the growth history to see whether they yield consistent
results for H(z) or w(z)

The underlying goal of empirical studies of cosmic acceleration is to
address two physically profound questions:
1. Does acceleration arise from a breakdown of GR on cosmological scales or from a
new energy component that exerts repulsive gravity within GR?
2. If acceleration is caused by a new energy component, is its energy density constant
in space and time, as expected for a fundamental vacuum energy, or does it show
variations that indicate a dynamical field?
Substantial progress towards answering these questions, in particular any definitive
rejection of the cosmological constant “null hypothesis,” would be a major breakthrough
in cosmology and fundamental physics.

## Comment sonder les propriétés de l'énergie noire à l'aide de nos observations

### Les anisotropies du CMB

Cosmic Microwave Background Anisotropies: Although CMB anisotropies provide limited
information about dark energy on their own, CMB constraints on the geometry, matter
content, and radiation content of the Universe play a critical role in dark energy studies
when combined with low redshift probes. In particular, CMB data supply measurements
of θs = rs/DA,c(zrec), the angular size of the sound horizon at recombination, from the
angular location of the acoustic peaks, measurements of Ωmh2 and Ωbh2 from the heights
of the peaks, and normalization of the amplitude of matter fluctuations at zrec from the
amplitude of the CMB fluctuations themselves.

Secondary anisotropies, including the
Integrated Sachs-Wolfe effect, the Sunyaev-Zel’dovich (SZ, [23]) effect, and gravitational
lensing of primary anisotropies, provide additional information about dark energy by
constraining low-redshift structure growth.

### Les supernovae thermonucléaires

Type Ia Supernovae: Type Ia supernovae, produced by the thermonuclear explosions of
white dwarfs, exhibit 10-15% scatter in peak luminosity after correction for light curve
duration (the time to rise and fall) and color (which is a diagnostic of dust extinction).
Since the peak luminosity is not known a priori, supernova surveys constrain ratios of
luminosity distances at different redshifts. If one is comparing a high redshift sample to
a local calibrator sample measured with much higher precision (and distances inferred
from Hubble’s law), then one essentially measures the luminosity distance in h−1Mpc,
constraining the combination hDL(z).

### Les oscillations acoustiques des baryons

- La méthode la plus précise actuellement utilisée pour mesurer l'histoire de l'expansion de l'univers fait appel à d'étranges motifs imprimés dans le ciel que l'on peut détecter quand on observe la distribution des galaxies dans l'univers avec une grande précision. Quand on les projette sur la sphère céleste, ces motifs ressemblent à des petits cercles entourés d'immenses anneaux, similaires à ceux créés par un caillou jeté dans un étang. Ces anneaux sont difficiles à détecter et se chevauchent, mais le plus surprenant c'est qu'ils ont tous la même taille caractéristique (que les astronomes appellent ***échelle acoustique***).
- Ces motifs sont dus à un phénomène physique assez insolite : les  ***oscillations acoustiques des baryons*** (ou BAO, pour Baryonic Acoustic Oscillations)
- La signature des BAO dans la répartition des galaxies a été mise en évidence pour la première fois en 2005 par deux équipes indépendantes d'astronomes associés aux relevés SDSS (Sloan Digital Sky Survey, au Nouveau Mexique) et 2dFGRS (2dF Galaxy Redshift Survey, en Australie).
> Ils ont montré que les anneaux ont une taille caractéristique de 500 millions d'années-lumière (150 Mpc) dans l'univers actuel. Autrement dit, si on observe une galaxie quelque part dans l’univers, on a (un peu) plus de chance de trouver une autre galaxie située à 500 millions d’années-lumière d’elle qu’une galaxie à 400 ou à 600 millions d’années-lumière. 
- Pour comprendre l'origine de ces motifs, il faut revenir aux conditions de l'univers primordial.
> Pendant les premières centaines de milliers d'années qui ont suivi le Big Bang, l'univers était un plasma chaud composé de photons, de protons, d'électrons et de matière noire qui interagissaient sans cesse. Ce plasma avait beau être extrêmement homogène, de petites surdensités de matière noire apparaissaient quand même en permanence. Attirée par la gravité de ces petits amas de matière noire, la matière ordinaire (dite "baryonique") tentait de s'accumuler, mais le rayonnement ambiant avait tendance à s'opposer à cette accumulation (à noter que la matière noire n'interagit pas avec la lumière, donc elle peut former des amas sans problème). Ces deux processus antagonistes ont généré des ondes de pressions qui se déplacent à la vitesse du son (170 000 km/s) dans ce plasma ultra-dense. Ces ondes de pression sont littéralement des ondes sonores de différentes intensités qui se déplacent dans toutes les directions. Dans cette cacophonie, certaines longueurs d'onde sont plus amplifiées que d'autres. La "note" la plus amplifiée possède une longueur d'onde d'un million d'années-lumière environ, ce qui représente une fréquence 48 octaves plus basses que la note la plus grave sur un piano ! Mais lorsque le cosmos atteint l’âge de 380 000 ans, la température du rayonnement primordial passe sous la barre des 3000 K (2700°C environ), et les électrons peuvent enfin se lier aux protons pour former les premiers atomes. Le plasma primordial est devenu un gaz d’atomes électriquement neutres. La lumière cesse alors d’interagir avec la matière et peut se balader dans l’espace à l’infini. Le cosmos, initialement opaque, devient alors transparent. C’est ***l’époque de la recombinaison***. À ce moment, la cacophonie s’arrête. Les ondes sonores sont soudainement figées à une certaine distance caractéristique des surdensités de matière noire. Ces surdensités sont les graines à partir desquelles se formeront les premières galaxies et autres grandes structures de l’univers. Les empreintes des ondes sonores en forme de coquilles sphériques, quant à elles, vont se mettre à enfler au fil de l’expansion cosmique, à la matière d’un cercle dessiné à la surface d’un ballon de baudruche que l’on gonfle. 13,8 milliards d’années plus tard, on pourra les observer sous la forme d’un léger surnombre de galaxies au niveau  de ces coquilles, et d’une très légère tendance de ces galaxies à être alignées le long de leur surface.
- Le phénomène des BAO lie de manière quantitative les anisotropies observées dans le CMB et les motifs d'anneaux observés dans la distribution spatiale des galaxies
> En effet, les BAO sont responsables des pics observés dans le ***spectre de puissance*** (à définir) du CMB. Ces derniers correspondent aux longueurs d'onde sonores les plus amplifiées dans le plasma primordial.
- En mesurant la taille de ces anneaux à différentes époques de l’univers, à l’aide de relevés astronomiques très fins, et en liant ces anneaux aux conditions initiales fournies par les anisotropies du CMB, on peut donc reconstruire l’histoire de l’expansion cosmique avec une grande précision.
- Les BAO servent de ***règle standard*** aux astronomes, c’est-à-dire que ces anneaux sont des objets dont on connaît la taille réelle. Quand on mesure leur diamètre apparent dans le ciel, comme on connaît leur taille on peut donc déterminer leur distance.
- Le grand relevé astronomique BOSS (Baryon Oscillation Spectroscopic Survey), qui fait partie du 3e relevé SDSS, et dont les données ont été acquises entre 2009 et 2014, constitue à ce jour la mesure la plus précise des anneaux produits par les BAO. À travers l’étude spectroscopique de plus d’un million de galaxies, les astronomes ont pu mesurer ces « règles standard » avec une précision d’1% sur une période qui couvre les 6 derniers milliards d’années de l’évolution cosmique (il y a une vingtaine d’années à peine, cette précision d’était que de 50% !).
- Un instrument fondamental pour détecter la signature des BAO dans les années à venir est DESI (Dark Energy Spectroscopic Instrument, "instrument spectroscopique pour l'énergie noire"), installé sur le télescope Mayall de 4 mètres de l'Observatoire National de Kitt Peak (Arizona) et capable de mesurer 5000 spectres de galaxies simultanément
> En 2021, une collaboration internationale de 50 institutions dirigée par le Berkeley Lab a débuté un programme d'observation de 5 ans avec DESI qui devrait l'amener à cartographier la position de plus de 30 millions de galaxies dans le ciel de l'hémisphère Nord, dont près de 2,4 millions de quasars. Couvrant les 11 derniers milliards d’années de l’histoire de l’univers, il pourra atteindre des profondeurs que BOSS ne pouvait pas atteindre, et permettra l'étude la plus fine de l'histoire de l'expansion cosmique jamais produite.      	      	      

*(Illustration des BAO. Crédit: Gabriela Secara, Institut Périmètre [Source](https://insidetheperimeter.ca/fr/desi-launches-five-year-quest-to-understand-the-universe/))*

![BAO](https://insidetheperimeter.ca/wp-content/uploads/2021/05/test-DESI_Baryon_Acoustic_Oscillation.png)

### Le cisaillement gravitationnel

Weak Gravitational Lensing: Gravitational light bending by a clustered distribution
of matter shears the shapes of higher redshift background galaxies in a spatially
coherent manner, producing a correlated pattern of apparent ellipticities. By studying
the weak lensing signal for source galaxies binned by photometric redshift (estimated
from broad-band colors), one can probe the history of structure growth.

Clusters of Galaxies: Like weak lensing, the abundance of massive dark matter halos
probes structure growth by constraining σ8Ωα
m, where α ≈ 0.3–0.5. These halos can be
identified as dense concentrations of galaxies or through the signatures of hot (107–108 K)
gas in X-ray emission or SZ distortion of the CMB.

Direct Determination of H0: The value of H0 sets the current value of the critical density
ρc = 3H2
0 /8πGN, and combination with CMB measurements provides a long lever arm
for constraining the evolution of dark energy. The challenge in direct H0 measurements is
establishing distances to galaxies that are far enough away that their peculiar velocities
are small compared to the expansion velocity v = H0d. This can be done by building
a ladder of distance indicators tied to stellar parallax on its lowest rung, or by using
gravitational lens time delays or geometrical measurements of maser data to circumvent
this ladder

A still more ambitious period begins late in this decade and continues through the
2020s, with experiments that include the Dark Energy Spectroscopic Instrument (DESI),
the Subaru Prime Focus Spectrograph (PFS), the Large Synoptic Survey Telescope
(LSST), and the space missions Euclid and WFIRST (Wide Field Infrared Survey
Telescope). DESI and PFS both aim for major improvements in the precision of BAO,
RSD, and other measurements of galaxy clustering in the redshift range 0.8 < z < 2,
where large comoving volume allows much smaller cosmic variance errors than low
redshift surveys like BOSS. LSST will be the ultimate ground-based optical weak lensing
experiment, measuring several billion galaxy shapes over 20,000 deg2 of the southern
hemisphere sky, and it will detect and monitor many thousands of SNe per year. Euclid
and WFIRST also have weak lensing as a primary science goal, taking advantage of the
high angular resolution and extremely stable image quality achievable from space. Both
missions plan large spectroscopic galaxy surveys, which will provide better sampling at
high redshifts than DESI or PFS because of the lower infrared sky background above
the atmosphere. WFIRST is also designed to carry out what should be the ultimate
supernova cosmology experiment, with deep, high resolution, near-IR observations and
the stable calibration achievable with a space platform.

Performance forecasts necessarily become more uncertain the further ahead we
look, but collectively these experiments are likely to achieve 1–2 order of magnitude
improvements over the precision of current expansion and growth measurements, while
simultaneously extending their redshift range, improving control of systematics, and
enabling much tighter cross-checks of results from entirely independent methods. The
critical clue to the origin of cosmic acceleration could also come from a surprising
direction, such as laboratory or solar system tests that challenge GR, time variation
of fundamental “constants,” or anomalous behavior of gravity in some astronomical
environments. Experimental advances along these multiple axes could confirm today’s
relatively simple, but frustratingly incomplete, “standard model” of cosmology, or they
could force yet another radical revision in our understanding of energy, or gravity, or the
spacetime structure of the Universe.

Lentilles gravitationnelles

La relativité générale proposée par Einstein
en 1915 est une théorie de la gravitation dont
la validité est aujourd’hui confortée par l’accord
entre ses prédictions et tous les tests expéri-
mentaux. La gravitation étant une interaction à
longue portée, elle constitue la clef de voûte de
notre description de l'Univers à grande échelle
et sert de cadre général à l'interprétation des
observations astronomiques. Une de ses pré-
dictions les plus remarquables est la déflexion
des rayons lumineux lorsqu'ils croisent des
concentrations de matière au cours de leur
propagation. Ce phénomène de “lentille
gravitationnelle” illustre qu'en relativité générale
les photons suivent des géodésiques d'un
espace-temps courbe dont les déformations
sont localement déterminées par les concentra-
tions de matière. En modifiant la propagation
des photons, les objets astronomiques nous
apparaissent déplacés sur le ciel par rapport à
leur position réelle, en l’absence de déflexion
gravitationnelle (voir figure 1). La mesure de
la déflexion de rayons lumineux des étoiles
observées près du bord du Soleil lors de
l'éclipsede1919compteparmilessuccèshistoriques
des prédictions d'Einstein.

Comme l'avait perçu
l'astronome Fritz Zwicky dès les années 1930,
c'est ici que se trouve l'intérêt astrophysique
des lentilles gravitationnelles : elles peuvent
nous servir de véritable balance cosmique, une
sonde à distance permettant de mesurer la
masse contenue dans les condensations de
matière distribuées dans l'univers. Ce qui est
remarquable, c'est que pour mesurer les masses
il suffit de connaître les distances de la source
et de la lentille ainsi que leurs positions apparentes
sur le ciel. À aucun moment, l'état dynamique
de la lentille ou les propriétés thermodynamiques
des gaz la constituant n'interviennent. C'est un
effet très pur, qui est parfaitement décrit par les
équations de la relativité générale.
L'intuition de Fritz Zwicky fut confirmée
au cours des trente dernières années, après la
découverte du premier quasar à image multiple
(1979), puis du premier arc gravitationnel
(1987). Dans ces deux cas, les astronomes ont
découvert des configurations spécifiques de len-
tilles gravitationnelles reflétant certaines de leurs
remarquables propriétés : amplification de la
lumière de la source, démultiplication de ses
images, altération de sa forme apparente.

grâce aux défor-
mations gravitationnelles des galaxies lointaines,
il devient désormais possible de reconstruire la
distribution de masse d’objets astronomiques
aussi complexes que les amas de galaxies.

Une analyse
statistique permet ainsi de reconstruire le profil
de la distribution de masse du déflecteur, en
produisant une cartographie des « distorsions
gravitationnelles » qui répond à celle de la
distribution de la matière noire. Cette matière
noire peut donc être « vue », comme la limaille
de fer sur une feuille de papier « voit » le champ
magnétique d'un aimant placé au-dessous.

Une des options possibles pour caractériser
l'énergie noire est d'adopter une attitude mini-
maliste, même si elle est incomplète, en supposant
que cette composante se comporte comme un
fluide parfait. Comme tout indique que notre
univers est homogène et isotrope à très grande
échelle, cette hypothèse est somme toute plutôt
raisonnable. Dans ce cas, l’équation d'état
reliant sa pression P à sa densité d’énergie ρ
peut s'écrire sous la forme P = w ρ, comme
celle de la matière noire ordinaire (w = 0) ou
celle des photons (w = 1/3). Pour l'énergie
noire, l'éventail des valeurs de ce paramètre est
assez vaste et il pourrait même évoluer au
cours du temps. S'il s'agit cependant d'une
constante cosmologique, alors w vaut -1 et est
constant.S'il s'agissait par contre d'une composante
de type cordes cosmiques, alors on aurait
w = -1/3. Dans tous les cas, la distribution des
fluctuations de densité, caractérisée par le
champ de distorsion gravitationnelle, sera
différente suivant la valeur de w.
Tout comme la matière noire, certaines de
ces propriétés peuvent donc être décryptées
au travers du spectre de fluctuations tracé
directement par les effets de distorsion gravita-
tionnelle. Mais contrairement à l'étude d'un
amas de galaxies, il faut cette fois analyser les
déformations gravitationnelles accumulées par
toutes les structures traversées le long d'une
ligne de visée, puis conduire l'analyse sur une très
vaste partie du ciel pour explorer les fluctuations
à chaque échelle et reconstruire le spectre
complet de densité. Une fois recomposé, il est
alors comparé aux prédictions théoriques
d'une panoplie de scenarii cosmologiques.

Si l’on exclut Einstein qui avait en son
temps suggéré l’hypothèse d’une constante
cosmologique pour de mauvaises raisons
(il cherchait à rendre l’Univers statique),
les chercheurs se sont longtemps accordés
à dire que les observations de notre cosmos
pouvaient toutes être interprétées en ne
faisant appel qu’à de la matière et à une
faible quantité de rayonnement. L’expansion
de notre Univers était connue depuis 1929,
lorsque l’astronome Edwin Hubble constata
que les galaxies s’éloignent toutes les unes
des autres. Sous l’effet attractif de la gravité,
cette expansion de l’espace semblait ne
pouvoir que ralentir. Cette conviction est
brusquement ébranlée en 1998, quand
deux équipes indépendantes étudient des
supernovae lointaines précisément dans le
but de mesurer ce ralentissement et
démontrent, sans ambiguïté, que l’expansion
de notre Univers allait au contraire en
s’accélérant.

Tentons de comprendre ce dont il s’agit.
Les supernovae thermonucléaires ou SNIa
(par opposition aux supernovae gravita-
tionnelles ou SNII, plus fréquentes, issues
d’étoiles massives s’effondrant sur elles-
mêmes), trouvent leurs précurseurs dans
les systèmes stellaires constitués d’une
étoile peu dense, telle une géante rouge,
et d’une naine blanche, petit astre ayant
achevé sa phase de combustion nucléaire.
La supernova résulte de l’explosion cata-
clysmique de la naine blanche lorsque sa
masse devient trop importante, suite à
l’accrétion de la matière provenant de
l’étoile compagnon, pour que l’équilibre
gravité-pression soit maintenu. L’explosion
se produisant à une masse prédéfinie de
1,4 fois la masse du Soleil, la luminosité
émise est rigoureusement déterminée et la
mesure de la luminosité observée est donc
un indicateur direct de la distance de l’objet
(d’après la loi de décroissance du flux
comme l’inverse du carré de la distance).
Par ailleurs, des observations spectrosco-
piques permettent de déterminer le
décalage spectral z = (λreçu - λémis)/λémis
entre les longueurs d’onde λ reçue et
émise, communément appelé “redshift”
de la supernova. En effet, conséquence
directe de l’expansion de l’Univers, les raies
d’absorption caractéristiques des éléments
chimiques qui composent étoiles et galaxies
apparaissent à des longueurs d’onde décalées
vers le rouge par rapport à celles qu’on leur
connaît en laboratoire, de façon analogue
à l’effet Doppler pour le son lors du
déplacement relatif entre émetteur et
récepteur. Pour les supernovae les plus
proches, on retrouve la fameuse loi de
Hubble, relation linéaire entre redshift et
distance d d’un astre, z = H0 d/c, où H0
est la constante de Hubble et c la vitesse de
la lumière. Par conséquent, plus un objet est
éloigné de nous, plus son redshift est grand
et plus la lumière qui nous en parvient a
été émise tôt dans l’histoire de l’Univers.
À plus grande distance, cette relation n’est
plus linéaire, et l’écart à la linéarité est dicté
par le contenu énergétique de l’Univers
(matière, rayonnement...). Le paramètre H(z)
qui décrit le taux d’expansion de l’Univers
n’est plus constant mais dépend du redshift ;
H0 ≃ 70 km/s/Mpc est la valeur qu’il prend
aujourd’hui. Conformément à l’intuition,
un Univers composé exclusivement de
matière, freiné par la gravitation, verrait son
taux d’expansion décroître avec le temps.
Contre toute attente, les observations de
supernovae ont toutefois indiqué une
accélération et non une décélération de cette
expansion, impliquant une révision en pro-
fondeur de notre compréhension du cosmos.

En cosmologie, c’est la révolution, et
trois prix Nobel (Brian Schmidt et Adam
Riess pour le projet High-z Supernova
Search Team, et Saul Perlmutter pour le
Supernova Cosmology Project [1]) viennent
immédiatement couronner cette surprenante
découverte. Mais tout reste encore à com-
prendre. Faut-il développer une nouvelle
théorie de la gravité qui ne se réduirait à la
relativité générale que dans certaines
limites particulières, ou considérer une forme
d’énergie nouvelle, baptisée « énergie noire »,
de pression négative et agissant sur l’espace
comme on pourrait imaginer que le ferait
une gravité répulsive ? Cette énergie noire
est-elle de densité constante pendant toute
l’histoire de l’Univers, à l’instar de la
constante cosmologique qu’Einstein avait
introduite dans ses équations, ou est-ce une
forme plus complexe d’énergie ?

À elle seule,
l’énergie noire est aujourd’hui invoquée pour
représenter 73% du contenu énergétique
de notre Univers

L’information cosmologique la plus
précise se trouverait dans l’étude de la
répartition de la matière noire, puisque
celle-ci est bien plus abondante que la
matière baryonique. Étant cependant, par
définition, non lumineuse (c’est bien pour
cela qu’elle est ainsi dénommée), on ne peut
la cartographier directement. Une approche
indirecte consiste à reconstruire les contours
de densité de la matière noire à partir des
déformations gravitationnelles qu’elle
engendre sur des galaxies en arrière-plan :
c’est le principe des lentilles gravitation-
nelles

Une autre approche est de se focaliser sur
les astres visibles. Comme le confirment
es simulations numériques de notre
Univers, matière baryonique et matière noire
sont distribuées de façon comparable. On
peut donc étudier l’une pour comprendre
l’autre.

Les chercheurs choisirent dans un
premier temps de se concentrer sur des
galaxies anciennes, rouges et particulièrement
lumineuses, les LRG (pour “luminous red
galaxies”). Ces LRG se trouvent au cœur
des amas de galaxies et présentent l’intérêt
d’être de bons marqueurs de la densité de
matière. Leur redshift, indicateur de distance
comme nous l’avons vu plus haut, est
déterminé à partir de leur spectre lumineux.

La carte en 3D
peut ainsi être dressée au fil des observations.
En 2005, lors des premiers pas dans cette
quête, la carte obtenue à partir de cinquante
mille galaxies a conduit à la première indi-
cation d’une distance de séparation privilégiée
de l’ordre de 150 megaparsecs (1 parsec,
ou pc, équivaut à 3,3 années-lumière)
dans la répartition des galaxies
D’où vient cette
distance privilégiée ? Pourquoi les galaxies
ne se forment-elles pas aléatoirement dans
l’Univers ? Que nous apprend ce résultat
sur la matière et l’énergie noires ?

Des ondes sonores qui font vibrer
les baryons (ou BAO, pour Baryon Acoustic Oscillations)
Dans le plasma chaud et dense de l’Univers
primordial, l’attraction gravitationnelle des
régions les plus denses est compensée par
la pression radiative du gaz ionisé. Cette
pression est si importante que les pics
de surdensité, que l’on pense issus de
fluctuations quantiques dans les premières
secondes de l’Univers, se propagent en
oscillant telles des ondes sonores au lieu de
croître sous l’effet de la gravité. En raison
de son expansion, l’Univers se refroidit au
fil du temps et à l’âge de 380 000 ans, sa
température chute en dessous du seuil
minimal pour le maintenir à l’état de plasma
ionisé : les électrons s’associent aux protons
pour former des atomes d’hydrogène, et le
plasma initial se mue brutalement en un
milieu neutre. La lumière qui diffusait
inlassablement sur les électrons est à
présent libre de se propager. Détectée par
le satellite Planck, elle nous apporte

l’image des fluctuations de température que
les oscillations du plasma ont engendrées.
La distance sur laquelle les ondes acoustiques
ont eu le temps de s’étendre avant de se
retrouver figées est déterminée par le pro-
duit de la vitesse du son dans le plasma
(vitesse de la lumière divisée par racine
carrée de 3) par l’âge de l’Univers à ce
moment-là (380 000 ans), soit une distance
d’environ 150 Mpc aujourd’hui, compte
tenu de l’expansion de l’Univers. Ainsi,
autour de chaque pic initial de surdensité
essentiellement composé de matière noire,
donc qui ne se propage pas, se trouve un
petit excès concentrique causé par l’onde
de photons et de matière baryonique qui
en est issue. Par la suite, chacune de ces
surdensités est plus à même d’attirer
gravitationnellement la matière environ-
nante, si bien qu’aujourd’hui ces régions
sont celles dans lesquelles il est plus
probable de trouver des galaxies (fig. 3).
L’effet des oscillations du plasma, ou BAO,
est donc aujourd’hui observable dans la
distribution des structures de l’Univers.
C’est pourquoi les physiciens recherchaient
son empreinte dans la distribution des LRG
notamment. Pour la repérer, ils ont réper-
torié la distance séparant chaque galaxie de
toutes les autres. Dans l’histogramme ainsi
obtenu, une séparation privilégiée, à la posi-
tion attendue de 150 Mpc, est clairement
visible (fig. 2a), avec une signification
statistique excédant 7σ.

À l’instar d’un motif
imprimé sur un tissu extensible, la mesure
de cette distance étalon à différentes
époques est un marqueur de l’étirement
subi par le tissu, autrement dit de l’expan-
sion de l’Univers, dont le taux dépend de
la quantité et de la nature de ses diverses
composantes et notamment de la part
relative de matière et d’énergie noire.
Pour les LRG, par exemple, 10% d’énergie
noire en plus diminuerait de 6% le taux
d’expansion de l’Univers H(z) au redshift
de 0,57 où ces galaxies sont observées, et
augmenterait de 4% la taille de la distance
BAO. Obtenue aujourd’hui avec une
précision de l’ordre du pourcent, la mesure
des BAO permet de contraindre fortement
la quantité d’énergie noire dans l’Univers.

Les chercheurs ont décidé de mesurer
l’évolution du cosmos, non seulement
dans la répartition de galaxies rouges et
vieilles, mais également dans celle de
galaxies jeunes en pleine activité stellaire,
de quasars ultra-lumineux, voire même dans
la répartition des nuages ténus d’hydrogène

L’histoire de l’expansion de l’Univers ne
se résume pas à l’accélération récente
découverte grâce aux supernovae et aux
mesures des ondes baryoniques acoustiques.
La phase d’accélération ne remonte en effet
qu’aux six derniers milliards d’années
environ, alors que durant les premiers huit
milliards d’années ou presque, l’expansion,

freinée par la gravitation, devait ralentir
comme prévu dans un Univers alors
majoritairement composé de matière. La
mesure précise de l’évolution de l’expansion
de l’Univers avec le temps étant un formi-
dable outil pour traquer la contribution de
l’énergie noire et déterminer l’époque à
laquelle elle devient dominante, le SDSS
étudie les BAO à diverses époques. Bien
que très brillantes, les LRG ne sont toute-
fois visibles que sur les derniers six milliards
d’années-lumière environ. Quels astres
vont nous permettre d’explorer l’Univers
plus jeune ?
Une avancée considérable a été accomplie
en faisant appel à des quasars, objets parmi
les plus lumineux de l’Univers : il s’agit de
galaxies abritant en leur cœur un trou noir
de plusieurs millions, voire plusieurs
milliards de fois la masse du Soleil. Le
disque d’accrétion qui gravite autour du trou
noir central est chauffé à des températures
telles que la lumière émise, excédant celles
des galaxies les plus brillantes, est visible à
travers tout l’Univers. Le flux perçu en
provenance du quasar est encore détectable
par SDSS, même après s’être répandu dans
l’Univers pendant plus de douze milliards
d’années. À cette époque reculée, les quasars
sont insuffisamment nombreux pour être
utilisés en tant que marqueurs de la matière,
comme cela avait été fait pour les LRG.
Cependant, jouant le rôle de puissants
phares cosmiques, les quasars vont illuminer
le milieu intergalactique, et notamment
les nuages ténus d’hydrogène gazeux qui
s’y trouvent. De densité extrêmement
faible, typiquement un million de fois plus
petite que celle d’une galaxie (on trouve
environ 0,25 atome d’hydrogène par mètre
cube aujourd’hui, ce qui, l’Univers étant plus
dense par le passé, correspond à quelques
atomes par mètre cube à l’époque ici
concernée), ce brouillard d’hydrogène n’en
est pas moins la forme la plus fréquente
sous laquelle se trouve la matière baryonique.
Porte-t-il lui aussi l’empreinte des ondes
acoustiques de l’Univers primitif, ou ce signe
distinctif est-il réservé aux structures les
plus massives ?
Ce sont de nouveau les spectres mesurés
par SDSS qui vont nous apporter la
réponse. La lumière émise par le quasar est
plus ou moins absorbée selon la densité
d’hydrogène neutre intergalactique qui se
trouve sur sa trajectoire. L’absorption se
produit localement à la longueur d’onde λ
caractéristique de la transition électronique
la plus courante de l’hydrogène (appelée
transition Lyman-α, pour laquelle
λ = 121,6 nm), mais le redshift associé au
site où cette absorption a lieu imprime le
spectre à une longueur d’onde d’autant
plus grande que le nuage est lointain. Le
spectre du quasar, quand il est finalement
observé sur Terre, comporte ainsi une
succession de pics d’absorption, nommée
« forêt Lyman-α » qui retrace les variations
de densité rencontrées sur la ligne de visée

Le projet SDSS a mesuré les spectres
de plus de 170 000 quasars. L’analyse de
ces absorptions a permis de réaliser une
carte 3D de l’hydrogène dans l’Univers tel
qu’il était un à trois milliards d’années après
le Big Bang. Malgré la faiblesse du signal
dans cet Univers si jeune et dans un milieu
aussi dilué que le milieu intergalactique,
une équipe internationale, emmenée par
des chercheurs français du CEA (Saclay) et
du CNRS, a réussi à mettre en évidence
dans cette carte la même empreinte de la
règle étalon (fig. 2b) que celle identifiée
avec les LRG [4]. Comme illustré sur la
figure 5, les résultats confirment pour la
première fois que l’Univers a bien d’abord
connu une époque de décélération, lorsque
la matière était bien plus abondante que
l’énergie noire, suivie de la phase d’accé-
lération récente.

Pour pro-
gresser sur le dossier de l’énergie noire, il
va falloir faire appel à une autre génération
de projets, au sol (avec DESI ou LSST) ou
dans l’espace (avec Euclid).
Doté cette fois d’un télescope de 4 m de
diamètre installé au Kitt Peak, en Arizona
aux USA, et d’un tout nouveau spectro-
graphe capable de mesurer simultanément
les spectres de 5000 objets, le projet DESI
est l’ultime traque de l’énergie noire
depuis un observatoire terrestre. Équipé
de plus d’un robot pour positionner
automatiquement les fibres optiques au
plan focal du télescope, DESI peut obtenir
une nouvelle série de spectres toutes les
20 minutes. Les premières observations
débuteront au cours de l’année 2019. Et
c’est au total plus de vingt millions de
galaxies et de quasars que les chercheurs
auront à leur disposition après cinq années
de fonctionnement, pour retracer l’histoire
de l’énergie noire et de la matière au cours
des douze derniers milliards d’années de
l’Univers. Pour la première fois, l’époque
à laquelle l’énergie noire prend le dessus
sur la matière, lorsque l’Univers était âgé
d’environ sept milliards d’années, pourra
être étudiée avec précision (fig. 5).
Depuis l’espace, le spectrographe infra-
rouge du satellite Euclid (programme de
l’agence spatiale européenne ESA), dont
le lancement est prévu en 2020, poussera
encore plus loin les limites des grands
relevés 3D de galaxies. Euclid observera
plus de 30 millions de galaxies, jusqu’à la
lisière où les forêts Lyman-α des quasars
observés par DESI prendront la relève.
Enfin, le LSST, un nouveau télescope de
8 m de diamètre, installé sur le Cerro Pachòn
au Chili, sera pleinement opérationnel à
partir de 2022. Il étudiera les oscillations
baryoniques par une méthode différente.
Le redshift sera obtenu à partir du flux de
lumière mesuré dans six filtres allant de
l’UV à l’infrarouge proche. La moins
bonne détermination du redshift par cette
approche est compensée par un bien plus
grand nombre de galaxies observées, soit
plusieurs dizaines de milliards à la fin du
projet.





# L'inflation cosmique

# Le problème de la hiérarchie

*Sources*

- [The Hierarchy Problem: why the Higgs has a snowball’s chance in hell](https://www.quantumdiaries.org/2012/07/01/the-hierarchy-problem-why-the-higgs-has-a-snowballs-chance-in-hell/) - Quantum Diaries
- [The Hierarchy Problem](https://profmattstrassler.com/articles-and-posts/particle-physics-basics/the-hierarchy-problem/) - Matt Strassler 
- [The Higgs, The Hierarchy Problem, and the LHC](https://www2.physics.ox.ac.uk/sites/default/files/2014-11-24/higgs_lhc_jmr_nov14_pdf_93873.pdf) - John March-Russell
- [The Mystery of the Higgs Boson's Mass](https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5idXp6c3Byb3V0LmNvbS8xMTYyNjEzLnJzcw/episode/QnV6enNwcm91dC04OTE4NTUw) - Why this universe
- [Naturalness after the Higgs](https://cerncourier.com/a/naturalness-after-the-higgs/) - CERN
- [A Deepening Crisis Forces Physicists to Rethink Structure of Nature’s Laws](https://www.quantamagazine.org/crisis-in-particle-physics-forces-a-rethink-of-what-is-natural-20220301/) - Quanta Magazine
- [The Dawn of the Post-Naturalness Era](https://arxiv.org/abs/1710.07663) - Giudice (2017)
- [A Lecture on the Hierarchy Problem and Gravity](https://cds.cern.ch/record/2120792/files/CERN-2013-003-p145.pdf) - Dvali (2013) 
- [The Higgs & the Hierarchy Problem](https://www.youtube.com/watch?v=iywSF7BGhyU) - Anna Barth
- [Big Mysteries: The Higgs Mass](https://www.youtube.com/watch?v=IjCypYnBYwQ) - Fermilab
- [Naturally Speaking: The Naturalness Criterion and Physics at the LHC](https://arxiv.org/abs/0801.2562) - Giudice (2008)
- [Histoire de la cosmologie](https://cosmology.education/booklet/booklet.pdf) - Lucas Gautheron
- [Naturalness: A Snowmass White Paper](https://arxiv.org/abs/2205.05708) - Craig (2022)
- ["Particle Physics: The Higgs Boson and Beyond" by Andreas Hoecker (CERN)](https://youtu.be/XX4pL7pwl7w) - SLAC (2012)
- ["The Once and Future Higgs" by Prof. Nathaniel Craig (University of California, Santa Barbara)](https://youtu.be/GduJt1V2eNk) - TIFR (2021)

---

- Le modèle standard est constitué de 24 particules fondamentales.
- Le boson de Higgs joue un rôle fondamental dans le modèle standard
- Le boson de Higgs brise la symétrie électrofaible et donne sa masse aux bosons et aux fermions
- Les particules interagissent avec le champ de Higgs (qui remplit tout l'espace-temps), ce qui réduit leur vitesse. Et plus elles interagissent fortement, plus leur masse est grande (c'est proportionnel).
- Découvert en 2012 par le LHC, un accélérateur de particules de 27km de circonférence
- On peut sonder la composition de la matière de plus en plus profondément à mesure que l'on fait des collisions de plus en plus énergétiques
- Dans le modèle standard, le higgs est "le seul scalaire fondamental (tous les autres scalaires sont des états composites)"
- Un des problèmes les plus importants de la physique contemporaine
- coincidence spectaculaire
- Boson de Higgs prédit en 1954
- En une phrase : la masse observée du boson de Higgs est de 125 GeV (125 fois la masse du proton ?). Pourtant, des corrections quantiques issues des interactions avec d'autres particules du modèle standard prédisent une masse 17 ordres de grandeur plus grande (au niveau de la masse de Planck). C'est le problème de la hiérarchie électrofaible.
- The Higgs boson plays a key role in the Standard Model: it is related to the unification of the electromagnetic and weak forces, explains the origin of elementary particle masses
- Higgs—the last piece of the Standard Model
- Hierarchy problem. This is often ‘explained’ by saying that quantum corrections want to make the Higgs much heavier than we need it to be… say, 125-ish GeV. 
- The Higgs has a snowball’s chance in hell of having a mass in that ballpark.
- If you put a glass of water in a really hot place—you expect it to also become really hot, maybe even to off into steam.  It would be really surprising if we put an ice cube in a hot oven and 10 minutes later it had not melted. This is because the ambient thermal energy is expected to be transferred to the ice cube by the energetic air molecules bouncing off it. Sure, it is possible that the air molecules just happen to bounce in a way that doesn’t impart much thermal energy—but that would be ridiculously improbable, as we learn in thermodynamics.
- The Higgs is very similar: we expect its mass to be around 125 GeV (not too far from W and Z masses), but ambient quantum energy wants to make its mass much larger through interactions with virtual particles. While it is possible that the Higgs stays light without any additional help, it’s ridiculously improbable, as we learn from quantum physics.
- the Standard Model really, really wants the Higgs to be around the 100 GeV scale. This is because it needs something to “unitarize longitudinal vector boson scattering.” It needs to have some Higgs-like state accessible at low energies to explain why certain observed particle interactions are well behaved.
- The Hierarchy problem has been the main motivation for new physics at the TeV scale for over two decades. 
- it is possible that the Higgs mass is 125 GeV due to some miraculous almost-cancellation that set it to be in just the right ballpark to unitarize longitudinal vector boson scattering. But such miracles are rare in physics without any a priori explanation. The electron mass is an excellent example. There are some apparent (and somewhat controversial) counter-examples: the cosmological constant problem is a much more severe ‘fine-tuning’ problem which may be explained anthropically rather than through more fundamental principles.
-  What are the possible ways to solve the Hierarchy problem?
- There are two main directions that most people consider:
> * Supersymmetry. Recall in our electron analogy that the solution to the ‘electron mass hierarchy problem’ was that quantum mechanics doubled the number of particles: in addition to the electron, there was also a positron. The virtual electron–positron contributions solved the problem by smearing out the electric charge. Supersymmetry is an analogous idea where once again the set of particles is doubled, and in doing so the loop contributions of one particle to the Higgs are cancelled by the loop contributions of its super-partner. Supersymmetry has deep connections to an extension of space-time symmetry since it relates matter particles to force particles.
> * Compositeness/extra dimensions. The other solution is that maybe our description of physics breaks down much sooner than the Planck scale. In particular, maybe at the TeV scale the Higgs no longer behaves like a scalar particles, but rather as a bound state of two fermions. This is precisely what happens with the mesons: even though the pion is a scalar, there is no pion ‘hierarchy problem’ because as you probe smaller distances, you realize the pion is actually a bound state of two quarks and it starts behaving as such. One of the beautiful developments of theoretical physics in the 1990s and early 2000s was the realization that this is precisely what is being described by theories of extra dimensions through the so-called holographic principle.
- An important feature of nature that puzzles scientists like myself is known as the hierarchy, meaning the vast discrepancy between aspects of the weak nuclear force and gravity. There are several different ways to describe this hierarchy, each emphasizing a different feature of it. Here is one:
- The mass of the smallest possible black hole defines what is known as the Planck Mass. 
-     The masses of the W and Z particles, the force carriers of the weak nuclear force, are about 10,000,000,000,000,000 times smaller than the Planck Mass. Thus there is a huge hierarchy in the mass scales of weak nuclear forces and gravity.
- When faced with such a large number as 10,000,000,000,000,000, ten quadrillion, the question that physicists are naturally led to ask is: where did that number come from? It might have some sort of interesting explanation.
- But while trying to figure out a possible explanation, physicists in the 1970s realized there was actually a serious problem, even a paradox, behind this number. The issue, now called the hierarchy problem, has to do with the size of the non-zero Higgs field, which in turn determines the mass of the W and Z particles.
- The non-zero Higgs field has a size of about 250 GeV, and that gives us the W and Z particles with masses of about 100 GeV. But it turns out that quantum mechanics would lead us to expect that this size of a Higgs field is unstable, something like (warning: imperfect analogy ahead) a vase balanced precariously on the edge of a table. With the physics we know about so far, the tendency of quantum mechanics to jostle — those quantum fluctuations I’ve mentioned elsewhere — would seem to imply that there are two natural values for the Higgs field — in analogy to the two natural places for the vase, firmly placed on the table or smashed on the floor. Naively, the Higgs field should either be zero, or it should be as big as the Planck Energy, 10,000,000,000,000,000 times larger than it is observed to be. Why is it at a value that is non-zero and tiny, a value that seems, at least naively, so unnatural? This is the hierarchy problem.
- Many theoretical physicists have devoted significant fractions of their careers to trying to solve this problem. Some have argued that new particles and new forces are needed (and their theories go by names such as supersymmetry, technicolor , little Higgs, etc.) Some have argued that our understanding of gravity is mistaken and that there are new unknown dimensions (“extra dimensions”) of space that will become apparent to our experiments at the Large Hadron collider in the near future. Others have argued that there is nothing to explain, because of a selection effect: the universe is far larger and far more diverse than the part that we can see, and we live in an apparently unnatural part of the universe mainly because the rest of it is uninhabitable — much the way that although rocky planets are rare in the universe, we live on one because it’s the only place we could have evolved and survived. There may be other solutions to this problem that have not yet been invented.
- Many of these solutions — certainly all the ones with new particles and forces or with new dimensions — predict that new phenomena should be visible at the Large Hadron Collider. 
- By the way, you will often read the hierarchy problem stated as a problem with the Higgs particle mass.  This is incorrect.  The problem is with how big the non-zero Higgs field is.  (For experts — quantum mechanics corrects not the Higgs particle mass but the Higgs mass-squared parameter, changing the Higgs field potential energy and thus the field’s value, making it zero or immense.  That’s a disaster because the W and Z masses are known.  The Higgs mass is unknown, and therefore it could be very large — if the W and Z masses were very large too.  So it is the W and Z masses — and the size of the non-zero Higgs field — that are the problem, both logically and scientifically.)
- Either new particles are keeping the Higgs boson light, or the universe is oddly fine-tuned for our existence. 
- When Victor Weisskopf sat down in the early 1930s to compute the energy of a solitary electron, he had no way of knowing that he’d ultimately discover what is now known as the electroweak hierarchy problem. Revisiting a familiar puzzle from classical electrodynamics – that the energy stored in an electron’s own electric field diverges as the radius of the electron is taken to zero (equivalently, as the energy cutoff of the theory is taken to infinity) – in Dirac’s recently proposed theory of relativistic quantum mechanics, he made a remarkable discovery: the contribution from a new particle in Dirac’s theory, the positron, cancelled the divergence from the electron itself and left a quantum correction to the self-energy that was only logarithmically sensitive to the cutoff. 
-  the coupling between the Higgs boson and other particles of the Standard Model (SM) leads to yet another divergent self-energy, for which the logic of naturalness implied new physics at around the TeV scale. Thus the electroweak hierarchy problem was born – not as a new puzzle unique to the Higgs, but rather the latest application of Weisskopf’s wildly successful logic (albeit one for which the answer is not yet known). 
- History suggested two possibilities. As a scalar, the Higgs could only benefit from the sort of cancellation observed among fermions if there is a symmetry relating bosons and fermions, namely supersymmetry. Alternatively, it could be a light product of compositeness, just as the pions and kaons are light bound states of the strong interactions. These solutions to the hierarchy problem came to dominate expectations for physics beyond the SM, with a sharp target – the TeV scale – motivating successive generations of collider experiments. Indeed, when the physics case for the LHC was first developed in the mid-1980s, it was thought that new particles associated with supersymmetry or compositeness would be much easier to discover than the Higgs itself. But while the Higgs was discovered, no signs of supersymmetry or compositeness were to be found.
- In the meantime, other naturalness problems were brewing. The vacuum energy – Einstein’s infamous cosmological constant – suffers a divergence of its own, and even the finite contributions from the SM are many orders of magnitude larger than the observed value. Although natural expectations for the cosmological constant fail, an entirely different set of logic seems to succeed in its place. To observe a small cosmological constant requires observers, and observers can presumably arise only if gravitationally-bound structures are able to form. As Steven Weinberg and others observed in the 1980s, such anthropic reasoning leads to a prediction that is remarkably close to the value ultimately measured in 1998. To have predictive power, this requires a multitude of possible universes across which the cosmological constant varies; only the ones with sufficiently small values of the cosmological constant produce observers to bear witness.
- An analogous argument might apply to the electroweak hierarchy problem: the nuclear binding energy is no longer sufficient to stabilise the neutron within typical nuclei if the Higgs vacuum expectation value (VEV) is increased well above its observed value. If the Higgs VEV varies across a landscape of possible universes while its couplings to fermions are kept fixed, only universes with sufficiently small values of the Higgs VEV would lead to complex atoms and, presumably, observers. Although anthropic reasoning for the hierarchy problem requires stronger assumptions than for the cosmological-constant problem, its compatibility with null results at the LHC is enough to raise questions about the robustness of natural reasoning. 
- The success of the relaxion hypothesis in solving the hierarchy problem hinges on an array of other questions involving gravity. Whether the relaxion potential can remain sufficiently smooth over the vast trans-Planckian distances in field space required to set the value of the weak scale is an open question, one that is intimately connected to the fate of global symmetries in a theory of quantum gravity (itself the target of active study in what is known as the Swampland programme).
- the recognition that cosmology might play a role in solving the hierarchy problem has given rise to a plethora of new ideas. For instance, in Raffaele D’Agnolo and Daniele Teresi’s recent paradigm of “sliding naturalness”, the Higgs is coupled to a new scalar whose potential features two minima. In the true minimum, the cosmological constant is large and negative, and the universe would crunch away into oblivion if it ended up in this vacuum. In the second, local minimum, the cosmological constant is safely positive (and can be made compatible with the small observed value of the cosmological constant by Weinberg’s anthropic selection). The Higgs couples to this scalar in such a way that a large value of the Higgs VEV destabilises the “safe” minimum. During the inflationary epoch, only universes with suitably small values of the Higgs VEV can grow and expand, while those with large values of the Higgs VEV crunch away. A second scalar coupled analogously to the Higgs can explain why the VEV is small but non-zero. 
- Alternatively, in the paradigm of “Nnaturalness” proposed by Nima Arkani-Hamed and others, the multitude of SMs over which the Higgs mass varies occur in one universe, rather than many. The fact that the universe is predominantly composed of one copy of the SM with a small Higgs mass can be explained if inflation ends and reheats the universe through the decay of a single particle. If this particle is sufficiently light, it will preferentially reheat the copy of the SM with the smallest non-zero value of the Higgs VEV, even if it couples symmetrically to each copy. The sub-dominant energy density deposited in other copies of the SM leaves its mark in the form of dark radiation susceptible to detection by the Simons Observatory or upcoming CMB-S4 facility. 
- Finally, Gian Giudice, Matthew Mccullough and Tevong You have recently shown that inflation can help to understand the electroweak hierarchy problem by analogy with self-organised criticality. Just as adding individual grains of sand to a sandpile induces avalanches over diverse length scales – a hallmark of critical behaviour, obtained without tuning parameters – so too can inflation drive scalar fields close to critical points in their potential. This may help to understand why the observed Higgs mass lies so close to the boundary between the unbroken and broken phases of electroweak symmetry without fine tuning.
- Underlying Weisskopf’s natural reasoning is a long-standing assumption about relativistic theories of quantum mechanics: physics at short distances (the ultraviolet, or UV) is decoupled from physics at long distances (the infrared, or IR), making it challenging to apply a theory involving a large energy scale to a much smaller one without fine tuning. This suggests that loopholes may be found in theories that mix the UV and the IR, as is known to occur in quantum gravity. 
- While the connection between this type of UV/IR mixing and the mass of the Higgs remains tenuous, there are encouraging signs of progress.
- We have come a long way since Weisskopf first set out to understand the self-energy of the electron. The electroweak hierarchy problem is not the first of its kind, but rather the one that remains unresolved. The absence of supersymmetry or compositeness at the TeV scale beckons us to search for new solutions to the hierarchy problem, rather than turning our backs on it. In the decade since the discovery of the Higgs, this search has given rise to a plethora of novel approaches, building new bridges between particle physics, cosmology and gravity along the way. Despite the many differences among these new approaches, they share a common tendency to leave imprints on the Higgs boson. And so, as ever, we must look to experiment to show the way. 
-  The hierarchy problem, as the puzzle is called, asks why the Higgs boson is so lightweight — a hundred million billion times less massive than the highest energy scales that exist in nature. The Higgs mass seems unnaturally dialed down relative to these higher energies, as if huge numbers in the underlying equation that determines its value all miraculously cancel out.
-  The extra particles would have explained the tiny Higgs mass, restoring what physicists call “naturalness” to their equations. But after the LHC became the third and biggest collider to search in vain for them, it seemed that the very logic about what’s natural in nature might be wrong.
-  At first, the community despaired. “You could feel the pessimism,” said Isabel Garcia Garcia, a particle theorist at the Kavli Institute for Theoretical Physics at the University of California, Santa Barbara, who was a graduate student at the time. Not only had the $10 billion proton smasher failed to answer a 40-year-old question, but the very beliefs and strategies that had long guided particle physics could no longer be trusted. People wondered more loudly than before whether the universe is simply unnatural, the product of fine-tuned mathematical cancellations. Perhaps there’s a multiverse of universes, all with randomly dialed Higgs masses and other parameters, and we find ourselves here only because our universe’s peculiar properties foster the formation of atoms, stars and planets and therefore life. This “anthropic argument,” though possibly right, is frustratingly untestable.
-  Some of those who remained set to work scrutinizing decades-old assumptions. They started thinking anew about the striking features of nature that seem unnaturally fine-tuned — both the Higgs boson’s small mass, and a seemingly unrelated case, one that concerns the unnaturally low energy of space itself.
-  Their introspection is bearing fruit. Researchers are increasingly zeroing in on what they see as a weakness in the conventional reasoning about naturalness. It rests on a seemingly benign assumption, one that has been baked into scientific outlooks since ancient Greece: Big stuff consists of smaller, more fundamental stuff — an idea known as reductionism. “The reductionist paradigm … is hard-wired into the naturalness problems,” said Nima Arkani-Hamed, a theorist at the Institute for Advanced Study in Princeton, New Jersey.
- Now a growing number of particle physicists think naturalness problems and the null results at the Large Hadron Collider might be tied to reductionism’s breakdown. “Could it be that this changes the rules of the game?” Arkani-Hamed said. In a slew of recent papers, researchers have thrown reductionism to the wind. They’re exploring novel ways in which big and small distance scales might conspire, producing values of parameters that look unnaturally fine-tuned from a reductionist perspective.
- The Large Hadron Collider did make one critical discovery: In 2012, it finally struck upon the Higgs boson, the keystone of the 50-year-old set of equations known as the Standard Model of particle physics, which describes the 17 known elementary particles.
- The discovery of the Higgs confirmed a riveting story that’s written in the Standard Model equations. Moments after the Big Bang, an entity that permeates space called the Higgs field suddenly became infused with energy. This Higgs field crackles with Higgs bosons, particles that possess mass because of the field’s energy. As electrons, quarks and other particles move through space, they interact with Higgs bosons, and in this way they acquire mass as well.
- After the Standard Model was completed in 1975, its architects almost immediately noticed a problem.
- When the Higgs gives other particles mass, they give it right back; the particle masses shake out together. Physicists can write an equation for the Higgs boson’s mass that includes terms from each particle it interacts with. All the massive Standard Model particles contribute terms to the equation, but these aren’t the only contributions. The Higgs should also mathematically mingle with heavier particles, up to and including phenomena at the Planck scale, an energy level associated with the quantum nature of gravity, black holes and the Big Bang. Planck-scale phenomena should contribute terms to the Higgs mass that are huge — roughly a hundred million billion times larger than the actual Higgs mass. Naively, you would expect the Higgs boson to be as heavy as they are, thereby beefing up other elementary particles as well. Particles would be too heavy to form atoms, and the universe would be empty.
- For the Higgs to depend on enormous energies yet end up so light, you have to assume that some of the Planckian contributions to its mass are negative while others are positive, and that they’re all dialed to just the right amounts to exactly cancel out. Unless there’s some reason for this cancellation, it seems ludicrous — about as unlikely as air currents and table vibrations counteracting each other to keep a pencil balanced on its tip. This kind of fine-tuned cancellation physicists deem “unnatural.”
- Within a few years, physicists found a tidy solution: supersymmetry, a hypothesized doubling of nature’s elementary particles. Supersymmetry says that every boson (one of two types of particle) has a partner fermion (the other type), and vice versa. Bosons and fermions contribute positive and negative terms to the Higgs mass, respectively. So if these terms always come in pairs, they’ll always cancel.
- The search for supersymmetric partner particles began at the Large Electron-Positron Collider in the 1990s. Researchers assumed the particles were just a tad heavier than their Standard Model partners, requiring more raw energy to materialize, so they accelerated particles to nearly light speed, smashed them together, and looked for heavy apparitions among the debris.
- The fabric of space, even when devoid of matter, seems as if it should sizzle with energy — the net activity of all the quantum fields coursing through it. When particle physicists add up all the presumptive contributions to the energy of space, they find that, as with the Higgs mass, injections of energy coming from Planck-scale phenomena should blow it up. Albert Einstein showed that the energy of space, which he dubbed the cosmological constant, has a gravitationally repulsive effect; it causes space to expand faster and faster. If space were infused with a Planckian density of energy, the universe would have ripped itself apart moments after the Big Bang. But this hasn’t happened.
- Instead, cosmologists observe that space’s expansion is accelerating only slowly, indicating that the cosmological constant is small. Measurements in 1998 pegged its value as a million million million million million times lower than the Planck energy. Again, it seems all those enormous energy injections and extractions in the equation for the cosmological constant perfectly cancel out, leaving space eerily placid.
- In hindsight, the two naturalness problems seem more like symptoms of a deeper issue. “It’s useful to think about how these problems come about,” said Garcia Garcia in a Zoom call from Santa Barbara this winter. “The hierarchy problem and the cosmological constant problem are problems that arise in part because of the tools we’re using to try to answer questions — the way we’re trying to understand certain features of our universe.”
- 

# Le problème du lithium cosmique

*Sources*


- [Populating the periodic table: Nucleosynthesis of the elements](https://science.sciencemag.org/content/363/6426/474) - Johnson (2019)
- [The Primordial Lithium Problem](https://www.annualreviews.org/doi/10.1146/annurev-nucl-102010-130445) - Fields (2011)
- [The Cosmological Lithium Problem Revisited](https://arxiv.org/abs/1603.03864) - Bertulani et al (2016)
- [Etoile de population II](https://fr.wikipedia.org/wiki/%C3%89toile_de_population_II) - WIkipédia
- [Big-Bang Nucleosynthesis and the Baryon Density of the Universe](https://arxiv.org/pdf/astro-ph/9407006.pdf) - Copi et al (1995)
- [Big Bang Nucleosynthesis](https://pdg.lbl.gov/2019/reviews/rpp2019-rev-bbang-nucleosynthesis.pdf)  - Fields (2019)
- [Big Bang Nucleosynthesis (BBN)](https://www.astronomy.ohio-state.edu/weinberg.21/A5682/notes8.pdf) - Weinberg
- [The cosmological lithium problem](https://www.sciencedaily.com/releases/2018/10/181009102501.htm) - Science Daily (2018)

---

- La ***nucléosynthèse primordiale*** désigne le processus de formation des noyaux les plus légers dans les conditions ardentes de l'univers primordial, entre 1 s et 3 minutes (~180 s) environ après le Big Bang.
> À partir de 1 seconde après le Big Bang, la température du cosmos passe sous la barre des 10 milliards de degrés. Une séquence d'événements est alors initiée, qui mène à la synthèse de quelques éléments légers, comme ***le deutérium, l'hélium-3, l'hélium-4 et le lithium-7***.
- La théorie de la nucléosynthèse primordiale standard (Big Bang Nucleosynthesis) repose sur le modèle standard de la physique des particules ainsi que du modèle standard de la cosmologie (ΛCDM), qui modélise un univershomogène et isotrope en expansion selon les règles de la relativité générale contenant de la matière noire et de l'énergie sombre.
- Dans la théorie de la nucléosynthèse primordiale standard, les abondances des éléments légers sont encapsulées dans un unique paramètre, la ***densité de baryons cosmique***, noté η (les baryons dans ce contexte sont les protons et les neutrons), qui est normalisée par rapport à la densité de photons du fond diffus cosmologique (baryon-photon ratio). Ce rapport est de l'ordre de 10^-9, ce qui veut dire que pour chaque baryon de l'univers, il y a environ 1 milliard de photons du CMB.
> La densité de photon du CMB est de 413 photons/cm3.
- Le ***problème du lithium cosmique*** désigne l'énorme différence entre nos théories et nos observations concernant l'abondance de lithium 7 dans l'univers.
> On observe 3 à 4 fois moins de lithium-7 dans l'univers que ce qui est prédit par la théorie. 
- Par contre, les abondances d'hélium et de deutérium sont reproduites avec beaucoup de succès
> Ces abondances constituent même l'un des quatre piliers observationnels du modèle du Big Bang chaud.
- Le problème du lithium est apparu lorsque les astronomes ont commencé à étudier les propriétés du rayonnement de fond diffus cosmologique à l'aide du satellite WMAP, et le désaccord entre théorie et observation est devenu de plus en plus plus important au fil des nouvelles observations (à l'époque de la première fournée de données de WMAP, il n'y avait qu'un facteur 2-3).

### Comment mesure t-on les abondances des éléments légers dans l'univers ?

- En analysant la carte du fond diffus cosmologique, on peut prédire la densité de baryons cosmiques avec une grande précision et ainsi tester la théorie de la nucléosynthèse primordiale.
> Les récentes données du satellite Planck donnent une prédiction de η = 6x10^-10, qui est en très bon accord avec les abondances observées de deutérium et d'hélium-4 de z=1000 à z=0.
- Les abondance en deutérium (un isotope de l'hydrogène constitué d'1 proton et 1 neutron) sont mesurées dans des nuages d'hydrogène très lointains (z~3) et très pauvres en éléments lourds qui sont observés sur la ligne de visée de quasars encore plus lointains. On ne peut pas mesurer les abondances en deutérium dans les étoiles, car il est entièrement détruit dans ces systèmes.
> À l'heure actuelle, ce sont les abondances qui sont (de loin) les mieux reproduites par la théorie.
- Les abondances en hélium-3 sont mesurées dans le milieu interstellaire dans la Voie Lactée, faute de pouvoir les mesurer dans des galaxies lointaines. Comme notre galaxie est riche en éléments lourds, on ne peut pas utiliser pour le moment les abondances en hélium-3 pour contraindre la période de nucléosynthèse primordiale.
- Les abondances en hélium-4 sont mesurées dans des régions de formation d'étoiles (appelées ***régions HII***) de galaxies voisines pauvres en éléments lourds (metal-poor)
- Les abondances en lithium sont principalement mesurées dans l'atmosphère (photosphère) de très vieilles étoiles (âgées de 11 à 13,5 milliards d'années) pauvres en éléments lourds présentes dans le halo stellaire de notre Galaxie. On connaît environ une centaine de ces étoiles dites de ***population II***. Les abondances observées sont relativement faibles, notamment parce que le lithium de l'atmosphère de ces étoiles est en permanence emporté dans les profondeurs par les mouvements convectifs de leur enveloppe, où il est détruit par la chaleur intense.
> Ce qu'on mesure en pratique ce sont les raies d'absorption dans le spectre de ces étoiles qui correspondent à la signature du lithium

### Quelles sont les solutions possibles au problème du lithium cosmique ?

- Soit nos prédictions théoriques (cosmologie+physique des particules) sont correctes, mais ce sont les observations astrophysiques qui sont incomplètes
> Grâce au LSST, on aura bientôt accès à une population bien plus grande d'étoiles pauvres en éléments lourds dans des galaxies proches et lointaines qui permettra d'avoir des statistiques plus fiables sur les abondances de lithium.
- Il pourrait aussi exister des processus encore inconnus au niveau de la physique nucléaire qui pourrait altérer nos prédictions sur les abondances du lithium
> Ces processus pourraient en particulier amplifier la destruction du béryllium-7. On peut vérifier cette hypothèse à l'aide d'expériences de physique nucléaire comme celles menées par la collaboration n_TOF (neutron-Time Of Flight) au CERN depuis 2018.
- Il pourrait enfin exister des solutions au-delà du modèle standard (comme la supersymmétrie par exemple) qui pourrait impliquer de nouveaux processus au niveau cosmologique ou de la physique des particules
> Ces solutions sont testées dans les accélérateurs de particules comme le LHC et du côté des expériences de détection de matière noire.

# Le principe cosmologique

- [Probing cosmic isotropy with a new X-ray galaxy cluster sample through the LX–T scaling relation](https://www.aanda.org/articles/aa/full_html/2020/04/aa36602-19/aa36602-19.html) - Migkas et al (2020)
- [Observation d'une anisotropie de l'Univers !](https://www.ca-se-passe-la-haut.fr/2020/04/observation-dune-anisotropie-de-lunivers.html) - Ca se passe là-haut
- [The CMB Dipole: Eppur Si Muove](https://arxiv.org/abs/2111.12186) - Sullivan et Scott (2021)
- [Cosmic Microwave Background Dipole](https://astronomy.swin.edu.au/cosmos/c/Cosmic+Microwave+Background+Dipole) - SAO Encyclopedia of astronomy
- [The Motion of the Local Group with Respect to the 15,000 Kilometer per Second Abell Cluster Inertial Frame](https://adsabs.harvard.edu/full/1994ApJ...425..418L) - Lauer et Postman (1994)
- [Testing the Cosmological Principle in the radio sky](https://arxiv.org/pdf/1905.12378.pdf) - Bengaly et al (2021)
- [Is the Observable Universe Consistent with the Cosmological Principle?](https://arxiv.org/abs/2207.05765) - Aluri et al (2022)
- [Cosmologists Parry Attacks on the Vaunted Cosmological Principle](https://www.quantamagazine.org/giant-arc-of-galaxies-puts-basic-cosmology-under-scrutiny-20211213/) - Quanta (2021)
- [Un intrigant anneau géant fait de sursauts gamma](https://www.futura-sciences.com/sciences/actualites/sursaut-gamma-intrigant-anneau-geant-fait-sursauts-gamma-59317/) - Futura Sciences (2015)
- [The dipole repeller](https://www.nature.com/articles/s41550-016-0036) - Hoffman et al (2017)
- [Testing the Cosmological Principle](https://indico.cern.ch/event/1036660/attachments/2241767/3801102/TestingCosmoPrinciple.pdf) - Subir Sarkar (2021)

---

- L'homogénéité (à grande échelle, ie au-delà de 70 h-1 Mpc) et l'isotropie (statistiquement parlant) de l'univers est à la base du modèle standard de la cosmologie. Autrement dit, il n'y a pas d'observateur privilégié ou de direction privilégiée dans l'univers. Où que l'on soit dans l'univers, et quelque soit la direction où l'on regarde, on devrait observer la même chose à grande échelle. Cette idée constitue le ***principe cosmologique***.
> The cosmological principle grew out of the Copernican principle, Nicolaus Copernicus’ 1543 realization that Earth is not the fixed center of creation. Not only is Earth not special, but nothing anywhere is special. 
- The universe is clearly not homogeneous on the human scale. Teleport a person one light-year from here and you’ll ruin their day. But drop the Hubble Space Telescope halfway across the universe, and it will return familiar-looking galaxy-filled images. 
- Theorists reconstruct the cosmos’s past and predict its future using a standard theoretical model based largely on general relativity, Albert Einstein’s theory of gravity. Einstein’s theory describes the interplay between matter and space-time — the bendy fabric of the universe. But Einstein’s treatment involves 10 interlinked equations and 20 variables, a system of equations that is generally too complicated to solve.

Cosmologists lean on the cosmological principle to restrict their focus to a universe acting as a smooth and symmetric fluid. By ignoring bumps of matter like galaxies and requiring the universe to expand in the same way along all three axes, the cosmological principle deletes parts of the equations and links some of the variables, dramatically simplifying the system of equations. Theorists can then predict the velocity and acceleration of the cosmos’s expansion with just two equations — the Friedmann equations, derived from Einstein’s by Alexander Friedmann, a Russian cosmologist, in 1922. It’s a bit like computing the volume of the Earth: You could fret over every mountain and ravine, or you could assume the planet is a sphere and call it a day.

### Le dipôle cosmologique

- Lorsque l'on cartographie le fond diffus cosmologique, on remarque qu'il n'est pas directement isotrope. La plus grande anisotropie de température qu'il présente est un dipole. On l'appelle le ***dipôle cosmologique*** (CMB dipole). Son amplitude (ΔT/T\~10-3) est plus grande que les autres fluctuations de température (ΔT/T\~10-5). 
> Afin d'étudier les anisotropies de l'ordre de 10-5, on doit donc soustraire la contribution du dipôle cosmologique.
- Son existence a été révélée pour la première fois par les mesures du satellite COBE dans les années 90.

*Dipôle cosmologique révélé par COBE ([Source](https://apod.nasa.gov/apod/ap010128.html))*

![COBE_dipole](https://apod.nasa.gov/apod/image/0101/dipole_cobe.jpg)

- On interprète généralement ce dipôle comme le résultat du mouvement combiné de la Terre autour du Soleil, du Soleil autour de la Voie Lactée, de la Voie Lactée dans le Groupe Local, et du Groupe Local vers un ***Grand Attracteur*** dans le référentiel du CMB (modèle proposé par Lynden Bell et al en 1988), qui génère un ***effet Doppler*** : on observe le CMB avec un décalage spectral vers le bleu dans la direction de notre mouvement et un décalage vers le rouge dans la direction opposée. 
> * Notre système solaire se déplace à environ 370 km/s par rapport au référentiel dans lequel le CMB est isotrope, et le Groupe Local se déplace à environ 630 (+-20) km/s (2,2 millions de km/h) par rapport au CMB dans la direction du super-amas de Shapley (à 600 millions d'années-lumière d'ici), à cause du Grand Attracteur (une région contenant une demi-douzaine d'amas de galaxies qui se trouve au coeur du super-amas Laniakea, à 150 millions d'années-lumière d'ici)
> * Ce Grand Attracteur est difficile à observer dans le domaine du visible parce qu'il est situé directement derrière le plan Galactique.
- Depuis 2017, on sait aussi qu'il existe une région relativement vide de matière (noire et baryonique) située dans la direction opposée du Grand Attracteur. Cette région, qu'on a appelé ***répulseur du dipôle*** (Dipole Repeller), agit de manière effective comme un répulseur sur le mouvement des galaxies avoisinantes. La force attractive causée par le Grand Attracteur (sur-densité de matière dans la direction de notre mouvement) ET la force "répulsive" causée par le répulseur du dipôle (sous-densité de matière dans la direction opposée) contribuent de manière équivalente au mouvement du Groupe Local dans l'espace, et donc au dipôle cosmologique.

*Le mouvement du Groupe Local dans son contexte cosmique (Source : Hoffman et al 2017)*

![dipole-repeller](https://user-images.githubusercontent.com/4954089/203095826-6f110b58-2853-4938-80f3-af813e76cfe7.png)

- Lorsque ce mouvement global est corrigé, le CMB est remarquablement isotrope.
- Cependant, l'isotropie du CMB (à l'échelle de 10-5) n'implique pas forcément l'isotropie de l'univers. Pour tester l'isotropie de l'univers, on a besoin d'autres mesures indépendantes, par exemple en étudiant la distribution statistique de matière dans l'univers. 

### Des structures plus grandes que l'échelle d'homogénéité

- Depuis les années 80, des relevés astronomiques révèlent l'existence d'une liste croissante de grandes structures dont la taille dépasse l'échelle maximale à partir de laquelle l'univers est sensé être homogène. L'estimation haute de cette échelle d'homogénéité est de ~370 Mpc (1,2 milliards d'années-lumière).
- La plupart de ces structures sont des ***amas de quasars*** (en anglais : large quasar group ou LQG). Ces collections de quasars font partie des plus grandes structures cosmiques connues. On pense qu'ils pourraient être les précurseurs des filaments galactiques que l'on observe dans l'univers proche.
- ***Liste des plus grandes structures cosmiques par ordre croissant de taille :***
> * Le ***grand mur de Sloan*** est une structure mesurant plus d'1,37 milliards d'années-lumière, découverte en 2003 dans les données du SDSS
> * Le ***Clowes–Campusano Large Quasar Group*** (CCLQG) est un amas de quasars constitué de 34 quasars et mesurant environ 2 milliards d’années-lumière de diamètre (~630 Mpc), découvert en 1991.
> * ***L'amas de quasars U1.11*** est une collection de 38 quasars dont le diamètre est estimé à 2,5 milliards d'années-lumière (~780 Mpc). Il a été découvert en 2011 à proximité du CCLQG dans les données du SDSS.
> * ***L'arc géant*** est une structure en forme de sourire mesurant 3,3 milliards d'années-lumière (1/28e du diamètre de l'univers observable), découverte en 2021 dans les données du SDSS. Sa taille apparente dans le ciel est aussi étendue que 20 pleines lunes. Elle se trouve à 9,2 milliards d'années-lumière d'ici (z~0.8).
> * Le ***Huge-LQG*** (Huge Large Quasar Group, « Immense grand amas de quasars ») est un amas composé de 73 quasars mesurant environ 4 milliards d'années-lumière de diamètre (~1240 Mpc), découvert en 2013 dans les données du SDSS
> * ***L'anneau géant de sursauts gamma*** (Giant GRB ring) est un grand anneau constitué par 9 sursauts gamma (hypernovae ou collisions entre étoiles à neutrons) qui s'étalerait sur environ 5,6 milliards d'années-lumière. Il a été découvert en 2015. Son diamètre apparent dans le ciel est équivalent à 70 pleines lunes.
> * Le ***Grand Mur d'Hercule-Couronne boréale*** est un filament cosmique mesurant 10 milliards d'années-lumière, découvert en 2013 dans les données d'un relevé de sursauts gamma. Il s'agit de la plus vaste et la plus massive structure cosmique connue de l'univers observable. Mais certaines études mettent en doute son existence.
- Le modèle standard de la cosmologie n'interdit pas complètement l'existence de telles structures, mais il les rend extrêmement rares.

### Conclusion

- Avec les données actuelles, on ne peut pas encore conclure que le principe cosmologique n'est pas valide. Mais dans les décennies à venir, 

- The isotropy of the late Universe and consequently of the X-ray galaxy cluster scaling relations is an assumption greatly used in astronomy. However, within the last decade, many studies have reported deviations from isotropy when using various cosmological probes; a definitive conclusion has yet to be made. New, effective and independent methods to robustly test the cosmic isotropy are of crucial importance.

referred axes noted in other astronomical surveys spanning the
electromagnetic spectrum were also found to be aligned with the CMB kinematic dipole, pointing towards
the Virgo cluster [73, 182]. The standard narrative says that these are coincidences
-  
Previous work has shown no statistically significant violation of isotropy in the obser-
vational data of Type Ia Supernova distances [4, 5, 6] and of gamma-ray bursts [7, 8, 9].
More stringent tests require the far higher number densities delivered by large galaxy sur-
veys. The simplest way to test consistency with the CMB is to measure the dipole of a
(sufficiently wide) galaxy survey, which should be aligned with the direction of the CMB
dipole [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. For currently available data sets,
the matter dipole direction is not inconsistent with the CMB, but the amplitude is too large,
probably arising from the quality of current data sets. Forecasts predict that future all-sky
radio continuum surveys with the SKA should achieve the accuracy necessary to make a
stringent test of consistency with the CMB dipole [23, 24]
- ensions
have emerged within the ΛCDM model, most notably a statistically significant discrepancy in the
value of the Hubble constant, H0. Since the notion of cosmic expansion determined by a single
parameter is intimately tied to the CP, implications of the H0 tension may extend beyond ΛCDM
to the CP itself. This review surveys current observational hints for deviations from the expecta-
tions of the CP, highlighting synergies and disagreements that warrant further study. Setting aside
the debate about individual large structures, potential deviations from the CP include variations of
cosmological parameters on the sky, discrepancies in the cosmic dipoles, and mysterious alignments
in quasar polarizations and galaxy spins.
- If the Universe is not FLRW, but we view it through the prism of FLRW, cos-
mological tensions are inevitable. Interestingly, a host of fascinating observational tensions exist, including
the H0 tension [45, 51–58], the S8 tension [178–181], potentially a curvature tension [119, 296], and an
AISW tension 
- But our peculiar velocity might not fully explain the perceived lopsidedness of the CMB; the distortion could also include the effect of the whole universe drifting. If this is the case, gauging our motion against distant galaxies will give a different result than if we measure our speed against the CMB, since those galaxies will be moving too. 
-  we document a series of intriguing alignments that are surprising in a statistically homo-
geneous and isotropic universe. One curious aspect of some of these alignments is their observation over
large – potentially gigaparsecs – scales and axes that overlap with the CMB dipole direction. If not due to
experimental systematics or interstellar physics, one exciting possibility is that these features are cosmo-
logical in origin. 

# La tension de Hubble

*Sources*

- [La crise cosmique de la constante de Hubble](https://www.pourlascience.fr/sd/cosmologie/la-crise-cosmique-de-la-constante-de-hubble-19032.php) - Pour la Science (2020)
- [Exploring the Hubble tension](https://cerncourier.com/a/exploring-the-hubble-tension/) - CERN Courrier (2021)
- [La tension de Hubble : la cosmologie en crise ?](https://scienceetonnante.com/2022/02/04/la-tension-de-hubble/) - Science Etonnante (2022)
- [Tensions dans le modèle cosmologique : l’espoir d’une nouvelle génération de télescopes](https://www.pourlascience.fr/sd/cosmologie/tensions-dans-le-modele-cosmologique-l-espoir-d-une-nouvelle-generation-de-telescopes-23923.php) - Pour  la Science (2022)

---



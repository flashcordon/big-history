# Table des matières

- [Le modèle standard de la cosmologie](#Le-modèle-standard-de-la-cosmologie)
- [La cosmologie quantique](#La-cosmologie-quantique)
- [Matière noire](#Matière-noire)
- [Energie sombre](#Energie-sombre)
- [L'inflation cosmique](#Linflation-cosmique)
- [Le problème de la hiérarchie](#Le-problème-de-la-hiérarchie)
- [Le problème du lithium cosmique](#Le-problème-du-lithium-cosmique)
- [Le principe cosmologique](#Le-principe-cosmologique)
- [La tension de Hubble](#La-tension-de-Hubble)

# Le modèle standard de la cosmologie

*Sources*

- [Dark matter and cosmic structure](https://arxiv.org/abs/1210.0544) - White et Frenk (2012)

---

The model
describes the geometry and material content of the Uni-
verse, explaining how structure - galaxies of various sizes
and types, groups and clusters of galaxies, the entire cos-
mic web of filaments and voids - emerged from a hot and
near-uniform Big Bang. This astonishing richness appears
to have grown from quantum zero-point fluctuations pro-
duced during a period of cosmic inflation occurring very
soon after the beginning of our Universe. Over the sub-
sequent 13.7 billion years, these small density perturba-
tions were amplified by the relentless action of gravita-
tional forces due predominantly to dark matter, but illumi-
nated by complex astrophysical phenomena involving ordi-
nary matter. In the standard model, the dark matter, which
makes up five sixths of all matter, is hypothesised to be a
weakly interacting non-baryonic elementary particle, cre-
ated in the early stages of cosmic evolution.

Although mas-
sive neutrinos had already been suggested as a dark matter
candidate by Cowsik and McClelland [8,9] and Szalay and
Marx [10], White and Rees considered an early population
of low-mass stars as the most plausible identification for
the dark matter in their theory.
The evolution of the halo population in this theory was
based on Press and Schechter’s [11] analytic model for the
growth of cosmic structure from a Gaussian initial density
field. To test this model, Press and Schechter carried out
computer N-body simulations. This was the first time nu-
merical experiments were used for quantitative exploration
of nonlinear structure formation in an expanding universe.

The direct N-body integrators of the
1970’s could follow ∼ 1000 particles within an expanding
spherical volume (N = 4000 in two simulations by Aarseth,
Gott and Turner [13]) so small particle numbers and edge
effects seriously limited the conclusions which could be
drawn. 

Today’s standard model of cosmology arose through the
confluence of three distinct disciplines: particle physics, as-
tronomy and computing. In the early 1980s, new develop-
ments in each of these areas coalesced to establish the phys-
ical foundations and methodology of what, twenty years
later, would become the standard model.

From particle physics came two fundamental new ideas.
The first was Guth and Linde’s theory of cosmic inflation
[27, 28] and the realisation that it could give rise to quan-
tum fluctuations that might seed the universe with adia-
batic, scale invariant density perturbations [29–32]. The
second was the proposal that the dark matter (DM) could
be composed of non-baryonic particles. This idea took cen-
tre stage after Lubimov et al. [33] measured a ∼ 30eV mass
for the electron neutrino, enough to provide the critical
density needed to close the universe [8]. Such light par-
ticles would remain relativistic until relatively late times,
but it was soon recognized that other (hypothetical) parti-
cles which were more massive and decoupled earlier could
also be dark matter. This led to the convenient classification
of candidate dark matter particles into three families: hot,
warm and cold dark matter, names that reflect their typical
velocities at some early time, for example at the epoch of
recombination [21]. Light neutrinos are the prototype for
hot dark matter (HDM) and are the only DM candidates
that are proven to exist from earth-bound experiments; a
supersymmetric particle [34] or an axion [35] emerged as
plausible candidates for cold dark matter (CDM); a non-
standard gravitino was the initially suggested candidate for
warm dark matter (WDM) [36], but more recently, a sterile
neutrino has seemed a more attractive possibility

For the first time, there was a plausible physical mech-
anism to generate small density irregularities in the very
early universe and several concrete proposals for the iden-
tity of the dark matter which would drive their late-time
amplification. Thus, the ingredients were in place for de-
tailed linear calculations of the evolution of structure in the
universe, from its initial generation until the formation of
the first nonlinear objects. 

Inflation seeds the universe with nearly scale invari-
ant, adiabatic density perturbations of very small (and tun-
able) amplitude and a simple power-law power spectrum,
P(k) ∝ k n , where n is close to, but smaller than unity.
As the universe expands, the growth of these perturba-
tions is regulated initially by the dominant radiation com-
ponent and later by the dark matter. 

the smallest structure which can
form directly is radically different in each case: for HDM,
superclusters form first and must fragment to make galax-
ies; for WDM and CDM, small objects form first and grow
by merging and accretion to make bigger systems. Dark
matter objects significantly smaller than galaxies can form
in CDM, but not in WDM.
The proposal of inflationary fluctuation generation and
the rise of interest in particle dark matter coincided with the
publication of the first extensive 3D survey of galaxies, the
CfA redshift survey [39]. Lilliputian by today’s standards,
this survey gave the first clear picture of the richness of
the large-scale distribution of galaxies, offering a glimpse
of what would later be called the “cosmic web” 

N-body simulations had been used
in previous years to study the growth of nonlinear structure
in an expanding universe, and the size of feasible calcula-
tions was increasing rapidly through developments in both
hardware and algorithms. 

the ex-
clusion of a neutrino-dominated universe was a major suc-
cess for the then new cosmological methodology: a light
neutrino mass in the ∼ 30 eV range was conclusively ruled
out by a combination of N-body simulations and astronom-
ical observations. Attention then shifted to the alternative
possibility of a universe dominated by cold dark matter.
The first simulations of structure formation in such a uni-
verse [44] (hereafter DEFW), illustrated in Fig. 2, revealed
that this hypothesis gave far better results when compared
to the CfA data. 

following the formation and evo-
lution of the galaxies within each dark halo in the simu-
lated volume. It might seem that this would best be accom-
plished by direct inclusion of the baryonic component and
all the astrophysical processes affecting it. Unfortunately,
this is far beyond current computational capabilities. The
difficulty is that the relevant baryonic processes are not
only much more diverse and complex than the purely gravi-
tational processes which affect the dark matter, but are also
sensitive to much smaller length- and time-scales than can
be followed in a cosmological simulation (related, for ex-
ample, to star formation and evolution or black hole ac-
cretion). Although this is currently a very active area of
computational astrophysics and progress is rapid, such hy-
drodynamic galaxy formation simulations are still far from
reproducing basic properties of the galaxy population such
as their abundance as a function of stellar mass or their spa-
tial clustering [85–88].
An alternative approach takes the assembly history and
structure of each dark halo from a large cosmological simu-
lation and calculates the growth of galaxies within it using
simplified phenomenological treatments of baryonic pro-
cesses. The latter are typically based on physical insights
gained from simulations of individual systems and from
observation [89–91]. Uncertain parameters such as the ef-
ficiencies of star formation, of black hole growth or of
feedback from stars and black holes can be adjusted to re-
produce the properties of the observed galaxy distribution,
thus providing a practical and empirical route to measure
these efficiencies and their dependence on galaxy mass and
epoch.

Recent semi-analytic galaxy formation simulations fol-
low the evolution of millions of galaxies throughout vol-
umes comparable to those of new very large redshift sur-
veys like the SDSS. In particular, several generations of
publicly available models based on the Millennium Sim-
ulatio [96, 98–101] have produced ever closer matches
to the observed galaxy population and have been widely
used by the community4. This success reflects the fact
that simulations of this kind make it possible to construct
mock surveys where the simulated galaxy population is
"observed" with a virtual telescope to produce a sample
in which galaxy properties and the large-scale structure
can be compared directly with those observed in real sur-
veys

Dark matter halos are the fundamental nonlinear units of
cosmic structure and galaxies condense in their cores. As
a result, halos have a special status in cosmology. Under-
standing their basic properties – formation histories, inter-
nal structure and abundance – is an important step in devis-
ing astronomical tests of the ΛCDM paradigm and in ex-
ploring how galaxies form. Halos are also the prime hunt-
ing ground for dark matter detection. Direct detection ex-
periments target dark matter particles at the Earth’s posi-
tion within our own Galactic halo; indirect detection exper-
iments target the radiation from decaying or annihilating
dark matter particles, and this is also strongly affected by
the structure of halos.

The idea that dark matter is made of free elementary par-
ticles of some new type will likely be fully accepted by
the scientific community only once non-gravitational ef-
fects of these particles have been unambiguously measured.
Two routes to making such measurements are being ac-
tively pursued. Direct detection experiments attempt to
measure the effects of dark matter particles in a laboratory.

The second route to non-gravitational detection is by
searching for annihilation or decay products from dis-
tant dark matter concentrations. In typical supersymmetric
models WIMPS can decay and can annihilate with each
other. Both processes can have a significant branching ra-
tio into photons which would then be detectable with γ-
ray telescopes. Since particle decay is independent of en-
vironment, it would produce γ-ray images of dark matter
halos which trace their mass distribution, just as optical
images of galaxies trace their star distribution. 

The evidence that the universe conforms to the expecta-
tions of the CDM model is compelling but not decisive.
Current observational tests span a very wide range of
scales. Fluctuations in the microwave background probe
from the whole observable universe down to the scale of
rich galaxy clusters. Galaxy and quasar clustering probe
scales between one and a few hundred megaparsecs. Gravi-
tational lensing measurements constrain the detailed struc-
ture of galaxy and cluster halos. Observations of the Ly-
α forest at z ∼ 2 probe down to scales far below those
responsible for forming bright galaxies like the Milky
Way, putting severe constraints on warm dark matter and
on any subdominant contribution from massive neutrinos
[194, 208, 209]. Nevertheless, new data are needed to test
the model on even smaller scales. Currently, warm dark
matter remains possible if the particles are sufficiently mas-
sive to evade the Ly-α forest constraints. More exotic pos-
sibilities such as self-interacting dark matter may also be
possible, provided their properties are carefully tuned. Ex-
perimental searches for such alternative types of dark mat-
ter would need to be quite different than those for WIMPS
or axions, so it is imperative that astrophysicists continue
to test the standard model and to evaluate possible evidence
for alternatives. Dwarf galaxies are a prime source of such
evidence, both in the field and as satellites of larger galax-
ies.
On subgalactic scales, currently viable hypotheses for
the dark matter can produce strikingly different structure.

There are three aspects of small-scale structure where
potential conflicts with the cold dark matter model have
been identified: (i) the luminosity function of galactic satel-
lites, (ii) the abundance of galactic substructures as a func-
tion of mass or circular velocity and (iii) the structure of the
halos that host faint satellites or field dwarfs. Problem (ii)
was the first to be clearly identified in the late 1990s when
N-body simulations revealed a large number of subhalos
within galactic halos, vastly exceeding the number of satel-
lites then known to be orbiting in the halo of the Milky Way
[136, 175]. In its original form, the problem was expressed
as an excess of subhalos as a function of their circular ve-
locity or mass but it was soon re-expressed as an excess of
subhalos as a function of their galaxy luminosity. Although
closely related, these two statements are conceptually dis-
tinct and the solutions that have been proposed differ for
the two cases.
The small number of visible satellites in the Milky Way
compared to the large number of subhalos in N-body simu-
lations of galactic halos is often referred to as “the satellite
problem”. Soon after it was first formulated, a new popu-
lation of ultra-faint dwarf satellites was discovered in the
Milky Way [211–215], but they are too small and too few
to alter the argument. 

The three problems mentioned at the beginning of this
section remain a subject of lively debate and suggest clear
avenues for astrophysical progress, both theoretical and ob-
servational. Higher resolution N-body simulations will fur-
ther explore predictions for warm and self-interacting dark
matter. More realistic N-body and hydrodynamic simula-
tions should clarify whether baryonic effects are indeed vi-
able explanations for the apparent discrepancies with pre-
dicted halo structure. 

Most searches are focused on cold dark matter. This is
entirely justified: the cold dark matter model has been the
catalyst for the momentous developments of the past thirty
years. The statistical properties of the large-scale distribu-
tion of galaxies, the form and amplitude of the microwave
background temperature fluctuations and many aspects of
galaxy formation were predicted (in advance!) by theoret-
ical models that had cold dark matter at their heart. Time
and again when astronomical data seemed to conflict with
the model (for example, the clustering of galaxy clusters,
large-scale streaming data, galaxies at very high redshift)
the conflict has been resolved either by revisions of the data
or by refinments of the theory. Nevertheless, even the most
ardent supporters of the cold dark matter idea must remain
sceptical until the dark matter particles are finally and con-
clusively discovered.

1970-1980 Prehistory
Dark matter (DM) halos are proposed to surround all galaxies
Massive neutrinos are suggested as a dark matter candidate
Large-scale structure is characterized through the measurement of galaxy correlation functions
First analytic models and computer simulations are built for nonlinear structure formation in an expanding universe
Simulations from idealized initial conditions contrast the “isothermal” case where structure grows by hierarchical merging
with the "adiabatic" case where galaxies form by fragmentation of large-scale structure

1980-83 The breakthrough years
The CfA redshift survey provides a first representative picture of the cosmic web
A claimed measurement of a 30eV mass for νe galvanizes research into elementary particle dark matter
Quantum fluctuations during an early inflationary era are suggested as a physical model for the initial generation of density
fluctuations
Improved calculations of early linear evolution provide accurate initial conditions for late-time growth, dividing particle
candidates into Hot, Warm and Cold Dark Matter (HDM, WDM, CDM respectively)
Simulations from these initial conditions exclude HDM (and hence all known particle species) for the bulk of the DM

1983-2002 Establishment of the standard model
Simulations from CDM initial conditions produce large-scale structure resembling that seen in redshift surveys
Galaxies are shown to trace the DM distribution in a biased way depending strongly on galaxy type and on epoch
The first direct detection experiments and the first searches for DM annihilation radiation are set up
The COBE measurement of temperature fluctuations in the cosmic microwave background (CMB) indicates an amplitude
too small for a universe without particle DM, but too large for a universe with a closure density of such DM
Large-scale correlations of galaxies and the baryon fraction of rich clusters prove inconsistent with a high-density universe
Simulations establish the detailed structure expected for DM halos in a CDM universe
Supernova measurements rule out a high density universe and strongly suggest accelerated expansion
Boomerang and Maxima convincingly localize the first acoustic peak in the CMB, showing our universe to be flat
WMAP convincingly measures the second acoustic peak also, excluding many alternatives to the standard model and giving
precise estimates of the baryon and dark matter densities

2002-present Pushing to new frontiers
Simulations explore the predictions of the standard model at higher precision and on both larger and smaller scales
Gravitational lensing measurements check the detailed structure predicted for dark halos
Large redshift surveys give precise measures of galaxy biasing, and of the Gaussianity and power spectrum of the initial
conditions, detecting weak baryon-induced features
Simulations and observations explore constraints on the nature of the dark matter based on the inner structure of dark halos
and on structure in the intergalactic medium
Both direct detection experiments and the Fermi satellite begin to exclude parts of the parameter space expected for WIMPs

# Anomalies et tensions dans le modèle standard de la cosmologie

*Sources*

- [Anomalies in Physical Cosmology](https://arxiv.org/abs/2208.05018) - Peebles (2022)
- [Cosmology Intertwined: A Review of the Particle Physics, Astrophysics, and Cosmology Associated with the Cosmological Tensions and Anomalies](https://arxiv.org/abs/2203.06142) - Abdalla et al (2022)

---

# La cosmologie quantique

*Sources*

- [Quantum cosmology: a review](https://arxiv.org/abs/1501.04899) - Bojowald (2015)
- [Loop Quantum Cosmology: A brief review](https://arxiv.org/abs/1612.01236) - Agulo et Singh (2016)
- [Conceptual Problems in Quantum Gravity and Quantum Cosmology](https://www.hindawi.com/journals/isrn/2013/509316/) - Kiefer (2013)
- [Quantum Gravity](https://plato.stanford.edu/entries/quantum-gravity/) - Stanford Encyclopedia of philosophy
- [String Cosmology: A Review](https://arxiv.org/abs/0710.2951) - McAllister et Silverstein (2007)
- [Loop Quantum Gravity](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5567241/) - Carlo Rovelli (1998)

---

# Matière noire

*Sources*

- [A History of Dark Matter](https://arxiv.org/pdf/1605.04909.pdf) - Berton et Hooper (2016)
- [Dark matter and cosmic structure](https://arxiv.org/abs/1210.0544) - Frenk and White (2012)
- [La matière noire, une sombre affaire](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p4.pdf) - Combes (2016)
- [Dark matter and cosmic structure](https://arxiv.org/abs/1210.0544) - White et Frenk (2012)
- [The Search for Dark Matter Is Dramatically Expanding](https://www.quantamagazine.org/physicists-are-expanding-the-search-for-dark-matter-20201123/) - Quanta Magazine
- [Axion Dark Matter: What is it and Why Now?](https://arxiv.org/abs/2105.01406) - Chadha-Day et al (2021)
- [In the hunt for dark matter, are axions our best bet?](https://www.space.com/dark-matter-axions-best-bet) - Space.com (2022)
- [Dark Matter Candidates from Particle Physics and Methods of Detection](https://arxiv.org/abs/1003.0904) - Feng et al (2010)
- [Dark Matter Candidates](https://www.slac.stanford.edu/econf/C040802/papers/L002.PDF) - Baltz (2004)
- [What is Dark Matter Made Of? These Are the Top Candidates](https://www.discovermagazine.com/the-sciences/what-is-dark-matter-made-of-these-are-the-top-candidates) - Discover Magazine
- [What could dark matter be?](https://www.symmetrymagazine.org/article/what-could-dark-matter-be) - Symmetry Magazine (2015)
- [A Brief Review on Primordial Black Holes as Dark Matter](https://www.frontiersin.org/articles/10.3389/fspas.2021.681084/full) - Villanueva-Domingo et al (2021)
- [(Just can’t get enough) primordial black holes](https://astrobites.org/2018/02/13/just-cant-get-enough-primordial-black-holes/) - Astroites (2018)
- [Exploring the high-redshift PBH-ΛCDM Universe: early black hole seeding, the first stars and cosmic radiation backgrounds](https://arxiv.org/abs/2109.08701) - Cappelluti et al (2021)
- [Neutrinos Stériles et Matière Noire font Bon Ménage](https://www.ca-se-passe-la-haut.fr/2014/05/neutrinos-steriles-et-matiere-noire.html) - Ca se passe là-haut (2014)
- [A History of Dark Matter](https://arxiv.org/abs/1605.04909) - Berton et Hooper (2016)
- [WIMPs and MACHOs](https://sites.astro.caltech.edu/~george/ay20/eaa-wimps-machos.pdf) - ENCYCLOPEDIA OF ASTRONOMY AND ASTROPHYSICS (2002)
- [Forget WIMPs, Axions And MACHOs: Could WIMPzillas Solve The Dark Matter Problem?](https://www.forbes.com/sites/startswithabang/2018/05/09/forget-wimps-axions-and-machos-could-wimpzillas-solve-the-dark-matter-problem/?sh=3a0cc9fc3a22) - Ethan Siegel
- [Supersymmetry](https://home.cern/fr/science/physics/supersymmetry) - CERN
- [WIMPZILLAS!](https://arxiv.org/abs/hep-ph/9810361) - Kolb et al (1998)
- [Wimpzillas leave tracks say astronomers](https://www.newscientist.com/article/dn2342-wimpzillas-leave-tracks-say-astronomers/) - New Scientist 
- [Superheavy dark matter through Higgs portal operators](https://arxiv.org/abs/1708.04293) - Kolb et al (2019)
- [Oh-My-God (Particle), It’s WIMPzilla!](https://mcdonaldinstitute.ca/apbites/2019-09-11-its-wimpzilla/) - AstroParticle Bites (2019) 
- [WIMPZillas: The Biggest WIMPs](https://darkmatterdarkenergy.com/2018/06/26/wimpzillas-the-biggest-wimps/) - Stephen Perrenod
- [Axion dark matter: What is it and why now?](https://www.science.org/doi/10.1126/sciadv.abj3618) - Chadha-Day et al (2022)
- [Les sym ́etries au fondement de la physique moderne](https://web.umons.ac.be/app/uploads/sites/31/2019/01/News-Complexys_J.-Fran%C3%A7ois.pdf) - Jordan François
- [Axions : l'Autre Matière Noire](https://www.ca-se-passe-la-haut.fr/2013/11/axions-lautre-matiere-noire.html) - Ca se passe là-haut (2013) 
- [The universe according to Frank Wilczek](https://www.templetonprize.org/the-universe-according-to-frank-wilczek/) - Templeton Prize
- [OSQAR](https://home.cern/fr/science/osqar) - CERN
- [Le chasseur de matière noire XENON1T détecte un signal suspect](https://www.cieletespace.fr/actualites/le-chasseur-de-matiere-noire-xenon1t-detecte-un-signal-suspect) - Ciel et Espace (2020)
- [La matière noire pointe-t-elle enfin le bout de son nez dans Xenon 1T ?](https://www.futura-sciences.com/sciences/actualites/nouvelle-physique-matiere-noire-pointe-t-elle-enfin-bout-son-nez-xenon-1t-60517/) - Ciel et Espace (2020)
- [L'asymétrie CP ou le privilège de la matière](http://www.astrosurf.com/luxorion/quantique-asymetrie-cp.htm) - Astrosurf
- [Matière noire (axions) : nouveaux résultats de deux haloscopes](https://www.ca-se-passe-la-haut.fr/2021/05/matiere-noire-axions-nouveaux-resultats.html) - Ca se passe là-haut (2021)
- [Limits on the Macho content of the Galactic Halo from the EROS-2 Survey of the Magellanic Clouds](https://www.aanda.org/articles/aa/abs/2007/26/aa6017-06/aa6017-06.html) - Tisserand et al (2007)
- [The OGLE view of microlensing towards the Magellanic Clouds – IV. OGLE-III SMC data and final conclusions on MACHOs](https://academic.oup.com/mnras/article/416/4/2949/976362) - Wyrzykowski et al (2011)
- [MACHOs](https://astronomy.swin.edu.au/cosmos/m/MACHOs) - ENCYCLOPEDIA OF ASTRONOMY AND ASTROPHYSIC
- [Introduction to Dark Matter](https://link.springer.com/chapter/10.1007/978-3-030-95852-7_1) - Derek Jackson Kimball et Dmitry Budker (2023)   
- [MaCHO Dark Matter (What is it?)](https://youtu.be/9GzB7mNWXAE) - Chris Pattison  
- [The Dark Matter Enigma](https://inference-review.com/article/the-dark-matter-enigma) - Jean-Pierre Luminet (2020)
- [Neutrinos stériles : MINOS+ contredit MiniBooNE](https://www.ca-se-passe-la-haut.fr/2019/03/neutrinos-steriles-minos-contredit.html) - Ca se passe là-haut (2019)

---

The Swiss-American astronomer Fritz Zwicky is arguably the most famous and widely
cited pioneer in the field of dark matter. In 1933, he studied the redshifts of various galaxy
clusters, as published by Edwin Hubble and Milton Humason in 1931 [162], and noticed
a large scatter in the apparent velocities of eight galaxies within the Coma Cluster, with
differences that exceeded 2000 km/s [346]. The fact that Coma exhibited a large velocity
dispersion with respect to other clusters had already been noticed by Hubble and Humason,
but Zwicky went a step further, applying the virial theorem to the cluster in order to estimate
its mass

Zwicky started by estimating the total mass of Coma to be the product of the number
of observed galaxies, 800, and the average mass of a galaxy, which he took to be 109 solar
masses, as suggested by Hubble. He then adopted an estimate for the physical size of the
system, which he took to be around 106 light-years, in order to determine the potential energy
of the system. From there, he calculated the average kinetic energy and finally a velocity
dispersion. He found that 800 galaxies of 109 solar masses in a sphere of 106 light-years
should exhibit a velocity dispersion of 80 km/s. In contrast, the observed average velocity
dispersion along the line-of-sight was approximately 1000 km/s. From this comparison, he
concluded:
If this would be confirmed, we would get the surprising result that dark matter
is present in much greater amount than luminous matter.

The overall situation was that of a community that was struggling to find a unified
solution to a variety of problems. The dark matter hypothesis was not commonly accepted,
nor was it disregarded. Instead, there was a consensus that more information would be
needed in order to understand these systems.

n the 1960s, Kent Ford developed an image tube spectrograph that Vera Rubin and he
used to perform spectroscopic observations of the Andromeda Galaxy. The observations of
the M31 rotation curve Rubin and Ford published in 1970 [267] represented a step forward in
terms of quality.

the idea that galaxies and
galaxy clusters are embedded in massive dark matter halos
was not widely discussed until the 1970’s. This changed
with the influential, observationally based papers of Os-
triker, Peebles and Yahil [4] and Einasto, Kaasik, Saar and
Chernin [5] and with Ostriker and Peebles’ theoretical ar-
gument that massive halos are required to stabilize spiral
galaxy disks [6]. Within four years, a hierarchically merg-
ing population of dark matter halos formed the basis for the
galaxy formation scenario proposed by White and Rees [7],
providing the gravitational potential wells within which
gas cools and condenses to form galaxies. 

t was also in 1970 that the first explicit statements began to appear arguing that ad-
ditional mass was needed in the outer parts of some galaxies, based on comparisons of the
rotation curves predicted from photometry and those measured from 21 cm observations

Morton Roberts was among the first to recognize the implications of the observed flatness
of galactic rotation curves. Together with R. Whitehurst, he published in 1972 a rotation
curve of M31 that extended to 120 arcminutes from its center [335]. In 1973, together with
Arnold Rots, he extended the analysis to M81 and M101, and argued that these spiral
galaxies each exhibited flat rotation curves in their outer parts [260] (see Fig. 3). The
authors’ interpretation of these data was unambiguous:
The three galaxies rotation curves decline slowly, if at all, at large radii,
implying a significant mass density at these large distances. It is unreasonable
to expect the last measured point to refer to the ‘edge’ of the galaxy, and we
must conclude that spiral galaxies must be larger than indicated by the usual
photometric measurements [...]. The present data also require that the mass to
luminosity ratio vary with radius increasing in distance from the center.

In 1978 ( ?) Rubin, Ford and Norbert Thonnard published optical rotation curves for ten high-luminosity spiral galaxies and found that they were flat out to the outermost
measured radius [268]. This work has become one of the most well-known and widely cited
in the literature, despite the fact that the optical measurements did not extend to radii as
large as those probed by radio observations, thus leaving open the possibility that galaxies
may not have dark matter halos, as pointed out, for example, by Agris J. Kalnajs in 1983
(see the discussion at the end of Ref. [150]) and by Stephen Kent in 1986 [175]. Rubin, Ford
and Thonnard themselves acknowledged the credit that was due to the preceding analyses:
Roberts and his collaborators deserve credit for first calling attention to flat
rotation curves. [...] These results take on added importance in conjunction with
the suggestion of Einasto, Kaasik, and Saar (1974) and Ostriker, Peebles and
Yahil (1974) that galaxies contain massive halos extending to large r

By the late 1980s, the hypothesis that the missing mass
consists of one or more yet-unknown subatomic particle species had gained enough support
to become established as the leading paradigm for dark matter.

Le terme « matière noire »
(ou dark matter en anglais) commence à
apparaître vers la fin des années 1970 [6]

Jusqu’en 1980, pour la majorité des
astronomes la matière noire devait être
composée de matière ordinaire, donc de
baryons. En 1984, il est définitivement
établi que l’essentiel de la matière noire ne
peut pas être de la matière baryonique,
ceci pour deux raisons. La première est
que l’abondance du deutérium et de
l’hélium formés dans la nucléosynthèse
primordiale n’est compatible avec les
observations que si la fraction de baryons
ne représente que 5% de la densité critique
(la densité nécessaire pour arrêter
l’expansion de l’Univers par sa gravité).
D’autre part, la formation des structures
dans l’Univers sous l’effet de la gravitation
est très lente à cause de l’expansion, et la
matière ordinaire, ionisée et très couplée
avec les photons, prend du retard pour
s’effondrer sous l’effet de la gravité. Si l’on
attend que la température dans l’expansion
tombe en-dessous de 3000 degrés, pour
que l’hydrogène se recombine et que le
gaz s’effondre, il n’y a plus assez de temps
pour former les galaxies. Il est donc
nécessaire de disposer de matière exotique,
sans interactions avec la lumière, qui peut
s’effondrer tôt après le Big-Bang, avant
même que la matière ordinaire puisse le
faire [7]. Le nom de cette matière devrait
alors être matière transparente, plutôt que
matière noire : les rayons lumineux la
traversent !

Il faut noter aussi que, dès les premières
découvertes dans les années 1930, la
solution « législative », c’est-à-dire de
changement des lois de la physique, a été
avancée. Il est toutefois plus facile
d’ajouter de la matière invisible à chaque
fois qu’il en manque que d’imaginer et
d’établir une théorie généralisant la loi de
la gravité de Newton et la relativité
générale. Une percée a été faite par le
théoricien israélien Moti Milgrom dès
1983 [8], avec un modèle de gravité
modifiée en champ faible (MOND) qui
rencontre beaucoup de succès à l’échelle
des galaxies, mais des problèmes aux grandes
échelles

Aujourd’hui, les observations ont énor-
mément progressé, et nous avons un
recensement détaillé des régions de
l’Univers où il manque de la masse, i.e. où
la masse dynamique est supérieure à la masse
visible. Au niveau des galaxies massives
(voir l’encadré), la matière noire apparaît
surtout dans leurs régions extérieures,
comme un halo entourant le système
visible. Dans notre galaxie, la Voie lactée,
le besoin de masse supplémentaire ne se
fait sentir qu’au-delà de la position du
Soleil, qui est situé à 25 000 années-lumière
du centre (soit la moitié du rayon du
disque d’étoiles). 

Dans ce domaine, un outil s’est considé-
rablement développé ces vingt dernières
années : les lentilles gravitationnelles
en règle générale,
les images de toutes les galaxies d’arrière-
plan sont déformées, allongées de façon
minime, et c’est l’ensemble statistique de
ces centaines ou milliers de déformations
qui renseigne sur la quantité de matière
noire. C’est le phénomène de lentille faible,
ou cisaillement gravitationnel, qui permet
de cartographier la matière invisible

Les cartes de la matière noire obtenues
ainsi, à l’échelle des amas de galaxies,
correspondent dans la grande majorité des
cas aux cartes de matière visible. Il existe
pourtant des cas très intéressants de sépara-
tion spatiale des deux matières : les collisions
violentes entre sous-amas. Le prototype de
ces collisions est l’amas dit du Boulet
(photo p. 5). Trois sortes de matière sont
représentées ici : les galaxies qui se com-
portent comme un milieu sans collision, la
matière noire cartographiée par les lentilles
gravitationnelles, et un gaz très chaud, à des
millions de degrés, qui émet des rayons X.
Le gaz est dissipatif, il est stoppé entre les
deux amas par l’onde de choc de la collision.
Or dans les amas riches de galaxies, le gaz
chaud contient l’essentiel de la masse visible.
La matière visible ne coïncide plus avec la
matière noire ! Celle-ci reste concentrée
sur les galaxies. Cette séparation a permis
de conclure à l’existence réelle de matière
invisible, pouvant se déconnecter complè-
tement de la matière ordinaire.

dans les années 1980, les
neutrinos ont été considérés comme can-
didats à la matière noire, vu leur abondance
dans l’Univers, presque comparable à celle
des photons. Il suffirait que leur masse soit
le dix millionième de la masse de l’électron
pour représenter toute la matière noire.
Aujourd’hui la masse des neutrinos est
estimée à 25 fois moins, et ne constituerait
donc que 4% de la matière noire tout au plus.
De toute façon, les simulations numériques
des années 1980-90 montraient clairement
que les neutrinos ne pouvaient pas constituer
toute la matière noire, car faisant partie de
la matière noire « chaude ». En effet, dès
le changement de paradigme de 1984 sur
la nature de la matière noire, les divers
candidats peuvent être classés selon leur
« température ». Le caractère « chaud » ou
« froid » repose sur le critère de la vitesse
moyenne qu’ont les particules lorsqu’elles
se découplent du plasma primitif, où photons,
baryons et matière noire sont en équilibre.
Selon que les particules sont relativistes ou
non lorsqu’elles se découplent de la soupe
primitive juste après le Big-Bang, on parle
de matière noire chaude (Hot Dark Matter
ou HDM), ou de matière noire froide
(Cold Dark Matter ou CDM). Les neutrinos
font clairement partie de la première caté-
gorie (HDM), même si aujourd’hui ils
sont complètement ralentis et ne sont
restés relativistes que durant les premiers
3-4% de l’âge de l’Univers. La conséquence
de leur vitesse relativiste est que ces particules
trop chaudes vont empêcher les fluctuations
de matière à petite échelle de se concentrer
sous l’effet de leur propre gravité. En effet,
les particules de matière noire chaude par-
courent pratiquement les mêmes distances
que les photons : elles ont un libre parcours
moyen presque égal à l’horizon(a), et leur
« pression » stabilise la matière sur les échelles
inférieures. Aux échelles supérieures, la
matière pourra s’effondrer sous l’influence
de sa propre gravité. Les structures de
taille inférieure devront alors se former par
fragmentation, dans un schéma top-down,
du haut vers le bas.
Ce schéma ne correspond pas aux
observations, notamment le spectre de
masse des galaxies aux différents âges de
l’Univers, qui favorisent le schéma
contraire bottom-up, ou hiérarchique. Les
premières structures à se former sont les
plus petites structures instables, qui forment
ensuite les grandes structures par fusions
successives

 Les simulations numériques de
l’évolution de l’Univers avec les différents
types de matière noire (fig. 4) ont montré
que le modèle CDM, maintenant appelé
modèle standard, est celui qui représente
le mieux la formation des grandes struc-
tures et explique toutes les observations du
fond cosmologique micro-onde

Pourtant, à l’échelle des galaxies, les
prédictions du modèle CDM ne sont
plus compatibles avec les observations. En
particulier, dans ce modèle, la matière noire
standard s’accumule au centre des galaxies,
ce qui est contraire à ce que montrent les
mesures de courbes de rotation (fig. 1). De
plus, les simulations cosmologiques dans le
modèle standard (CDM) prédisent l’existence
de milliers de galaxies satellites autour des
galaxies massives, milliers de galaxies qui
ne sont pas observées. 

L’autre problème du modèle standard
CDM est l’accumulation des particules de
matière noire au centre des galaxies, en un
pic de densité qui n’est pas observé. Pour
le résoudre, il a été proposé un modèle de
particules de matière noire qui entrent en
collisions entre elles. Ces collisions agi-
raient surtout dans les parties les plus
denses au centre, et auraient pour effet de
disperser les particules et de rendre leurs
trajectoires aléatoires dans cette région.
Elles se distribueraient alors selon un
nuage étendu, un « cœur », au lieu d’un
pic de matière noire. Il reste toutefois
beaucoup de problèmes à résoudre pour
expliquer la taille des cœurs différente
d’une galaxie à l’autre. D’autre part, une
limite supérieure de la section efficace de
collisions entre particules de matière noire
a été déduite des amas de galaxies en collision,
comme le Boulet (photo p. 5).
Enfin, d’autres solutions sont explorées pour
résoudre le problème des galaxies satellites
manquantes : recourir à la matière noire
« tiède » (fig. 4). Les structures à très petite
échelle sont alors supprimées, mais une grande
partie du modèle hiérarchique subsiste


Et si finalement il n’existait pas de matière
inconnue, mais qu’une modification de la
théorie de la gravité pouvait résoudre le
problème de la masse manquante dans les
galaxies et de la formation des structures de
l’Univers ? À l’échelle des galaxies, le modèle
de gravité modifiée qui représente le mieux
les observations est celui de M. Milgrom,
MOND, qui reproduit les courbes de
rotation des naines aux géantes, de même
que les relations d’échelle (fig. 1 et fig. 2).
En champ de gravité faible, la force de
gravité ne décroît alors plus avec la distance
en 1/R 2, mais en 1/R. 

Do galaxies always contain dark matter? Since most of the dark
matter is in the outskirts of a galaxy, possible exceptions would be
galaxies that have been tidally stripped, and the dwarfs that might
have formed by dissipative settling from tidal streams. Apart from
such effects, dark halos are expected to be universal in the standard
ΛCDM cosmology. There are possibly interesting challenges to this
prediction. The large S0 galaxy NGC 3115 is in the field, which
is unusual because most S0s at this luminosity are in clusters. If
it has a dark matter halo with mass typical of its stellar mass then
the halo of NGC 3115 must be much less dense than usual, the
halo core radius much broader (Cappellari, Romanowsky, Brodie, et
al. 2015). The low surface brightness satellites NGC1052-DF2 and
NGC1052-DF4 of the elliptical NGC 1052 also look like exceptions.
Keim, van Dokkum, Danieli, et al. (2022) argue that “the dark matter
halo masses of these galaxies cannot be much greater than their
stellar masses.” It is too soon to declare a challenge to the ΛCDM
theory from the evidence of galaxies with little dark matter, but the
development of the empirical evidence will be worth following

### L'amas de la balle (Bullet cluster)

The combination of X-ray and gravitational lensing data for the`bullet cluster’
1E0657-56 (a merger of two large clusters) confirms that dark matter is very
different in nature from the bulk of normal, baryonic matter (X-ray emitting gas)

X-ray emitting gas
feels ram pressure
whereas dark matter
(and galaxies) pass
through (at least
relatively) unhindered.

Consistent with weakly
interacting, cold dark
matter (CDM) paradigm

*Image composite de l'amas de la balle. Le gaz intergalactique chaud est en rose, et la distribution de matière noire en bleu. Crédit : X-ray: NASA/CXC/CfA/M.Markevitch, Optical and lensing map: NASA/STScI, Magellan/U.Arizona/D.Clowe, Lensing map: ESO WFI. [Source](https://www.esa.int/ESA_Multimedia/Images/2007/07/The_Bullet_Cluster2).*

![bulletcluster](https://www.esa.int/var/esa/storage/images/esa_multimedia/images/2007/07/the_bullet_cluster2/10084622-2-eng-GB/The_Bullet_Cluster_pillars.jpg)

### Les principaux candidats à la matière noire

- ***Les axions***
> L’existence de ces particules hypothétiques très peu massive (entre 1 et 1000 μeV) a été postulée en 1977 par Roberto Peccei et Helen Quinn comme solution d’une énigme du modèle standard : le ***problème CP fort***. Pour le comprendre, il faut revenir sur le concept de ***symétries de jauge***, qui est fondamental en physique des particules. Une symétrie de jauge, c’est une transformation qui laisse les propriétés d’un système physique inchangées, invariantes. Par exemple, si on tourne autour du centre d’une boule de billard, l’apparence de la boule reste inchangée. L’ensemble des rotations 3D possibles constituent donc les symétries de la boule. En physique des particules, il existe 3 transformations fondamentales : la ***symétrie C*** (conjugaison de charge), qui change le signe de la charge électrique et transforme une particule en son antiparticule, la ***symétrie P*** (transformation de parité), qui inverse toutes les coordonnées spatiales d’une particule, et la ***symétrie T*** (renversement du temps), qui inverse la flèche du temps. On peut combiner ces opérations pour créer d’autres types de symétries, comme la symétrie CP ou la symétrie CPT. Ainsi, en vertu de la symétrie CP, les particules et les antiparticules devraient obéir aux mêmes lois physiques. Autrement dit, matière et antimatière auraient dû être produits en quantités égales dans l’univers primordial. Pourtant, l’univers qu’on observe est indéniablement dominé par la matière, et l’antimatière est quasi-inexistante à l’échelle cosmique.  Cette victoire de la matière peut être expliquée en partie par la ***brisure de la symétrie CP***. Autrement dit, il y aurait des différences entre les lois qui régissent l’évolution de la matière et celles qui régissent l’antimatière. En 1964, une équipe de physiciens du laboratoire national de Brookhaven a mis en évidence expérimentalement que la symétrie CP pouvait être effectivement brisée dans le phénomène de désintégration du kaon, qui met en jeu l’interaction faible. Cette particule se désintègre en effet préférentiellement en positron plutôt qu’en électron. L’interaction faible brise donc la symétrie CP. Mais qu’en est-il de l’interaction forte ? Selon la ***chromodynamique quantique*** (la théorie qui décrit les interactions entre les quarks et les gluons qui composent les protons et les neutrons), celle-ci devrait aussi briser la symétrie CP. Or, toutes nos expériences nous montrent qu’elle semble malgré tout être préservée. Le problème CP fort consiste à se demander pourquoi les lois de la chromodynamique quantique préservent la symétrie CP. Pour résoudre ce problème, Peccei et Quinn ont postulé l’existence d’un nouveau champ quantique interagissant avec les gluons. Et les particules qui composent ce champ sont les fameux axions. Pourquoi axion ? En référence aux axes de symmétrie, mais aussi, de manière humoristique, à une marque de détergent. Frank Wilczek, qui a eu l’honneur de nommer cette particule, trouvait en effet que celle-ci permettait de « nettoyer » le problème. De nos jours, les axions font partie des candidats favoris des physicien.nes à la matière noire. Pour les détecter, ils se servent d’une étrange propriété de l’axion : lorsqu’il est  soumis à un fort champ magnétique, il se désintègre en paires de photons. On peut alors utiliser un instrument appelé ***haloscope*** pour détecter des axions du halo de matière noire galactique. Celui-ci est composé d’un électroaimant supraconducteur et d’une cavité résonnante qui amplifie le signal photonique (qui arrive dans le domaine des micro-ondes). Le plus célèbre de ces haloscopes est une expérience nommée ADMX (Axion Dark Matter eXperiment), construite à l’université de Washington. Cet instrument a permis de mieux contraindre la plage de masses de l’axion. Et en 2020, l’expérience XENON1T, située dans le laboratoire souterrain de San Grasso en Italie, aurait découvert les premiers indices sérieux de l’existence d’axions en provenance du Soleil. La découverte reste cependant à confirmer.

- ***Les WIMPs***
> Les WIMPs (accronyme de “Weakly Interacting Massive Particles”) désignent une classe de particules hypothétiques qui n’interagiraient avec la matière ordinaire que via la force nucléaire faible. Ils ont constitué le paradigme dominant dans la quête de la matière noire depuis les années 80, grâce à de solides fondations théoriques et des prédictions testables expérimentalement. Si elles existent, les WIMPs devraient être entre 1 et 1000 fois plus massifs que les protons (1-1000 GeV). Parmi les WIMPs, le candidat le plus étudié est (de loin) le ***neutralino***. L'existence de cette particule est prédite par la théorie de la ***supersymétrie***, une extension du modèle standard de la physique des particules. Celle-ci fait l’hypothèse qu’il existerait un lien fondamental (une symétrie) entre les deux grandes catégories de particules : les bosons (particules dont le spin est entier ou nul) et les fermions (particules dont le spin est demi-entier ou nul). Dans cette théorie, toutes les particules du modèle standard possèdent un « partenaire supersymétrique » (ou ***superpartenaire***) plus massif dont le spin diffère du sien de moitié. Ainsi, des bosons auraient des superpartenaires fermioniques et inversement. Les superpartenaires fermioniques des bosons ont le même nom que ces derniers, mais avec le suffixe -ino (ex : le superpartenaire du photon est le photino, et celui du Higgs est le higgsino), tandis que les superpartenaires bosoniques des fermions ont le même nom que ces derniers avec le préfixe s- (ex : les superpartenaires des quarks sont les squarks, et ceux des électrons sont les sélectrons). Dans ce cadre, le neutralino est une combinaison de 3 superpartenaires : le photino, le higgsino et le Z-ino. Selon l'extension supersymétrique minimale du modèle standard de la physique des particules, il existerait 4 neutralinos possibles, et le plus léger d’entre eux est souvent considéré comme l’un des candidats à la matière noire le plus prometteur. Pourquoi ? Les neutralinos étant leur propre antiparticules, ils sont sensés s’annihiler entre eux dans les premiers instants du cosmos. Mais à mesure que l’univers s’étend, le  taux d’anihilation devient négligeable et l’abondance des neutralinos est « gelée » à une densité relique que l’on peut calculer. Pour un neutralino de l’ordre de 100 fois la masse du proton, cette densité relique correspond à la densité de la matière noire mesurée à l’échelle cosmique ! On appelle cette coïncidence intrigante le ***miracle du WIMP***, et c’est elle qui explique en grande partie l’engouement des scientifiques sur ce candidat. Une manière de détecter des WIMPs est d’observer les flux de rayons gamma produits dans les halos de matière noire suffisamment denses lorsque des paires de ces particules fantômatiques s’annihilent entre elles. Une autre technique consiste à faire collisionner des particules à très grande vitesse (au LHC par exemple), en espérant que les débris de la collision révèlent des signes de WIMPs. Une dernière technique consiste à construire de grandes cuves souterraines (ie à l’abri des rayons cosmiques) remplies d’atomes de xénon, comme le détecteur LUX (Large Underground Xenon experiment) situé aux États-Unis, et d’attendre qu’une particule de matière noire interagisse avec les atomes de la cuve en produisant des électrons et des photons. Mais après des décennies de recherche et des expériences de plus en plus précises, aucun signe de neutralinos ou plus généralement de WIMPs n’a été détecté pour le moment. Et si leur existence n’est pas encore exclue, la communauté scientifique commence à privilégier d’autres candidats.

- ***Les WIMPZILLAS***
> Contrairement aux WIMPs, qui sont des particules très légères, les WIMPZILLAS sont des particules hypothétiques ultra-lourdes, dont la masse pourrait être comprise entre 10^9 et 10^16 GeV, et qui interagiraient très faiblement avec la matière. Elles ont été proposées à la fin des années 90 par Edward Kolb, Daniel Chung et Antonio Riotto. Ces particules auraient pu être créées dans les conditions de l’univers primordial, pendant la période de réchauffement qui fait suite à l’inflation. Si ces WIMPZILLAS existent, elles pourraient potentiellement résoudre un autre problème de l’astrophysique : la nature des rayons cosmiques d’ultra-haute énergie. En effet, les WIMPZILLAS pourraient se désintégrer en particules de hautes énergies (essentiellement des rayons gamma dont l’énergie est supérieure à 10^9 GeV) qui pourraient voyager jusqu’à la Terre. Ces particules pourraient être détectées par les futurs observatoires Pierre Augier (en Argentine) et l’Extreme Universe Space Observatory (à bord de l’ISS), en ciblant le centre de la Galaxie. Mais les rayons cosmiques d’ultra-haute énergie sont des événements ultra-rares, et établir des statistiques prend du temps. En attendant, il est difficile de  distinguer les rayons cosmiques issus de la désintégration des WIMPZILLAS des rayons cosmiques issus d’autres processus astrophysiques.

- ***Les MACHOs***
> Les MACHOs (acronyme de MAssive Compact Halo Objects, nommé ainsi en opposition aux WIMPs par l’astronome Kim Griest) sont des astres constitués de matière ordinaire (baryonique) qui seraient si peu lumineux qu’ils n’auraient pas été détectés par les relevés astronomiques jusqu’à présent. Ces astres incluent des étoiles peu massives (naines rouges), des cadavres stellaires (naines blanches, étoiles à neutrons, trous noirs), des planètes errantes et des naines brunes. Les MACHOs sont des candidats intrigants à la matière noire, car ils ne font pas intervenir de matière exotique ou de nouvelle physique. Comme les télescopes ne sont pas assez sensibles pour capter leur lumière, une manière de les recenser fait intervenir ***l’effet de microlentille gravitationnelle*** : lorsqu’un MACHO passe devant une étoile lointaine en arrière plan, il courbe l’espace autour de lui et altère le trajet des photons provenant de l’étoile. Cela se traduit par une augmentation passagère de la luminosité apparente de l’étoile au moment du passage du MACHO. De tels alignements sont très rares (ils concernent environ une étoile sur un million), mais en observant de grandes quantités d’étoiles (des millions) sur des temps suffisamment longs (des années), on peut assigner une borne supérieure au nombre de MACHOs (ainsi que leur masse) dans une galaxie donnée. C’est notamment le travail qu’ont réalisé les collaborations MACHO, EROS (Expérience pour la Recherche d'Objets Sombres) et OGLE (Optical Gravitational Lensing Experiment), en observant les étoiles des nuages de Magellan pendant plusieurs années. Ces analyses ont montré que les MACHOs ne pouvaient pas représenter plus de 8 % de la masse « noire » du halo galactique. D’autres études indépendantes issues de l’analyse du fond diffus cosmologique et de l’abondance des éléments légers dans l’univers ont montré que moins de 20 % de la matière de l’univers était d’origine baryonique. Il y a donc un consensus selon lequel les MACHOs ne constituent qu’une petite fraction de la matière noire à l’échelle du cosmos. De nos jours, les seuls MACHOs qui résistent encore aux contraintes fournies par le CMB et la nucléosynthèse primordiale sont les ***trous noirs primordiaux***.

- ***Les trous noirs primordiaux***
> Les observations actuelles ont éliminé les trous noirs stellaires (issus de la mort d’étoiles massives) comme candidats à la matière noire. Il n’y en a juste pas assez pour expliquer nos observations. Mais les trous noirs stellaires ne sont pas les seuls trous noirs possibles. En 1971, Stephen Hawking a proposé l’hypothèse que la matière noire pourrait être constituée de trous noirs qui se seraient formés dans les conditions ardentes de l’univers primordial, pendant les quelques secondes qui ont suivi sa période inflationnaire (lorsque la température de l’univers est passée sous la barre de 10 milliards de de degrés), à partir de surdensités de matières qui se seraient effondrées sur elles-mêmes sous leur propre poids. Les trous noirs primordiaux représentent un candidat attrayant parce qu’ils ne requièrent pas de nouveau type de matière exotique. La masse de ces trous noirs primordiaux pourrait s’étendre de 10^-10 à 10^7 masses solaires, c'est-à-dire de la masse d’un astéroïde (et plus petits qu’un grain de sable) à celle d’un trou noir supermassif. Ceux plus légers que cette limite basse auraient fini par s’évaporer complètement avant nos jours. La détection des ondes gravitationnelles émises par la fusion de paires de trous noirs en 2016 a remis sur le devant de la scène l’hypothèse des trous noirs primordiaux comme candidats à la matière noire (les trous noirs détectés étaient en effet légèrement plus massifs que ce à quoi on s’attendrait pour des trous noirs stellaires). Il est aussi possible que les trous noirs primordiaux aient constitué les graines des trous noirs supermassifs que l’on retrouve aujourd’hui au centre de toutes les grosses galaxies.

- ***Les neutrinos stériles***
> Les neutrinos stériles sont un (ou plusieurs) type(s) hypothétique(s) de ***neutrinos*** très massifs, électriquement neutres, n’interagissant avec la matière que par le biais de leur gravité (contrairement aux 3 types -saveurs- de neutrinos du modèle standard de la physique des particules, qui interagissent aussi avec la matière par le biais de la force nucléaire faible), et stables sur les échelles de temps cosmologiques. Ils ont été proposés en 1977 par Benjamin Lee et Steven Weinberg. Cette hypothèse a connu son heure de gloire au milieu des années 90, lorsque l’expérience LSND (Liquid Scintillator Neutrino Detector) située au Nouveau Mexique avait montré qu’il existait un léger excès d'***(anti-)neutrinos électroniques*** par rapport aux deux autres saveurs (***muonique*** et ***tauique***). Ce résultat pouvait s’expliquer si les (anti-)neutrinos pouvaient se transformer temporairement en une quatrième saveur sur leur trajet. Si leur masse est supérieure au keV, les neutrinos stériles pourraient rendre compte de l’intégralité de la matière noire dans l’univers. La désintégration des neutrinos stériles produirait des photons qui seraient détectables sous la forme d’une raie spectrale dans le domaine des rayons X. En 2014, une telle raie d’émission à 3,55 keV a été détectée dans des amas de galaxies observées par les télescopes spatiaux XMM-Newton et Chandra. Une telle détection est compatible avec la désintégration d’un neutrino stérile d’une masse de 7 keV. Mais ce résultat fait encore débat dans la communauté scientifique. 

*Candidats à la matière noire. Crédit : Samuel Velasco/Quanta Magazine [Source](https://www.quantamagazine.org/physicists-are-expanding-the-search-for-dark-matter-20201123/)*

![DMparticles](https://d2r55xnwy6nx47.cloudfront.net/uploads/2020/11/Dark_Matter_graphic_2880x1620_Lede_final.svg)

*Liste plus exhaustive de candidats. Crédit : Alves Batista et al (2021). [Source](https://arxiv.org/abs/2110.10074)*

![DMcandidates](https://i.imgur.com/VgeZTI3.png)

# Energie sombre

*Sources*

- [Dark Energy: A Short Review](https://arxiv.org/abs/1401.0046) - Mortonson et al (2013)
- [Dark Energy Versus Modified Gravity](https://www.annualreviews.org/doi/abs/10.1146/annurev-nucl-102115-044553) - Joyce et al (2016)
- [Dark energy: A brief review](https://link.springer.com/article/10.1007/s11467-013-0300-5) - Li et al (2013)
- [Changement de rythme dans l’expansion de l’Univers : Un premier rôle pour le côté obscur](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p12.pdf) - Palanque Delabrouille (2016)
- [L'histoire de l'expansion de l'Univers dévoilée](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2022/01/refdp202271p22.pdf) - Burtin (2022)
- [Avancées de la recherche Cisaillement gravitationnel et sondage de l’Univers](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2006/01/refdp20061p5.pdf) - Mellier (2006)
- [La constante cosmologique : la plus grande erreur d’Einstein](https://books.openedition.org/cdf/9443?lang=fr) - Combes
- [Listening for the Size of the Universe](https://astro.ucla.edu/~wright/BAO-cosmology.html) - Edward Wright (2014)
- [Astronomy Jargon 101: Baryon Acoustic Oscillations](https://www.universetoday.com/153273/astronomy-jargon-101-baryon-acoustic-oscillations/) - Paul Sutter
- [BOSS Measures the Universe to One-Percent Accuracy](https://newscenter.lbl.gov/2014/01/08/boss-one-percent/) - Berkeley Lab (2014)
- [DESI lance un programme de 5 ans pour mieux comprendre l’univers](https://insidetheperimeter.ca/fr/desi-launches-five-year-quest-to-understand-the-universe/) - PI (2021)
- [La distribution des galaxies, une fenêtre sur l'Univers primordial](https://www.pourlascience.fr/sd/cosmologie/la-distribution-des-galaxies-une-fenetre-sur-l-univers-primordial-11827.php) - Pour la Science (2014)
- [What are baryonic acoustic oscillations?](https://sci.esa.int/web/euclid/-/what-are-baryonic-acoustic-oscillations-) - ESA
- [What the hell are Baryon Acoustic Oscillations?](https://medium.com/starts-with-a-bang/what-the-hell-are-baryon-acoustic-oscillations-cfee6d726538) - Ethan Siegel
- [Dark Energy Spectroscopic Instrument (DESI) Creates Largest 3-D Map of the Cosmos](https://www.ifae.es/news/2022/01/13/dark-energy-spectroscopic-instrument-desi-creates-largest-3-d-map-of-the-cosmos/) - IFAE (2022)
- [Listening to the sound of the universe](https://web.physics.ucsb.edu/~jatila/CMB-sounds/CMB/) - Jatila van der Veen
- [Dark matter in galaxy clusters](https://www.slac.stanford.edu/econf/C070730/talks/allen_080607.pdf) - Steve Allen
- [PROBES OF COSMIC ACCELERATION](https://ned.ipac.caltech.edu/level5/March08/Frieman/Frieman7.html) - Frieman (2008)
- [Probing Dark Energy with Galaxy Clusters](https://www.nasa.gov/mission_pages/chandra/probing-dark-energy-with-galaxy-clusters.html) - NASA (2016)
- [Probing dark energy via galaxy cluster outskirts](https://academic.oup.com/mnras/article/457/3/3266/2588928) - Morandi et Sun (2016)
- [La masse des amas déduite des rayons X](https://media4.obspm.fr/public/ressources_lu/pages_structures/massesx.html) - Observatoire de Paris
- [LES AMAS DE GALAXIES EN LUMIERE VISIBLE ET EN RAYONS X](http://www.astrosurf.com/thizy/rochelle2009/doc/2009oct27E%20Amas%20de%20Galaxies%20(Florence%20Durret).pdf) - Florence Durret (2009)
- [Studies of dark energy with x-ray observatories](https://www.pnas.org/doi/10.1073/pnas.0914905107) - Vikhlinin (2010)
- [Dark energy two decades after: Observables, probes, consistency tests](https://arxiv.org/abs/1709.01091) - Huterer et Chafer (2017)
- [Découverte de l'accélération de l'expansion de l'Univers](https://cosmology.education/decouverte-acceleration-expansion/distance-luminosite/#!) - Lucas Gautheron
- [Cosmology with cosmic shear observations: a review]( https://iopscience.iop.org/article/10.1088/0034-4885/78/8/086901) – Kilbinger (2015)
[Cisaillement cosmique](https://www.college-de-france.fr/agenda/seminaire/amas-de-galaxies-et-grandes-structures-de-univers/cisaillement-cosmique) – Collège de France
- [How Does DARK ENERGY Change the CMB Temperature? | The Integrated Sachs-Wolfe Effect](https://youtu.be/F3XOz1dR9YA) - Chris Pattison
- [Faster walk on the dark side](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/2006/faster.html) - NASA
- [L’effet Sachs-Wolfe intégré ou la traversée des grandes structures](http://public.planck.fr/outils/astrophysique/effet-sachs-wolfe-integre-isw) - Planck HFI
- [RayGal : une nouvelle simulation du cosmos pour percer les secrets de l'énergie noire](https://www.futura-sciences.com/sciences/actualites/cosmologie-raygal-nouvelle-simulation-cosmos-percer-secrets-energie-noire-94917/) - Futura Sciences (2022)
- [Comment Estimer la masse des amas de galaxies?](http://physique.unice.fr/sem6/2017-2018/PagesWeb/PT/Galaxie/les_amas/comment.html) - Observatoire de la côte d'Azur
- [Gravitational Lensing in the Canary Islands](https://astrobites.org/2012/11/25/gravitational-lensing-in-the-canary-islands/) - Astrobites
- [Cosmological constant](http://www.scholarpedia.org/article/Cosmological_constant) - Scholarpedia
- [Ask Ethan: Is Einstein’s Cosmological Constant The Same As Dark Energy?](https://www.forbes.com/sites/startswithabang/2020/12/25/ask-ethan-is-einsteins-cosmological-constant-the-same-as-dark-energy/?sh=33a76ee84636) - Ethan Siegel
- [La plus grosse erreur de toute l’histoire de la physique](https://scienceetonnante.com/2012/05/14/la-plus-grosse-erreur-de-toute-lhistoire-de-la-physique/) - Science Etonnante

---

-	En 1917, alors que les observations astronomiques sont principalement limitées aux étoiles de notre Galaxie, Albert Einstein tente d’appliquer sa toute jeune théorie de la relativité générale à un modèle d’univers qu’il juge statique. Pour contrer les effets de la gravitation, il introduit un nouveau terme dans ses équations : la ***constante cosmologique*** (symbolisée par la lettre grecque Λ). 
-	En 1922, le mathématicien russe Alexandre Friedmann démontre que la solution d’univers statique d’Einstein est aussi stable qu’un stylo posé sur sa pointe, et propose de le remplacer par un modèle d’univers en expansion. Le modèle de Friedman sera corroboré par les observations de la fuite des galaxies par Edwin Hubble à la fin des années 20. Il est désormais à la base du modèle cosmologique standard.
> À partir des années 30, Einstein va rapidement renoncer à son modèle d’univers statique. Il confiera plus tard à son collègue George Gamow que l’introduction de la constante cosmologique dans ses équations était « la plus grosse erreur de sa vie » (biggest blunder).
- La constante cosmologique va gagner une seconde vie à la fin des années 90. À cette époque, la majorité des cosmologistes sont persuadés que l’expansion de l’univers est en train de ralentir. Mais en 1998, deux équipes indépendantes d’astronomes des collaborations High-Z supernova Team et Supernova Cosmology Project découvrent que des supernovae thermonucléaires lointaines (0<z<1) sont moins brillantes que ce qu’elles devraient être dans un univers en cours de décélération (ce à quoi on s'attendrait dans un univers constitué uniquement de matière et de rayonnement). 
- La manière la plus simple d’interpréter ces observations est de considérer que l’univers se trouve dans une période d’expansion accélérée depuis environ 6 milliards d’années, qui fait suite à une période de décélération (justifiée par des supernovae thermonucléaires à z>1 plus brillantes que prévues cette fois).
- Cette découverte a valu le prix Nobel de physique en 2011 à trois chercheurs associés aux deux collaborations : Saul Perlmutter, Brian Schmidt et Adam Riess.
- De nos jours, de nombreuses observations indépendantes vont dans le sens d’une période d’expansion cosmique accélérée dans l’histoire récente de l’univers.
- Selon les données du télescope spatial Planck publiées en 2013, l’énergie sombre représenterait 68,3% de la densité énergétique totale de l’univers.
- On ne connaît pas la nature du phénomène qui cause cette expansion accélérée. 

## Comment modéliser la cause de l’expansion accélérée

- Une manière d’interpréter les observations dans le cadre du modèle cosmologique standard est d’invoquer l’existence d’une mystérieuse ***énergie sombre*** qui remplirait uniformément l’univers et qui serait dotée d’une « pression négative » qui la ferait se comporter comme une force répulsive.
> On modélise l’énergie sombre comme un fluide parfait caractérisé par son équation d’état, qui relie sa pression à sa densité d’énergie : P = wρ. (w est un nombre sans dimension)
- La modélisation la plus simple de cette hypothétique énergie sombre fait intervenir la fameuse constante cosmologique, qui pourrait être assimilée de façon élégante à une sorte d’énergie du vide dont la densité reste constante à mesure que l’univers s’étend.
> Si l’énergie sombre est une constante cosmologique, w = -1.
- Dans le langage de la théorie quantique des champs, il est possible d’assimiler cette énergie noire à l’***énergie du vide*** (vacuum energy), qui émerge à cause du ***principe d’incertitude d’Heisenberg***.
> La physique quantique ne permet pas l’existence d’états dans le vide où l’énergie est strictement nulle. L’énergie du vide est l’énergie de l’état fondamental des champs quantiques dans un espace vide de matière. Cet état est constamment fluctuant, et permet l’émergence et l’annihilation permanente de paires de particules virtuelles dans le vide quantique.
- Le problème, c’est que lorsqu’on mesure la densité de l’énergie sombre aux échelles cosmologiques, et qu’on calcule la densité de l’énergie du vide à l’aide des équations de la théorie quantique des champs, les deux valeurs sont incroyablement différentes : la densité du vide quantique excède celle de l’énergie sombre d’un facteur 10 puissance 120 ! Ce problème n’est pas encore résolu à ce jour.
> Certains appellent ce résultat « la plus grosse erreur de toute l’histoire de la physique ».
- La constante cosmologique n’est qu’une des modélisations possibles de l’énergie sombre. De nombreuses théories de gravitation quantique produisent ainsi des candidats à l’énergie sombre qui possèdent d’autres propriétés (par exemple différentes valeurs de w).
- Une autre forme hypothétique que pourrait prendre l’énergie sombre est la ***quintessence***, un champ scalaire dont la contribution varierait dans le temps, contrairement à une constante cosmologique (ie, son équation d’état évolue au fil du temps).
> Une forme particulière de quintessence est ***l’énergie fantôme***, proposé par le physicien Robert Caldwell et dont le paramètre w < -1. 
- Toutes ces propositions d’énergie sombre partent du principe que la relativité générale est la bonne théorie pour décrire l’évolution à grande échelle du cosmos. Mais il existe aussi l’option que la relativité générale doit être révisée aux échelles les plus vastes. C’est l’approche dite de ***gravité modifiée*** (ou de ***gravité sombre*** - dark gravity).
- Les modèles de gravité modifiée produisent des scénarios d’évolution des grandes structures cosmiques différents de ceux du modèle standard de la cosmologie.
- Malheureusement, il n’existe à ce jour aucun modèle de gravité modifiée qui permette d’expliquer l’accélération de l’expansion cosmique observée.

*Evolution de l'expansion cosmique dans le scénario où l'énergie sombre est une constante cosmologique (Big Freeze). [Source](https://phys.libretexts.org/Bookshelves/Astronomy__Cosmology/Big_Ideas_in_Cosmology_(Coble_et_al.)/17%3A_Dark_Energy_and_the_Fate_of_the_Universe/17.04%3A_Cosmic_Concordance_and_Cosmological_Parameters)*

![bigfreeze](https://files.mtstatic.com/site_4539/22172/0?Expires=1670329144&Signature=fQ9JJW4nsEkrF4Fve7nb~Zs5mEqLneKvkcEUfrGzn3rCgIh3RSqPvHS2cj3dqtq7VNe2bi3YByGcKNSZIVXi77diOvXv~8QhRJ96dyuC0CvO3njbvP-fPokd5ilLmylFHIr~gTd1vUYRh0L8ESPpeZwo4T3ZBTxKWnJAKipViEw_&Key-Pair-Id=APKAJ5Y6AV4GI7A555NA)

## Comment sonder les propriétés de l'énergie noire à l'aide de nos observations

- Il y a essentiellement deux classes de méthodes pour étudier les signatures observables de l'énergie sombre : les méthodes dites "géométriques", qui se concentrent sur le taux d'expansion de l'univers (à l'aide de règles ou de chandelles standards), et les méthodes se focalisant sur la croissance des grandes structures cosmiques.

### Les anisotropies du CMB

-	Le CMB a été émis à une époque où la contribution de l’énergie sombre dans l’évolution cosmique était négligeable.
-	Par contre, l’analyse statistique des anisotropies de température du CMB (à travers son spectre de puissance) permet de contraindre les paramètres du modèle standard de la cosmologie avec une précision inférieure au pourcent. En effet, l’énergie sombre affecte la distance (comobile) qui nous sépare de l’époque de la recombinaison, et donc les échelles angulaires auxquelles les fluctuations de températures du CMB sont observées. Ces échelles se traduisent dans la position horizontale des pics dans le spectre de puissance.
- L'énergie sombre laisse une autre trace, plus subtile, dans les anisotropies de température du CMB, générée cette fois-ci au cours du voyage des photons du CMB depuis leur source jusqu'à nous.
> * Les grandes structures cosmiques comme les amas de galaxies courbent l'espace-temps autour d'eux et génèrent des puits de potentiel gravitationnel. Lorsqu'un photon du CMB traverse un amas de galaxies, il tombe dans le puits de potentiel. Et à la manière d'une bille qui est accélérée lorsqu'elle roule dans une cuvette, le photon gagne de l'énergie. Autrement dit, il bleuit (sa longueur d'onde est décalée vers le bleu). Et parce qu'émerger du puits gravitationnel est coûteux en énergie, le photon perd de l'énergie lorsqu'il quitte l'amas de galaxies. Autrement dit, il rougit. En règle générale, comme l’énergie se conserve, les deux effets se compensent, et le photon à la sortie de l’amas possède la même longueur d’onde qu’avant son arrivée. Ce phénomène a été théorisé en 1967 par les astronomes Rainer Kurt Sachs et Arthur Michael Wolfe et porte aujourd'hui le nom d'***effet Sachs-Wolfe***. Dans un univers dominé par l'énergie sombre (z < 1), la situation est légèrement différente. Lorsque le photon pénètre dans l’amas de galaxies, il rougit comme avant. Cependant, la structure est si vaste (il faut des centaines de millions d’années à un photon pour la traverser) qu’elle subit les effets de l’accélération de l’expansion cosmique. Ces effets se traduisent par une inflation de l’amas de galaxies, et donc par un « aplatissement » du puits de potentiel gravitationnel. Autrement dit, la forme de la cuvette évolue dans le temps. Elle est moins profonde qu’avant. Dans ce cas de figure, le photon bleui qui émerge de l’amas est moins rougi que dans la situation précédente. Lorsqu’il arrive dans nos détecteurs, le photon sera donc légèrement décalé vers le bleu par rapport à sa longueur d’onde de départ. On appelle ce phénomène l’***effet Sachs-Wolfe intégré*** (ou effet ISW, pour Integrated Sachs-Wolfe).
> * À noter que la traversée d’un vide cosmique produit aussi un effet Sachs-Wolfe intégré, mais qu’il fonctionne dans l’autre sens. Le photon rougit en arrivant dans le vide, et bleuit à sa sortie.
- Lorsqu’on cartographie la distribution de la matière dans une région du ciel et qu’on corrèle cette carte avec la carte des anisotropies du CMB dans la même direction du ciel (on fait ce qu’on appelle une ***corrélation croisée***), on peut donc en tirer des indices sur la nature et les propriétés de l’énergie sombre (à travers son effet sur la croissance des grands structures).

*Illustration de l'effet Sachs-Wolfe intégré. Crédit : NASA's Cosmic Times [Source](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/2006/faster.html)*

![ISW](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/2006/images/sachs_wolfe_illustration.gif)

### Les supernovae thermonucléaires

- Les ***supernovae thermonucléaires*** (aussi appelées ***supernovae de type Ia***, ou SNIa) sont des phénomènes résultant de l’explosion de naines blanches situées dans des couples stellaires.
> Dans de tels systèmes, la naine blanche (constituée majoritairement de carbone et d’oxygène) accumule de la matière provenant de sa partenaire à sa surface. Cette accumulation a lieu jusqu’à ce que la naine blanche atteigne une masse critique de 1,4 masse solaire (la ***masse de Chandrasekhar***). À ce moment-là, on pense que des réactions de fusion du carbone se déclenchent dans son cœur et s’emballent en quelques secondes à peine, provoquant une explosion thermonucléaire qui oblitère complètement cette dernière.
- La durée de ce type d’explosion est de l’ordre d’un mois, durée pendant laquelle la luminosité de la SNIa augmente puis diminue jusqu’à disparaitre dans la nuit. À son pic de luminosité, une SNIa peut être aussi brillante qu’une galaxie tout entière.
- Les SNIa sont très utiles pour les cosmologistes puisqu’elles constituent des ***chandelles standards***, c’est-à-dire des objets (idéaux) dont la luminosité intrinsèque est toujours la même, et qui servent donc à estimer les distances dans l’univers.
> Si on connaît la luminosité intrinsèque d’un objet, il suffit de mesurer sa luminosité apparente pour en déduire la distance (de luminosité) grâce à la ***loi en carré inverse***. 
- Une fois que l’on a mesuré la distance de luminosité d’une SNIa, on peut obtenir son redshift pour un modèle cosmologique donné à l’aide de la ***relation distance-redshift***. Observer la luminosité apparente d’un échantillon de SNIa permet donc d’obtenir des informations sur l’histoire de l’expansion de l’univers.
- En pratique, les SNIa ne sont pas des chandelles standard parfaites : à leur pic, la luminosité d’un échantillon de ces explosions présente des différences de l’ordre de 10-15% (après correction de la durée de l’explosion et de sa couleur). Ces différences limitent la précision à laquelle les distances cosmiques sont inférées. 
- On recense aujourd’hui des milliers de SNIa, dont près d’un millier sont utilisées à des fins cosmologiques.
> Ce sont des événements relativement rares. En moyenne, dans une galaxie typique, il se produit une SNIa tous les 100 ans environs.

*Courbe de luminosité d'un échantillon de SN Ia. Crédit : Huterer et Schafer (2018). [Source](https://astrobites.org/2019/02/18/type-ia-supernovae-could-use-some-more-color/)*

![SNIa](https://astrobites.org/wp-content/uploads/2020/01/Screen-Shot-2020-01-03-at-3.30.49-PM-1080x469.png)

### Les oscillations acoustiques des baryons

- La méthode la plus précise actuellement utilisée pour mesurer l'histoire de l'expansion de l'univers fait appel à d'étranges motifs imprimés dans le ciel que l'on peut détecter quand on observe la distribution des galaxies dans l'univers avec une grande précision. Quand on les projette sur la sphère céleste, ces motifs ressemblent à des petits cercles entourés d'immenses anneaux, similaires à ceux créés par un caillou jeté dans un étang. Ces anneaux sont difficiles à détecter et se chevauchent, mais le plus surprenant c'est qu'ils ont tous la même taille caractéristique (que les astronomes appellent ***échelle acoustique***).
- Ces motifs sont dus à un phénomène physique assez insolite : les  ***oscillations acoustiques des baryons*** (ou BAO, pour Baryonic Acoustic Oscillations)
- La signature des BAO dans la répartition des galaxies a été mise en évidence pour la première fois en 2005 par deux équipes indépendantes d'astronomes associés aux relevés SDSS (Sloan Digital Sky Survey, au Nouveau Mexique) et 2dFGRS (2dF Galaxy Redshift Survey, en Australie).
> Ils ont montré que les anneaux ont une taille caractéristique de 500 millions d'années-lumière (150 Mpc) dans l'univers actuel. Autrement dit, si on observe une galaxie quelque part dans l’univers, on a (un peu) plus de chance de trouver une autre galaxie située à 500 millions d’années-lumière d’elle qu’une galaxie à 400 ou à 600 millions d’années-lumière. 
- Pour comprendre l'origine de ces motifs, il faut revenir aux conditions de l'univers primordial.
> Pendant les premières centaines de milliers d'années qui ont suivi le Big Bang, l'univers était un plasma chaud composé de photons, de protons, d'électrons et de matière noire qui interagissaient sans cesse. Ce plasma avait beau être extrêmement homogène, de petites surdensités de matière noire apparaissaient quand même en permanence. Attirée par la gravité de ces petits amas de matière noire, la matière ordinaire (dite "baryonique") tentait de s'accumuler, mais le rayonnement ambiant avait tendance à s'opposer à cette accumulation (à noter que la matière noire n'interagit pas avec la lumière, donc elle peut former des amas sans problème). Ces deux processus antagonistes ont généré des ondes de pressions qui se déplacent à la vitesse du son (170 000 km/s) dans ce plasma ultra-dense. Ces ondes de pression sont littéralement des ondes sonores de différentes intensités qui se déplacent dans toutes les directions. Dans cette cacophonie, certaines longueurs d'onde sont plus amplifiées que d'autres. La "note" la plus amplifiée possède une longueur d'onde d'un million d'années-lumière environ, ce qui représente une fréquence 48 octaves plus basses que la note la plus grave sur un piano ! Mais lorsque le cosmos atteint l’âge de 380 000 ans, la température du rayonnement primordial passe sous la barre des 3000 K (2700°C environ), et les électrons peuvent enfin se lier aux protons pour former les premiers atomes. Le plasma primordial est devenu un gaz d’atomes électriquement neutres. La lumière cesse alors d’interagir avec la matière et peut se balader dans l’espace à l’infini. Le cosmos, initialement opaque, devient alors transparent. C’est ***l’époque de la recombinaison***. À ce moment, la cacophonie s’arrête. Les ondes sonores sont soudainement figées à une certaine distance caractéristique des surdensités de matière noire. C’est comme si on avait soudainement gelé la surface de l’étang. Ces surdensités sont les graines à partir desquelles se formeront les premières galaxies et autres grandes structures de l’univers. Les empreintes des ondes sonores en forme de coquilles sphériques, quant à elles, vont se mettre à enfler au fil de l’expansion cosmique, à la matière d’un cercle dessiné à la surface d’un ballon de baudruche que l’on gonfle. 13,8 milliards d’années plus tard, on pourra les observer sous la forme d’un léger surnombre de galaxies au niveau  de ces coquilles, et d’une très légère tendance de ces galaxies à être alignées le long de leur surface.
- Le phénomène des BAO lie de manière quantitative les anisotropies observées dans le CMB et les motifs d'anneaux observés dans la distribution spatiale des galaxies
> En effet, les BAO sont responsables des pics observés dans le ***spectre de puissance*** (à définir) du CMB. Ces derniers correspondent aux longueurs d'onde sonores les plus amplifiées dans le plasma primordial.
- En mesurant la taille de ces anneaux à différentes époques de l’univers, à l’aide de relevés astronomiques très fins, et en liant ces anneaux aux conditions initiales fournies par les anisotropies du CMB, on peut donc reconstruire l’histoire de l’expansion cosmique avec une grande précision.
- L'empreinte des BAO sert de ***règle standard*** aux astronomes, c’est-à-dire que ces anneaux sont des objets dont on connaît la taille réelle. Quand on mesure leur diamètre apparent dans le ciel, comme on connaît leur taille on peut donc déterminer leur distance.
- Le grand relevé astronomique BOSS (Baryon Oscillation Spectroscopic Survey), qui fait partie du 3e relevé SDSS, et dont les données ont été acquises entre 2009 et 2014, constitue à ce jour la mesure la plus précise des anneaux produits par les BAO. À travers l’étude spectroscopique de plus d’un million de galaxies, les astronomes ont pu mesurer ces « règles standard » avec une précision d’1% sur une période qui couvre les 6 derniers milliards d’années de l’évolution cosmique (il y a une vingtaine d’années à peine, cette précision n’était que de 50% !).
- Un instrument fondamental pour détecter la signature des BAO dans les années à venir est DESI (Dark Energy Spectroscopic Instrument, "instrument spectroscopique pour l'énergie noire"), installé sur le télescope Mayall de 4 mètres de l'Observatoire National de Kitt Peak (Arizona) et capable de mesurer 5000 spectres de galaxies simultanément
> En 2021, une collaboration internationale de 50 institutions dirigée par le Berkeley Lab a débuté un programme d'observation de 5 ans avec DESI qui devrait l'amener à cartographier la position de plus de 30 millions de galaxies dans le ciel de l'hémisphère Nord, dont près de 2,4 millions de quasars. Couvrant les 11 derniers milliards d’années de l’histoire de l’univers, il pourra atteindre des profondeurs que BOSS ne pouvait pas atteindre, et permettra l'étude la plus fine de l'histoire de l'expansion cosmique jamais produite.      	      	      

*(Illustration des BAO. Crédit: Gabriela Secara, Institut Périmètre [Source](https://insidetheperimeter.ca/fr/desi-launches-five-year-quest-to-understand-the-universe/))*

![BAO](https://insidetheperimeter.ca/wp-content/uploads/2021/05/test-DESI_Baryon_Acoustic_Oscillation.png)

### L'effet de lentille gravitationnelle faible

-	Lorsque des rayons de lumière provenant de galaxies très lointaines traversent l’espace-temps déformé aux abords d’une grosse concentration de matière (dans ce contexte, un amas de galaxies) située entre les sources et l’observateur, on observe des images distordues des galaxies lointaines, comme si ces dernières étaient vues à travers la lentille d’une loupe. Ces distorsions sont dues à l’***effet de lentille gravitationnelle***, un phénomène prédit par la théorie de la relativité générale d’Albert Einstein.
>	La mesure de la déflexion de rayons lumineux des étoiles observées à proximité du Soleil lors d’une éclipse en 1919 constitue l’une des premières prédictions réussies de la relativité générale.
-	Dans certaines situations, l’effet de lentille gravitationnelle peut produire de multiples images de la source en arrière-plan. On parle alors de ***lentillage fort***. Celui-ci requiert un alignement particulier entre la source, la concentration de matière et l’observateur, ainsi que des distances relatives particulières entre les 3 parties. 
>	Les lentilles gravitationnelles fortes peuvent générer d’étranges mirages gravitationnels, tels que les croix ou les anneaux d’Einstein.
-	Mais dans le contexte de la traque de l’énergie sombre, l’effet qui nous intéresse particulièrement est le ***lentillage faible***, beaucoup plus fréquent, mais plus difficile à observer. Ce dernier crée un aplatissement ainsi qu’un alignement systématique des images des galaxies en arrière-plan qui forment un motif particulier.
-	On appelle ***cisaillement cosmique*** (cosmic shear) le signal statistique causé par le lentillage faible des amas de galaxies. Pour l’estimer, on mesure la forme d’un grand échantillon de galaxies lointaines dans une région du ciel donné, et on calcule la déformation moyenne de ces galaxies produite par l’effet de lentilles gravitationnelles.
>	Le cisaillement cosmique a été mesuré pour la première fois en 2000 par une équipe internationale d’astronomes incluant des chercheurs de l’IAP, grâce à l’étude des données du Canada France Hawaii Telescope (CFHT).
-	L’analyse statistique des déformations causées par l’effet de lentillage faible permet de reconstruire la distribution de matière noire dans les amas de galaxies et de suivre l’évolution de ces structures dans le temps (en observant le lentillage faible de ces galaxies lointaines à différents redshifts), ce qui permet de sonder l’influence de l’énergie sombre sur le taux de croissance des structures cosmiques.
>	* On peut comparer la cartographie de la distribution de la matière noire par cette méthode à la reconstruction des lignes de champ magnétique invisible avec de la limaille de fer
> * Lorsque l'on cartographie le cisaillement cosmique, on peut recenser les fluctuations de densité de différentes tailles qui se jouent à grande échelle dans l'univers et en déduire leur spectre de puissance. Et l'étude de ce spectre de puissance nous informe sur la nature et les propriétés de l'énergie sombre. C'est une méthode analogue à celle de l'analyse du spectre de puissance du CMB, mais qui nous informe cette fois du devenir des fluctuations de densité initiales.
-	De nombreux relevés astronomiques font de la mesure du cisaillement cosmique une priorité dans les années à venir. C’est notamment le cas du Dark Energy Survey (DES), du Hyper Suprime-Cam Subaru Strategic Program (HSC-SSP) ou encore du relevé que réalisera l'observatoire Vera Rubin (anciennement appelé LSST), qui a pour mission de mesurer plusieurs milliards (!) de formes de galaxies dans le ciel de l'hémisphère Sud. 

*Illustration de l'effet de lentilles gravitationnelles faible. Crédit : Michael Sachs. [Source](https://en.wikipedia.org/wiki/Weak_gravitational_lensing#/media/File:Gravitational-lensing-3d.png)*

![weak-lensing](https://upload.wikimedia.org/wikipedia/commons/b/b9/Gravitational-lensing-3d.png)

### Les amas de galaxies

- Les amas de galaxies sont les plus grandes structures liées par gravité dans l'Univers.
> Ils ont des tailles de l'ordre du million d'années-lumière (Mpc) et des masses de l'ordre du million de milliards de masses solaires (10^15 Msol)
- Les galaxies ne constituent que quelques pourcents de la masse totale d'un amas. Celles-ci baignent en effet dans un gaz très chaud (comprimé à 10-100 millions de degrés sous l'action de la gravité) et très peu dense, et ce gaz représente 80-90% de la masse baryonique d'un amas. De plus, la masse totale de l'amas est largement dominée par la matière noire qui la compose (combien ?)
- Quand on observe les amas de galaxies en rayons X, ceux-ci apparaissent comme des sources diffuses et étendues. Ce qu'on voit dans ces longueurs d'onde, c'est la contribution du gaz intergalactique surchauffé. 
> On ne peut pas observer les rayons X depuis des observatoires au sol, puisque ces longueurs d'onde sont absorbées par l'atmosphère terrestre. On doit donc avoir recours à des observatoires spatiaux comme Chandra ou XMM-Newton pour ce genre d'études.
- Pour les amas dits "relaxés" (ie en équilibre dynamique), la distribution spatiale de ce gaz est très fortement corrélée avec la distribution de la matière noire qui compose en grande partie
> Les amas relaxés constituent une minorité dans la population des amas de galaxies. 
- La formation des amas de galaxies dépend à la fois de la composition de l'univers (et donc des propriétés de la matière noire : Ω_m), de l'histoire de la formation des structures cosmiques (encapsulé dans σ8, l'amplitude des fluctuations de matière) et de l'histoire de l'expansion de l'univers (et donc des propriétés de l'énergie sombre : Ω_Λ, ω)
> En recensant les gros amas de galaxies "relaxés" dans l'univers, on peut comparer les nombres d'amas observés en fonction de leur distance (redshift) et les abondances prédites par différents scénarios d'évolution et de composition cosmique (issues de simulations cosmologiques), et ainsi inférer les propriétés de l'énergie sombre qui collent le mieux aux observations (Ω_Λ, ω) 
- À partir de l'observation d'un amas en rayons X, on peut calculer sa masse baryonique (car la luminosité X est proportionnelle au carré de la densité de gaz qui émet ces rayons X) et sa température. Et à partir d'un profil de densité théorique, si l'on fait l'hypothèse que le gaz est en équilibre hydrostatique dans le puits de potentiel de l'amas (ie l'amas ne présente pas de sous-structure et n'est pas en train de fusionner avec d'autres amas), alors on peut calculer la masse totale de l'amas en fonction du rayon, puis en intégrant sur le profil la masse totale de l'amas. On peut alors obtenir la ***fraction de gaz*** des amas (notée f_gas(z)), définie comme le rapport de la masse de gaz (mesurée via l'émission de rayons X) sur la masse totale de l'amas (galaxies + gaz intergalactique + matière noire)
> * Une autre manière d'obtenir la masse totale d'un amas est d'utiliser l'effet de lentilles gravitationnelle.
- Au milieu des années 90, les astronomes se sont rendus compte qu'ils pouvaientaussi utiliser les observations en rayons X des amas comme indicateurs de distance de ces amas, ce qui leur ont permis de tester les prédictions du modèle ΛCDM et en particulier de contraindre les propriétés de l'énergie sombre
> * Il faut pour cela supposer que les amas de galaxies sont des objets si vastes et si massifs que leur composition matérielle, notamment le rapport entre matière baryonique et matière noire est représentative de la composition matérielle de l'univers tout entier.
> * La fraction de gaz est corrélée avec la distance de l'amas, et cette distance dépend elle-même de la quantité de matière noire et d'énergie sombre dans l'univers. Comme la fraction de gaz doit être à peu près la même sur tous les amas de galaxies, en mesurant la fraction de gaz dans plein d'amas on doit pouvoir trouver les paramètres cosmologiques qui donnent les distances d'amas "correctes".
- Les données actuelles issues de l'observation des amas de galaxies fournissent une confirmation indépendante que l'expansion de l'univers est bien accélérée, et elles montrent que les propriétés de l'énergie sombre sont très proches d'une constante cosmologique
- Le télescope spatial Euclid de l'ESA, qui sera lancé en 2023, a pour mission de produire dans les années à venir l'un des catalogues d'amas de galaxies les plus fournis et les plus profonds de tous les temps, ce qui permettra aux astronomes de mieux contraindre les propriétés de l'énergie sombre.  

*Emission de rayons X du gaz chaud de 4 amas de galaxies, photographiée par le télescope spatial Chandra. Crédit : X-ray: NASA/CXC/Univ. of Alabama/A. Morandi et al; Optical: SDSS, NASA/STScI. [Source](https://www.nasa.gov/mission_pages/chandra/probing-dark-energy-with-galaxy-clusters.html)*

![xraychandra](https://www.nasa.gov/sites/default/files/styles/full_width_feature/public/thumbnails/image/clusters.jpg)

### La détermination directe du taux d'expansion cosmique

Direct Determination of H0: The value of H0 sets the current value of the critical density
ρc = 3H2
0 /8πGN, and combination with CMB measurements provides a long lever arm
for constraining the evolution of dark energy. The challenge in direct H0 measurements is
establishing distances to galaxies that are far enough away that their peculiar velocities
are small compared to the expansion velocity v = H0d. This can be done by building
a ladder of distance indicators tied to stellar parallax on its lowest rung, or by using
gravitational lens time delays or geometrical measurements of maser data to circumvent
this ladder

Direct measurements of the Hubble constant offer use-
ful complementary information that helps break degen-
eracy between dark energy and other cosmological pa-
rameters. This is because precise CMB measurements
effectively fix high-redshift parameters including the
physical matter density Ωmh2; independent measure-
ments of H0 (i.e. h) therefore help determine Ωm which
is degenerate with the dark energy equation of state.
Current & 3σ tension between the most precise direct
measurements of H0 from the Cepheid distance lad-
der [355, 356] and the indirect ΛCDM determination
from the CMB [74] is partially, but not fully, relieved
by allowing phantom dark energy (w < −1) or extra
relativistic degrees of freedom

A still more ambitious period begins late in this decade and continues through the
2020s, with experiments that include the Dark Energy Spectroscopic Instrument (DESI),
the Subaru Prime Focus Spectrograph (PFS), the Large Synoptic Survey Telescope
(LSST), and the space missions Euclid and WFIRST (Wide Field Infrared Survey
Telescope). DESI and PFS both aim for major improvements in the precision of BAO,
RSD, and other measurements of galaxy clustering in the redshift range 0.8 < z < 2,
where large comoving volume allows much smaller cosmic variance errors than low
redshift surveys like BOSS. LSST will be the ultimate ground-based optical weak lensing
experiment, measuring several billion galaxy shapes over 20,000 deg2 of the southern
hemisphere sky, and it will detect and monitor many thousands of SNe per year. Euclid
and WFIRST also have weak lensing as a primary science goal, taking advantage of the
high angular resolution and extremely stable image quality achievable from space. Both
missions plan large spectroscopic galaxy surveys, which will provide better sampling at
high redshifts than DESI or PFS because of the lower infrared sky background above
the atmosphere. WFIRST is also designed to carry out what should be the ultimate
supernova cosmology experiment, with deep, high resolution, near-IR observations and
the stable calibration achievable with a space platform.

Performance forecasts necessarily become more uncertain the further ahead we
look, but collectively these experiments are likely to achieve 1–2 order of magnitude
improvements over the precision of current expansion and growth measurements, while
simultaneously extending their redshift range, improving control of systematics, and
enabling much tighter cross-checks of results from entirely independent methods. The
critical clue to the origin of cosmic acceleration could also come from a surprising
direction, such as laboratory or solar system tests that challenge GR, time variation
of fundamental “constants,” or anomalous behavior of gravity in some astronomical
environments. Experimental advances along these multiple axes could confirm today’s
relatively simple, but frustratingly incomplete, “standard model” of cosmology, or they
could force yet another radical revision in our understanding of energy, or gravity, or the
spacetime structure of the Universe.


## Que sait-on de l'énergie sombre aujourd'hui ?

- L'énergie sombre ne domine le contenu énergétique de l'univers que depuis relativement récemment (z ~ 0.5)
- Elle affecte les distances dans l'univers, ainsi que la croissance et le nombre des grandes structures cosmiques 
- Les différentes contraintes observationnelles actuelles tendent vers w ~ -1 (à 5% près).
- Les prochains relevés astronomiques vont permettre d'étudier la variation temporelle (hypothétique) de la contribution de l'énergie sombre


# L'inflation cosmique

# Le problème de la hiérarchie

*Sources*

- [The Hierarchy Problem: why the Higgs has a snowball’s chance in hell](https://www.quantumdiaries.org/2012/07/01/the-hierarchy-problem-why-the-higgs-has-a-snowballs-chance-in-hell/) - Quantum Diaries
- [The Hierarchy Problem](https://profmattstrassler.com/articles-and-posts/particle-physics-basics/the-hierarchy-problem/) - Matt Strassler 
- [The Higgs, The Hierarchy Problem, and the LHC](https://www2.physics.ox.ac.uk/sites/default/files/2014-11-24/higgs_lhc_jmr_nov14_pdf_93873.pdf) - John March-Russell
- [The Mystery of the Higgs Boson's Mass](https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5idXp6c3Byb3V0LmNvbS8xMTYyNjEzLnJzcw/episode/QnV6enNwcm91dC04OTE4NTUw) - Why this universe
- [Naturalness after the Higgs](https://cerncourier.com/a/naturalness-after-the-higgs/) - CERN
- [A Deepening Crisis Forces Physicists to Rethink Structure of Nature’s Laws](https://www.quantamagazine.org/crisis-in-particle-physics-forces-a-rethink-of-what-is-natural-20220301/) - Quanta Magazine
- [The Dawn of the Post-Naturalness Era](https://arxiv.org/abs/1710.07663) - Giudice (2017)
- [A Lecture on the Hierarchy Problem and Gravity](https://cds.cern.ch/record/2120792/files/CERN-2013-003-p145.pdf) - Dvali (2013) 
- [The Higgs & the Hierarchy Problem](https://www.youtube.com/watch?v=iywSF7BGhyU) - Anna Barth
- [Big Mysteries: The Higgs Mass](https://www.youtube.com/watch?v=IjCypYnBYwQ) - Fermilab
- [Naturally Speaking: The Naturalness Criterion and Physics at the LHC](https://arxiv.org/abs/0801.2562) - Giudice (2008)
- [Histoire de la cosmologie](https://cosmology.education/booklet/booklet.pdf) - Lucas Gautheron
- [Naturalness: A Snowmass White Paper](https://arxiv.org/abs/2205.05708) - Craig (2022)
- ["Particle Physics: The Higgs Boson and Beyond" by Andreas Hoecker (CERN)](https://youtu.be/XX4pL7pwl7w) - SLAC (2012)
- ["The Once and Future Higgs" by Prof. Nathaniel Craig (University of California, Santa Barbara)](https://youtu.be/GduJt1V2eNk) - TIFR (2021)

---

- Le modèle standard est constitué de 24 particules fondamentales.
- Le boson de Higgs joue un rôle fondamental dans le modèle standard
- Le boson de Higgs brise la symétrie électrofaible et donne sa masse aux bosons et aux fermions
- Les particules interagissent avec le champ de Higgs (qui remplit tout l'espace-temps), ce qui réduit leur vitesse. Et plus elles interagissent fortement, plus leur masse est grande (c'est proportionnel).
- Découvert en 2012 par le LHC, un accélérateur de particules de 27km de circonférence
- On peut sonder la composition de la matière de plus en plus profondément à mesure que l'on fait des collisions de plus en plus énergétiques
- Dans le modèle standard, le higgs est "le seul scalaire fondamental (tous les autres scalaires sont des états composites)"
- Un des problèmes les plus importants de la physique contemporaine
- coincidence spectaculaire
- Boson de Higgs prédit en 1954
- En une phrase : la masse observée du boson de Higgs est de 125 GeV (125 fois la masse du proton ?). Pourtant, des corrections quantiques issues des interactions avec d'autres particules du modèle standard prédisent une masse 17 ordres de grandeur plus grande (au niveau de la masse de Planck). C'est le problème de la hiérarchie électrofaible.
- The Higgs boson plays a key role in the Standard Model: it is related to the unification of the electromagnetic and weak forces, explains the origin of elementary particle masses
- Higgs—the last piece of the Standard Model
- Hierarchy problem. This is often ‘explained’ by saying that quantum corrections want to make the Higgs much heavier than we need it to be… say, 125-ish GeV. 
- The Higgs has a snowball’s chance in hell of having a mass in that ballpark.
- If you put a glass of water in a really hot place—you expect it to also become really hot, maybe even to off into steam.  It would be really surprising if we put an ice cube in a hot oven and 10 minutes later it had not melted. This is because the ambient thermal energy is expected to be transferred to the ice cube by the energetic air molecules bouncing off it. Sure, it is possible that the air molecules just happen to bounce in a way that doesn’t impart much thermal energy—but that would be ridiculously improbable, as we learn in thermodynamics.
- The Higgs is very similar: we expect its mass to be around 125 GeV (not too far from W and Z masses), but ambient quantum energy wants to make its mass much larger through interactions with virtual particles. While it is possible that the Higgs stays light without any additional help, it’s ridiculously improbable, as we learn from quantum physics.
- the Standard Model really, really wants the Higgs to be around the 100 GeV scale. This is because it needs something to “unitarize longitudinal vector boson scattering.” It needs to have some Higgs-like state accessible at low energies to explain why certain observed particle interactions are well behaved.
- The Hierarchy problem has been the main motivation for new physics at the TeV scale for over two decades. 
- it is possible that the Higgs mass is 125 GeV due to some miraculous almost-cancellation that set it to be in just the right ballpark to unitarize longitudinal vector boson scattering. But such miracles are rare in physics without any a priori explanation. The electron mass is an excellent example. There are some apparent (and somewhat controversial) counter-examples: the cosmological constant problem is a much more severe ‘fine-tuning’ problem which may be explained anthropically rather than through more fundamental principles.
-  What are the possible ways to solve the Hierarchy problem?
- There are two main directions that most people consider:
> * Supersymmetry. Recall in our electron analogy that the solution to the ‘electron mass hierarchy problem’ was that quantum mechanics doubled the number of particles: in addition to the electron, there was also a positron. The virtual electron–positron contributions solved the problem by smearing out the electric charge. Supersymmetry is an analogous idea where once again the set of particles is doubled, and in doing so the loop contributions of one particle to the Higgs are cancelled by the loop contributions of its super-partner. Supersymmetry has deep connections to an extension of space-time symmetry since it relates matter particles to force particles.
> * Compositeness/extra dimensions. The other solution is that maybe our description of physics breaks down much sooner than the Planck scale. In particular, maybe at the TeV scale the Higgs no longer behaves like a scalar particles, but rather as a bound state of two fermions. This is precisely what happens with the mesons: even though the pion is a scalar, there is no pion ‘hierarchy problem’ because as you probe smaller distances, you realize the pion is actually a bound state of two quarks and it starts behaving as such. One of the beautiful developments of theoretical physics in the 1990s and early 2000s was the realization that this is precisely what is being described by theories of extra dimensions through the so-called holographic principle.
- An important feature of nature that puzzles scientists like myself is known as the hierarchy, meaning the vast discrepancy between aspects of the weak nuclear force and gravity. There are several different ways to describe this hierarchy, each emphasizing a different feature of it. Here is one:
- The mass of the smallest possible black hole defines what is known as the Planck Mass. 
-     The masses of the W and Z particles, the force carriers of the weak nuclear force, are about 10,000,000,000,000,000 times smaller than the Planck Mass. Thus there is a huge hierarchy in the mass scales of weak nuclear forces and gravity.
- When faced with such a large number as 10,000,000,000,000,000, ten quadrillion, the question that physicists are naturally led to ask is: where did that number come from? It might have some sort of interesting explanation.
- But while trying to figure out a possible explanation, physicists in the 1970s realized there was actually a serious problem, even a paradox, behind this number. The issue, now called the hierarchy problem, has to do with the size of the non-zero Higgs field, which in turn determines the mass of the W and Z particles.
- The non-zero Higgs field has a size of about 250 GeV, and that gives us the W and Z particles with masses of about 100 GeV. But it turns out that quantum mechanics would lead us to expect that this size of a Higgs field is unstable, something like (warning: imperfect analogy ahead) a vase balanced precariously on the edge of a table. With the physics we know about so far, the tendency of quantum mechanics to jostle — those quantum fluctuations I’ve mentioned elsewhere — would seem to imply that there are two natural values for the Higgs field — in analogy to the two natural places for the vase, firmly placed on the table or smashed on the floor. Naively, the Higgs field should either be zero, or it should be as big as the Planck Energy, 10,000,000,000,000,000 times larger than it is observed to be. Why is it at a value that is non-zero and tiny, a value that seems, at least naively, so unnatural? This is the hierarchy problem.
- Many theoretical physicists have devoted significant fractions of their careers to trying to solve this problem. Some have argued that new particles and new forces are needed (and their theories go by names such as supersymmetry, technicolor , little Higgs, etc.) Some have argued that our understanding of gravity is mistaken and that there are new unknown dimensions (“extra dimensions”) of space that will become apparent to our experiments at the Large Hadron collider in the near future. Others have argued that there is nothing to explain, because of a selection effect: the universe is far larger and far more diverse than the part that we can see, and we live in an apparently unnatural part of the universe mainly because the rest of it is uninhabitable — much the way that although rocky planets are rare in the universe, we live on one because it’s the only place we could have evolved and survived. There may be other solutions to this problem that have not yet been invented.
- Many of these solutions — certainly all the ones with new particles and forces or with new dimensions — predict that new phenomena should be visible at the Large Hadron Collider. 
- By the way, you will often read the hierarchy problem stated as a problem with the Higgs particle mass.  This is incorrect.  The problem is with how big the non-zero Higgs field is.  (For experts — quantum mechanics corrects not the Higgs particle mass but the Higgs mass-squared parameter, changing the Higgs field potential energy and thus the field’s value, making it zero or immense.  That’s a disaster because the W and Z masses are known.  The Higgs mass is unknown, and therefore it could be very large — if the W and Z masses were very large too.  So it is the W and Z masses — and the size of the non-zero Higgs field — that are the problem, both logically and scientifically.)
- Either new particles are keeping the Higgs boson light, or the universe is oddly fine-tuned for our existence. 
- When Victor Weisskopf sat down in the early 1930s to compute the energy of a solitary electron, he had no way of knowing that he’d ultimately discover what is now known as the electroweak hierarchy problem. Revisiting a familiar puzzle from classical electrodynamics – that the energy stored in an electron’s own electric field diverges as the radius of the electron is taken to zero (equivalently, as the energy cutoff of the theory is taken to infinity) – in Dirac’s recently proposed theory of relativistic quantum mechanics, he made a remarkable discovery: the contribution from a new particle in Dirac’s theory, the positron, cancelled the divergence from the electron itself and left a quantum correction to the self-energy that was only logarithmically sensitive to the cutoff. 
-  the coupling between the Higgs boson and other particles of the Standard Model (SM) leads to yet another divergent self-energy, for which the logic of naturalness implied new physics at around the TeV scale. Thus the electroweak hierarchy problem was born – not as a new puzzle unique to the Higgs, but rather the latest application of Weisskopf’s wildly successful logic (albeit one for which the answer is not yet known). 
- History suggested two possibilities. As a scalar, the Higgs could only benefit from the sort of cancellation observed among fermions if there is a symmetry relating bosons and fermions, namely supersymmetry. Alternatively, it could be a light product of compositeness, just as the pions and kaons are light bound states of the strong interactions. These solutions to the hierarchy problem came to dominate expectations for physics beyond the SM, with a sharp target – the TeV scale – motivating successive generations of collider experiments. Indeed, when the physics case for the LHC was first developed in the mid-1980s, it was thought that new particles associated with supersymmetry or compositeness would be much easier to discover than the Higgs itself. But while the Higgs was discovered, no signs of supersymmetry or compositeness were to be found.
- In the meantime, other naturalness problems were brewing. The vacuum energy – Einstein’s infamous cosmological constant – suffers a divergence of its own, and even the finite contributions from the SM are many orders of magnitude larger than the observed value. Although natural expectations for the cosmological constant fail, an entirely different set of logic seems to succeed in its place. To observe a small cosmological constant requires observers, and observers can presumably arise only if gravitationally-bound structures are able to form. As Steven Weinberg and others observed in the 1980s, such anthropic reasoning leads to a prediction that is remarkably close to the value ultimately measured in 1998. To have predictive power, this requires a multitude of possible universes across which the cosmological constant varies; only the ones with sufficiently small values of the cosmological constant produce observers to bear witness.
- An analogous argument might apply to the electroweak hierarchy problem: the nuclear binding energy is no longer sufficient to stabilise the neutron within typical nuclei if the Higgs vacuum expectation value (VEV) is increased well above its observed value. If the Higgs VEV varies across a landscape of possible universes while its couplings to fermions are kept fixed, only universes with sufficiently small values of the Higgs VEV would lead to complex atoms and, presumably, observers. Although anthropic reasoning for the hierarchy problem requires stronger assumptions than for the cosmological-constant problem, its compatibility with null results at the LHC is enough to raise questions about the robustness of natural reasoning. 
- The success of the relaxion hypothesis in solving the hierarchy problem hinges on an array of other questions involving gravity. Whether the relaxion potential can remain sufficiently smooth over the vast trans-Planckian distances in field space required to set the value of the weak scale is an open question, one that is intimately connected to the fate of global symmetries in a theory of quantum gravity (itself the target of active study in what is known as the Swampland programme).
- the recognition that cosmology might play a role in solving the hierarchy problem has given rise to a plethora of new ideas. For instance, in Raffaele D’Agnolo and Daniele Teresi’s recent paradigm of “sliding naturalness”, the Higgs is coupled to a new scalar whose potential features two minima. In the true minimum, the cosmological constant is large and negative, and the universe would crunch away into oblivion if it ended up in this vacuum. In the second, local minimum, the cosmological constant is safely positive (and can be made compatible with the small observed value of the cosmological constant by Weinberg’s anthropic selection). The Higgs couples to this scalar in such a way that a large value of the Higgs VEV destabilises the “safe” minimum. During the inflationary epoch, only universes with suitably small values of the Higgs VEV can grow and expand, while those with large values of the Higgs VEV crunch away. A second scalar coupled analogously to the Higgs can explain why the VEV is small but non-zero. 
- Alternatively, in the paradigm of “Nnaturalness” proposed by Nima Arkani-Hamed and others, the multitude of SMs over which the Higgs mass varies occur in one universe, rather than many. The fact that the universe is predominantly composed of one copy of the SM with a small Higgs mass can be explained if inflation ends and reheats the universe through the decay of a single particle. If this particle is sufficiently light, it will preferentially reheat the copy of the SM with the smallest non-zero value of the Higgs VEV, even if it couples symmetrically to each copy. The sub-dominant energy density deposited in other copies of the SM leaves its mark in the form of dark radiation susceptible to detection by the Simons Observatory or upcoming CMB-S4 facility. 
- Finally, Gian Giudice, Matthew Mccullough and Tevong You have recently shown that inflation can help to understand the electroweak hierarchy problem by analogy with self-organised criticality. Just as adding individual grains of sand to a sandpile induces avalanches over diverse length scales – a hallmark of critical behaviour, obtained without tuning parameters – so too can inflation drive scalar fields close to critical points in their potential. This may help to understand why the observed Higgs mass lies so close to the boundary between the unbroken and broken phases of electroweak symmetry without fine tuning.
- Underlying Weisskopf’s natural reasoning is a long-standing assumption about relativistic theories of quantum mechanics: physics at short distances (the ultraviolet, or UV) is decoupled from physics at long distances (the infrared, or IR), making it challenging to apply a theory involving a large energy scale to a much smaller one without fine tuning. This suggests that loopholes may be found in theories that mix the UV and the IR, as is known to occur in quantum gravity. 
- While the connection between this type of UV/IR mixing and the mass of the Higgs remains tenuous, there are encouraging signs of progress.
- We have come a long way since Weisskopf first set out to understand the self-energy of the electron. The electroweak hierarchy problem is not the first of its kind, but rather the one that remains unresolved. The absence of supersymmetry or compositeness at the TeV scale beckons us to search for new solutions to the hierarchy problem, rather than turning our backs on it. In the decade since the discovery of the Higgs, this search has given rise to a plethora of novel approaches, building new bridges between particle physics, cosmology and gravity along the way. Despite the many differences among these new approaches, they share a common tendency to leave imprints on the Higgs boson. And so, as ever, we must look to experiment to show the way. 
-  The hierarchy problem, as the puzzle is called, asks why the Higgs boson is so lightweight — a hundred million billion times less massive than the highest energy scales that exist in nature. The Higgs mass seems unnaturally dialed down relative to these higher energies, as if huge numbers in the underlying equation that determines its value all miraculously cancel out.
-  The extra particles would have explained the tiny Higgs mass, restoring what physicists call “naturalness” to their equations. But after the LHC became the third and biggest collider to search in vain for them, it seemed that the very logic about what’s natural in nature might be wrong.
-  At first, the community despaired. “You could feel the pessimism,” said Isabel Garcia Garcia, a particle theorist at the Kavli Institute for Theoretical Physics at the University of California, Santa Barbara, who was a graduate student at the time. Not only had the $10 billion proton smasher failed to answer a 40-year-old question, but the very beliefs and strategies that had long guided particle physics could no longer be trusted. People wondered more loudly than before whether the universe is simply unnatural, the product of fine-tuned mathematical cancellations. Perhaps there’s a multiverse of universes, all with randomly dialed Higgs masses and other parameters, and we find ourselves here only because our universe’s peculiar properties foster the formation of atoms, stars and planets and therefore life. This “anthropic argument,” though possibly right, is frustratingly untestable.
-  Some of those who remained set to work scrutinizing decades-old assumptions. They started thinking anew about the striking features of nature that seem unnaturally fine-tuned — both the Higgs boson’s small mass, and a seemingly unrelated case, one that concerns the unnaturally low energy of space itself.
-  Their introspection is bearing fruit. Researchers are increasingly zeroing in on what they see as a weakness in the conventional reasoning about naturalness. It rests on a seemingly benign assumption, one that has been baked into scientific outlooks since ancient Greece: Big stuff consists of smaller, more fundamental stuff — an idea known as reductionism. “The reductionist paradigm … is hard-wired into the naturalness problems,” said Nima Arkani-Hamed, a theorist at the Institute for Advanced Study in Princeton, New Jersey.
- Now a growing number of particle physicists think naturalness problems and the null results at the Large Hadron Collider might be tied to reductionism’s breakdown. “Could it be that this changes the rules of the game?” Arkani-Hamed said. In a slew of recent papers, researchers have thrown reductionism to the wind. They’re exploring novel ways in which big and small distance scales might conspire, producing values of parameters that look unnaturally fine-tuned from a reductionist perspective.
- The Large Hadron Collider did make one critical discovery: In 2012, it finally struck upon the Higgs boson, the keystone of the 50-year-old set of equations known as the Standard Model of particle physics, which describes the 17 known elementary particles.
- The discovery of the Higgs confirmed a riveting story that’s written in the Standard Model equations. Moments after the Big Bang, an entity that permeates space called the Higgs field suddenly became infused with energy. This Higgs field crackles with Higgs bosons, particles that possess mass because of the field’s energy. As electrons, quarks and other particles move through space, they interact with Higgs bosons, and in this way they acquire mass as well.
- After the Standard Model was completed in 1975, its architects almost immediately noticed a problem.
- When the Higgs gives other particles mass, they give it right back; the particle masses shake out together. Physicists can write an equation for the Higgs boson’s mass that includes terms from each particle it interacts with. All the massive Standard Model particles contribute terms to the equation, but these aren’t the only contributions. The Higgs should also mathematically mingle with heavier particles, up to and including phenomena at the Planck scale, an energy level associated with the quantum nature of gravity, black holes and the Big Bang. Planck-scale phenomena should contribute terms to the Higgs mass that are huge — roughly a hundred million billion times larger than the actual Higgs mass. Naively, you would expect the Higgs boson to be as heavy as they are, thereby beefing up other elementary particles as well. Particles would be too heavy to form atoms, and the universe would be empty.
- For the Higgs to depend on enormous energies yet end up so light, you have to assume that some of the Planckian contributions to its mass are negative while others are positive, and that they’re all dialed to just the right amounts to exactly cancel out. Unless there’s some reason for this cancellation, it seems ludicrous — about as unlikely as air currents and table vibrations counteracting each other to keep a pencil balanced on its tip. This kind of fine-tuned cancellation physicists deem “unnatural.”
- Within a few years, physicists found a tidy solution: supersymmetry, a hypothesized doubling of nature’s elementary particles. Supersymmetry says that every boson (one of two types of particle) has a partner fermion (the other type), and vice versa. Bosons and fermions contribute positive and negative terms to the Higgs mass, respectively. So if these terms always come in pairs, they’ll always cancel.
- The search for supersymmetric partner particles began at the Large Electron-Positron Collider in the 1990s. Researchers assumed the particles were just a tad heavier than their Standard Model partners, requiring more raw energy to materialize, so they accelerated particles to nearly light speed, smashed them together, and looked for heavy apparitions among the debris.
- The fabric of space, even when devoid of matter, seems as if it should sizzle with energy — the net activity of all the quantum fields coursing through it. When particle physicists add up all the presumptive contributions to the energy of space, they find that, as with the Higgs mass, injections of energy coming from Planck-scale phenomena should blow it up. Albert Einstein showed that the energy of space, which he dubbed the cosmological constant, has a gravitationally repulsive effect; it causes space to expand faster and faster. If space were infused with a Planckian density of energy, the universe would have ripped itself apart moments after the Big Bang. But this hasn’t happened.
- Instead, cosmologists observe that space’s expansion is accelerating only slowly, indicating that the cosmological constant is small. Measurements in 1998 pegged its value as a million million million million million times lower than the Planck energy. Again, it seems all those enormous energy injections and extractions in the equation for the cosmological constant perfectly cancel out, leaving space eerily placid.
- In hindsight, the two naturalness problems seem more like symptoms of a deeper issue. “It’s useful to think about how these problems come about,” said Garcia Garcia in a Zoom call from Santa Barbara this winter. “The hierarchy problem and the cosmological constant problem are problems that arise in part because of the tools we’re using to try to answer questions — the way we’re trying to understand certain features of our universe.”
- 

# Le problème du lithium cosmique

*Sources*


- [Populating the periodic table: Nucleosynthesis of the elements](https://science.sciencemag.org/content/363/6426/474) - Johnson (2019)
- [The Primordial Lithium Problem](https://www.annualreviews.org/doi/10.1146/annurev-nucl-102010-130445) - Fields (2011)
- [The Cosmological Lithium Problem Revisited](https://arxiv.org/abs/1603.03864) - Bertulani et al (2016)
- [Etoile de population II](https://fr.wikipedia.org/wiki/%C3%89toile_de_population_II) - WIkipédia
- [Big-Bang Nucleosynthesis and the Baryon Density of the Universe](https://arxiv.org/pdf/astro-ph/9407006.pdf) - Copi et al (1995)
- [Big Bang Nucleosynthesis](https://pdg.lbl.gov/2019/reviews/rpp2019-rev-bbang-nucleosynthesis.pdf)  - Fields (2019)
- [Big Bang Nucleosynthesis (BBN)](https://www.astronomy.ohio-state.edu/weinberg.21/A5682/notes8.pdf) - Weinberg
- [The cosmological lithium problem](https://www.sciencedaily.com/releases/2018/10/181009102501.htm) - Science Daily (2018)

---

- La ***nucléosynthèse primordiale*** désigne le processus de formation des noyaux les plus légers dans les conditions ardentes de l'univers primordial, entre 1 s et 3 minutes (~180 s) environ après le Big Bang.
> À partir de 1 seconde après le Big Bang, la température du cosmos passe sous la barre des 10 milliards de degrés. Une séquence d'événements est alors initiée, qui mène à la synthèse de quelques éléments légers, comme ***le deutérium, l'hélium-3, l'hélium-4 et le lithium-7***.
- La théorie de la nucléosynthèse primordiale standard (Big Bang Nucleosynthesis) repose sur le modèle standard de la physique des particules ainsi que du modèle standard de la cosmologie (ΛCDM), qui modélise un univershomogène et isotrope en expansion selon les règles de la relativité générale contenant de la matière noire et de l'énergie sombre.
- Dans la théorie de la nucléosynthèse primordiale standard, les abondances des éléments légers sont encapsulées dans un unique paramètre, la ***densité de baryons cosmique***, noté η (les baryons dans ce contexte sont les protons et les neutrons), qui est normalisée par rapport à la densité de photons du fond diffus cosmologique (baryon-photon ratio). Ce rapport est de l'ordre de 10^-9, ce qui veut dire que pour chaque baryon de l'univers, il y a environ 1 milliard de photons du CMB.
> La densité de photon du CMB est de 413 photons/cm3.
- Le ***problème du lithium cosmique*** désigne l'énorme différence entre nos théories et nos observations concernant l'abondance de lithium 7 dans l'univers.
> On observe 3 à 4 fois moins de lithium-7 dans l'univers que ce qui est prédit par la théorie. 
- Par contre, les abondances d'hélium et de deutérium sont reproduites avec beaucoup de succès
> Ces abondances constituent même l'un des quatre piliers observationnels du modèle du Big Bang chaud.
- Le problème du lithium est apparu lorsque les astronomes ont commencé à étudier les propriétés du rayonnement de fond diffus cosmologique à l'aide du satellite WMAP, et le désaccord entre théorie et observation est devenu de plus en plus plus important au fil des nouvelles observations (à l'époque de la première fournée de données de WMAP, il n'y avait qu'un facteur 2-3).

### Comment mesure t-on les abondances des éléments légers dans l'univers ?

- En analysant la carte du fond diffus cosmologique, on peut prédire la densité de baryons cosmiques avec une grande précision et ainsi tester la théorie de la nucléosynthèse primordiale.
> Les récentes données du satellite Planck donnent une prédiction de η = 6x10^-10, qui est en très bon accord avec les abondances observées de deutérium et d'hélium-4 de z=1000 à z=0.
- Les abondance en deutérium (un isotope de l'hydrogène constitué d'1 proton et 1 neutron) sont mesurées dans des nuages d'hydrogène très lointains (z~3) et très pauvres en éléments lourds qui sont observés sur la ligne de visée de quasars encore plus lointains. On ne peut pas mesurer les abondances en deutérium dans les étoiles, car il est entièrement détruit dans ces systèmes.
> À l'heure actuelle, ce sont les abondances qui sont (de loin) les mieux reproduites par la théorie.
- Les abondances en hélium-3 sont mesurées dans le milieu interstellaire dans la Voie Lactée, faute de pouvoir les mesurer dans des galaxies lointaines. Comme notre galaxie est riche en éléments lourds, on ne peut pas utiliser pour le moment les abondances en hélium-3 pour contraindre la période de nucléosynthèse primordiale.
- Les abondances en hélium-4 sont mesurées dans des régions de formation d'étoiles (appelées ***régions HII***) de galaxies voisines pauvres en éléments lourds (metal-poor)
- Les abondances en lithium sont principalement mesurées dans l'atmosphère (photosphère) de très vieilles étoiles (âgées de 11 à 13,5 milliards d'années) pauvres en éléments lourds présentes dans le halo stellaire de notre Galaxie. On connaît environ une centaine de ces étoiles dites de ***population II***. Les abondances observées sont relativement faibles, notamment parce que le lithium de l'atmosphère de ces étoiles est en permanence emporté dans les profondeurs par les mouvements convectifs de leur enveloppe, où il est détruit par la chaleur intense.
> Ce qu'on mesure en pratique ce sont les raies d'absorption dans le spectre de ces étoiles qui correspondent à la signature du lithium

### Quelles sont les solutions possibles au problème du lithium cosmique ?

- Soit nos prédictions théoriques (cosmologie+physique des particules) sont correctes, mais ce sont les observations astrophysiques qui sont incomplètes
> Grâce au LSST, on aura bientôt accès à une population bien plus grande d'étoiles pauvres en éléments lourds dans des galaxies proches et lointaines qui permettra d'avoir des statistiques plus fiables sur les abondances de lithium.
- Il pourrait aussi exister des processus encore inconnus au niveau de la physique nucléaire qui pourrait altérer nos prédictions sur les abondances du lithium
> Ces processus pourraient en particulier amplifier la destruction du béryllium-7. On peut vérifier cette hypothèse à l'aide d'expériences de physique nucléaire comme celles menées par la collaboration n_TOF (neutron-Time Of Flight) au CERN depuis 2018.
- Il pourrait enfin exister des solutions au-delà du modèle standard (comme la supersymmétrie par exemple) qui pourrait impliquer de nouveaux processus au niveau cosmologique ou de la physique des particules
> Ces solutions sont testées dans les accélérateurs de particules comme le LHC et du côté des expériences de détection de matière noire.

# Le principe cosmologique

- [Probing cosmic isotropy with a new X-ray galaxy cluster sample through the LX–T scaling relation](https://www.aanda.org/articles/aa/full_html/2020/04/aa36602-19/aa36602-19.html) - Migkas et al (2020)
- [Observation d'une anisotropie de l'Univers !](https://www.ca-se-passe-la-haut.fr/2020/04/observation-dune-anisotropie-de-lunivers.html) - Ca se passe là-haut
- [The CMB Dipole: Eppur Si Muove](https://arxiv.org/abs/2111.12186) - Sullivan et Scott (2021)
- [Cosmic Microwave Background Dipole](https://astronomy.swin.edu.au/cosmos/c/Cosmic+Microwave+Background+Dipole) - SAO Encyclopedia of astronomy
- [The Motion of the Local Group with Respect to the 15,000 Kilometer per Second Abell Cluster Inertial Frame](https://adsabs.harvard.edu/full/1994ApJ...425..418L) - Lauer et Postman (1994)
- [Testing the Cosmological Principle in the radio sky](https://arxiv.org/pdf/1905.12378.pdf) - Bengaly et al (2021)
- [Is the Observable Universe Consistent with the Cosmological Principle?](https://arxiv.org/abs/2207.05765) - Aluri et al (2022)
- [Cosmologists Parry Attacks on the Vaunted Cosmological Principle](https://www.quantamagazine.org/giant-arc-of-galaxies-puts-basic-cosmology-under-scrutiny-20211213/) - Quanta (2021)
- [Un intrigant anneau géant fait de sursauts gamma](https://www.futura-sciences.com/sciences/actualites/sursaut-gamma-intrigant-anneau-geant-fait-sursauts-gamma-59317/) - Futura Sciences (2015)
- [The dipole repeller](https://www.nature.com/articles/s41550-016-0036) - Hoffman et al (2017)
- [Testing the Cosmological Principle](https://indico.cern.ch/event/1036660/attachments/2241767/3801102/TestingCosmoPrinciple.pdf) - Subir Sarkar (2021)

---

- L'homogénéité (à grande échelle, ie au-delà de 70 h-1 Mpc) et l'isotropie (statistiquement parlant) de l'univers est à la base du modèle standard de la cosmologie. Autrement dit, il n'y a pas d'observateur privilégié ou de direction privilégiée dans l'univers. Où que l'on soit dans l'univers, et quelque soit la direction où l'on regarde, on devrait observer la même chose à grande échelle. Cette idée constitue le ***principe cosmologique***.
> The cosmological principle grew out of the Copernican principle, Nicolaus Copernicus’ 1543 realization that Earth is not the fixed center of creation. Not only is Earth not special, but nothing anywhere is special. 
- The universe is clearly not homogeneous on the human scale. Teleport a person one light-year from here and you’ll ruin their day. But drop the Hubble Space Telescope halfway across the universe, and it will return familiar-looking galaxy-filled images. 
- Theorists reconstruct the cosmos’s past and predict its future using a standard theoretical model based largely on general relativity, Albert Einstein’s theory of gravity. Einstein’s theory describes the interplay between matter and space-time — the bendy fabric of the universe. But Einstein’s treatment involves 10 interlinked equations and 20 variables, a system of equations that is generally too complicated to solve.

Cosmologists lean on the cosmological principle to restrict their focus to a universe acting as a smooth and symmetric fluid. By ignoring bumps of matter like galaxies and requiring the universe to expand in the same way along all three axes, the cosmological principle deletes parts of the equations and links some of the variables, dramatically simplifying the system of equations. Theorists can then predict the velocity and acceleration of the cosmos’s expansion with just two equations — the Friedmann equations, derived from Einstein’s by Alexander Friedmann, a Russian cosmologist, in 1922. It’s a bit like computing the volume of the Earth: You could fret over every mountain and ravine, or you could assume the planet is a sphere and call it a day.

### Le dipôle cosmologique

- Lorsque l'on cartographie le fond diffus cosmologique, on remarque qu'il n'est pas directement isotrope. La plus grande anisotropie de température qu'il présente est un dipole. On l'appelle le ***dipôle cosmologique*** (CMB dipole). Son amplitude (ΔT/T\~10-3) est plus grande que les autres fluctuations de température (ΔT/T\~10-5). 
> Afin d'étudier les anisotropies de l'ordre de 10-5, on doit donc soustraire la contribution du dipôle cosmologique.
- Son existence a été révélée pour la première fois par les mesures du satellite COBE dans les années 90.

*Dipôle cosmologique révélé par COBE ([Source](https://apod.nasa.gov/apod/ap010128.html))*

![COBE_dipole](https://apod.nasa.gov/apod/image/0101/dipole_cobe.jpg)

- On interprète généralement ce dipôle comme le résultat du mouvement combiné de la Terre autour du Soleil, du Soleil autour de la Voie Lactée, de la Voie Lactée dans le Groupe Local, et du Groupe Local vers un ***Grand Attracteur*** dans le référentiel du CMB (modèle proposé par Lynden Bell et al en 1988), qui génère un ***effet Doppler*** : on observe le CMB avec un décalage spectral vers le bleu dans la direction de notre mouvement et un décalage vers le rouge dans la direction opposée. 
> * Notre système solaire se déplace à environ 370 km/s par rapport au référentiel dans lequel le CMB est isotrope, et le Groupe Local se déplace à environ 630 (+-20) km/s (2,2 millions de km/h) par rapport au CMB dans la direction du super-amas de Shapley (à 600 millions d'années-lumière d'ici), à cause du Grand Attracteur (une région contenant une demi-douzaine d'amas de galaxies qui se trouve au coeur du super-amas Laniakea, à 150 millions d'années-lumière d'ici)
> * Ce Grand Attracteur est difficile à observer dans le domaine du visible parce qu'il est situé directement derrière le plan Galactique.
- Depuis 2017, on sait aussi qu'il existe une région relativement vide de matière (noire et baryonique) située dans la direction opposée du Grand Attracteur. Cette région, qu'on a appelé ***répulseur du dipôle*** (Dipole Repeller), agit de manière effective comme un répulseur sur le mouvement des galaxies avoisinantes. La force attractive causée par le Grand Attracteur (sur-densité de matière dans la direction de notre mouvement) ET la force "répulsive" causée par le répulseur du dipôle (sous-densité de matière dans la direction opposée) contribuent de manière équivalente au mouvement du Groupe Local dans l'espace, et donc au dipôle cosmologique.

*Le mouvement du Groupe Local dans son contexte cosmique (Source : Hoffman et al 2017)*

![dipole-repeller](https://user-images.githubusercontent.com/4954089/203095826-6f110b58-2853-4938-80f3-af813e76cfe7.png)

- Lorsque ce mouvement global est corrigé, le CMB est remarquablement isotrope.
- Cependant, l'isotropie du CMB (à l'échelle de 10-5) n'implique pas forcément l'isotropie de l'univers. Pour tester l'isotropie de l'univers, on a besoin d'autres mesures indépendantes, par exemple en étudiant la distribution statistique de matière dans l'univers. 

### Des structures plus grandes que l'échelle d'homogénéité

- Depuis les années 80, des relevés astronomiques révèlent l'existence d'une liste croissante de grandes structures dont la taille dépasse l'échelle maximale à partir de laquelle l'univers est sensé être homogène. L'estimation haute de cette échelle d'homogénéité est de ~370 Mpc (1,2 milliards d'années-lumière).
- La plupart de ces structures sont des ***amas de quasars*** (en anglais : large quasar group ou LQG). Ces collections de quasars font partie des plus grandes structures cosmiques connues. On pense qu'ils pourraient être les précurseurs des filaments galactiques que l'on observe dans l'univers proche.
- ***Liste des plus grandes structures cosmiques par ordre croissant de taille :***
> * Le ***grand mur de Sloan*** est une structure mesurant plus d'1,37 milliards d'années-lumière, découverte en 2003 dans les données du SDSS
> * Le ***Clowes–Campusano Large Quasar Group*** (CCLQG) est un amas de quasars constitué de 34 quasars et mesurant environ 2 milliards d’années-lumière de diamètre (~630 Mpc), découvert en 1991.
> * ***L'amas de quasars U1.11*** est une collection de 38 quasars dont le diamètre est estimé à 2,5 milliards d'années-lumière (~780 Mpc). Il a été découvert en 2011 à proximité du CCLQG dans les données du SDSS.
> * ***L'arc géant*** est une structure en forme de sourire mesurant 3,3 milliards d'années-lumière (1/28e du diamètre de l'univers observable), découverte en 2021 dans les données du SDSS. Sa taille apparente dans le ciel est aussi étendue que 20 pleines lunes. Elle se trouve à 9,2 milliards d'années-lumière d'ici (z~0.8).
> * Le ***Huge-LQG*** (Huge Large Quasar Group, « Immense grand amas de quasars ») est un amas composé de 73 quasars mesurant environ 4 milliards d'années-lumière de diamètre (~1240 Mpc), découvert en 2013 dans les données du SDSS
> * ***L'anneau géant de sursauts gamma*** (Giant GRB ring) est un grand anneau constitué par 9 sursauts gamma (hypernovae ou collisions entre étoiles à neutrons) qui s'étalerait sur environ 5,6 milliards d'années-lumière. Il a été découvert en 2015. Son diamètre apparent dans le ciel est équivalent à 70 pleines lunes.
> * Le ***Grand Mur d'Hercule-Couronne boréale*** est un filament cosmique mesurant 10 milliards d'années-lumière, découvert en 2013 dans les données d'un relevé de sursauts gamma. Il s'agit de la plus vaste et la plus massive structure cosmique connue de l'univers observable. Mais certaines études mettent en doute son existence.
- Le modèle standard de la cosmologie n'interdit pas complètement l'existence de telles structures, mais il les rend extrêmement rares.

### Conclusion

- Avec les données actuelles, on ne peut pas encore conclure que le principe cosmologique n'est pas valide. Mais dans les décennies à venir, 

- The isotropy of the late Universe and consequently of the X-ray galaxy cluster scaling relations is an assumption greatly used in astronomy. However, within the last decade, many studies have reported deviations from isotropy when using various cosmological probes; a definitive conclusion has yet to be made. New, effective and independent methods to robustly test the cosmic isotropy are of crucial importance.

referred axes noted in other astronomical surveys spanning the
electromagnetic spectrum were also found to be aligned with the CMB kinematic dipole, pointing towards
the Virgo cluster [73, 182]. The standard narrative says that these are coincidences
-  
Previous work has shown no statistically significant violation of isotropy in the obser-
vational data of Type Ia Supernova distances [4, 5, 6] and of gamma-ray bursts [7, 8, 9].
More stringent tests require the far higher number densities delivered by large galaxy sur-
veys. The simplest way to test consistency with the CMB is to measure the dipole of a
(sufficiently wide) galaxy survey, which should be aligned with the direction of the CMB
dipole [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]. For currently available data sets,
the matter dipole direction is not inconsistent with the CMB, but the amplitude is too large,
probably arising from the quality of current data sets. Forecasts predict that future all-sky
radio continuum surveys with the SKA should achieve the accuracy necessary to make a
stringent test of consistency with the CMB dipole [23, 24]
- ensions
have emerged within the ΛCDM model, most notably a statistically significant discrepancy in the
value of the Hubble constant, H0. Since the notion of cosmic expansion determined by a single
parameter is intimately tied to the CP, implications of the H0 tension may extend beyond ΛCDM
to the CP itself. This review surveys current observational hints for deviations from the expecta-
tions of the CP, highlighting synergies and disagreements that warrant further study. Setting aside
the debate about individual large structures, potential deviations from the CP include variations of
cosmological parameters on the sky, discrepancies in the cosmic dipoles, and mysterious alignments
in quasar polarizations and galaxy spins.
- If the Universe is not FLRW, but we view it through the prism of FLRW, cos-
mological tensions are inevitable. Interestingly, a host of fascinating observational tensions exist, including
the H0 tension [45, 51–58], the S8 tension [178–181], potentially a curvature tension [119, 296], and an
AISW tension 
- But our peculiar velocity might not fully explain the perceived lopsidedness of the CMB; the distortion could also include the effect of the whole universe drifting. If this is the case, gauging our motion against distant galaxies will give a different result than if we measure our speed against the CMB, since those galaxies will be moving too. 
-  we document a series of intriguing alignments that are surprising in a statistically homo-
geneous and isotropic universe. One curious aspect of some of these alignments is their observation over
large – potentially gigaparsecs – scales and axes that overlap with the CMB dipole direction. If not due to
experimental systematics or interstellar physics, one exciting possibility is that these features are cosmo-
logical in origin. 

# La tension de Hubble

*Sources*

- [La crise cosmique de la constante de Hubble](https://www.pourlascience.fr/sd/cosmologie/la-crise-cosmique-de-la-constante-de-hubble-19032.php) - Pour la Science (2020)
- [Exploring the Hubble tension](https://cerncourier.com/a/exploring-the-hubble-tension/) - CERN Courrier (2021)
- [La tension de Hubble : la cosmologie en crise ?](https://scienceetonnante.com/2022/02/04/la-tension-de-hubble/) - Science Etonnante (2022)
- [Tensions dans le modèle cosmologique : l’espoir d’une nouvelle génération de télescopes](https://www.pourlascience.fr/sd/cosmologie/tensions-dans-le-modele-cosmologique-l-espoir-d-une-nouvelle-generation-de-telescopes-23923.php) - Pour  la Science (2022)

---



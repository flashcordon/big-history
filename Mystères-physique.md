# La cosmologie quantique

*Sources*

- [Quantum cosmology: a review](https://arxiv.org/abs/1501.04899) - Bojowald (2015)
- [Loop Quantum Cosmology: A brief review](https://arxiv.org/abs/1612.01236) - Agulo et Singh (2016)
- [Conceptual Problems in Quantum Gravity and Quantum Cosmology](https://www.hindawi.com/journals/isrn/2013/509316/) - Kiefer (2013)

---

# Matière noire

*Sources*

- [A History of Dark Matter](https://arxiv.org/pdf/1605.04909.pdf) - Berton et Hooper (2016)
- [Dark matter and cosmic structure](https://arxiv.org/abs/1210.0544) - Frenk and White (2012)
- [La matière noire, une sombre affaire](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2016/04/refdp201651p4.pdf) - Combes (2016)

---

# Energie sombre

*Sources*

- [Dark Energy: A Short Review](https://arxiv.org/abs/1401.0046) - Mortonson et al (2013)
- [Dark Energy Versus Modified Gravity](https://www.annualreviews.org/doi/abs/10.1146/annurev-nucl-102115-044553) - Joyce et al (2016)
- [Dark energy: A brief review](https://link.springer.com/article/10.1007/s11467-013-0300-5) - Li et al (2013)
- [Changement de rythme dans l’expansion de l’Univers : Un premier rôle pour le côté obscur]() - Palanque Delabrouille (2016)
- [L'histoire de l'expansion de l'Univers dévoilée](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2022/01/refdp202271p22.pdf) - Burtin (2022)
- [Avancées de la recherche Cisaillement gravitationnel et sondage de l’Univers](https://www.refletsdelaphysique.fr/articles/refdp/pdf/2006/01/refdp20061p5.pdf) - Mellier (2006)
- [La constante cosmologique : la plus grande erreur d’Einstein](https://books.openedition.org/cdf/9443?lang=fr) - Combes

---

- In the first modern cosmological model, Einstein [1] modified his field equation of
General Relativity (GR), introducing a “cosmological term” that enabled a solution with
time-independent, spatially homogeneous matter density ρm and constant positive space
curvature. Although Einstein did not frame it this way, one can view the “cosmological
constant” Λ as representing a constant energy density of the vacuum [2], whose repulsive
gravitational effect balances the attractive gravity of matter and thereby allows a static
solution. After the development of dynamic cosmological models [3,4] and the discovery
of cosmic expansion [5], the cosmological term appeared unnecessary, and Einstein and
de Sitter [6] advocated adopting an expanding, homogeneous and isotropic, spatially flat,
matter-dominated universe as the default cosmology until observations dictated otherwise.
- By the mid-1990s, Big Bang cosmology was convincingly established, but the
Einstein-de Sitter model was showing numerous cracks, under the combined onslaught
of data from the cosmic microwave background (CMB), large scale galaxy clustering,
and direct estimates of the matter density, the expansion rate (H0), and the age of the
Universe. Introducing a cosmological constant offered a potential resolution of many of
these tensions. In the late 1990s, supernova surveys by two independent teams provided
direct evidence for accelerating cosmic expansion [8,9], establishing the cosmological
constant model (with Ωm ≈ 0.3, ΩΛ ≈ 0.7) as the preferred alternative to the Ωm = 1
scenario. Shortly thereafter, CMB evidence for a spatially flat universe [10,11], and
thus for Ωtot ≈ 1, cemented the case for cosmic acceleration by firmly eliminating the
free-expansion alternative with Ωm ≪ 1 and ΩΛ = 0. Today, the accelerating universe is
well established by multiple lines of independent evidence from a tight web of precise
cosmological measurements.
-  acceleration could arise from a more general form of “dark
energy” that has negative pressure, typically specified in terms of the equation-of-state-
parameter w = p/ρ (= −1 for a cosmological constant). Furthermore, the conclusion that
acceleration requires a new energy component beyond matter and radiation relies on the
assumption that GR is the correct description of gravity on cosmological scales. The title
of this article follows the common but inexact usage of “dark energy” as a catch-all term
for the origin of cosmic acceleration, regardless of whether it arises from a new form of
energy or a modification of GR. 

# L'inflation cosmique

# Le problème de la hiérarchie

*Sources*

- [The Hierarchy Problem: why the Higgs has a snowball’s chance in hell](https://www.quantumdiaries.org/2012/07/01/the-hierarchy-problem-why-the-higgs-has-a-snowballs-chance-in-hell/) - Quantum Diaries
- [The Hierarchy Problem](https://profmattstrassler.com/articles-and-posts/particle-physics-basics/the-hierarchy-problem/) - Matt Strassler 
- [The Higgs, The Hierarchy Problem, and the LHC](https://www2.physics.ox.ac.uk/sites/default/files/2014-11-24/higgs_lhc_jmr_nov14_pdf_93873.pdf) - John March-Russell
- [The Mystery of the Higgs Boson's Mass](https://podcasts.google.com/feed/aHR0cHM6Ly9mZWVkcy5idXp6c3Byb3V0LmNvbS8xMTYyNjEzLnJzcw/episode/QnV6enNwcm91dC04OTE4NTUw) - Why this universe
- [Naturalness after the Higgs](https://cerncourier.com/a/naturalness-after-the-higgs/) - CERN
- [A Deepening Crisis Forces Physicists to Rethink Structure of Nature’s Laws](https://www.quantamagazine.org/crisis-in-particle-physics-forces-a-rethink-of-what-is-natural-20220301/) - Quanta Magazine
- [The Dawn of the Post-Naturalness Era](https://arxiv.org/abs/1710.07663) - Giudice (2017)
- [A Lecture on the Hierarchy Problem and Gravity](https://cds.cern.ch/record/2120792/files/CERN-2013-003-p145.pdf) - Dvali (2013) 
- [The Higgs & the Hierarchy Problem](https://www.youtube.com/watch?v=iywSF7BGhyU) - Anna Barth
- [Big Mysteries: The Higgs Mass](https://www.youtube.com/watch?v=IjCypYnBYwQ) - Fermilab
- [Naturally Speaking: The Naturalness Criterion and Physics at the LHC](https://arxiv.org/abs/0801.2562) - Giudice (2008)
- [Histoire de la cosmologie](https://cosmology.education/booklet/booklet.pdf) - Lucas Gautheron
- [Naturalness: A Snowmass White Paper](https://arxiv.org/abs/2205.05708) - Craig (2022)
- ["Particle Physics: The Higgs Boson and Beyond" by Andreas Hoecker (CERN)](https://youtu.be/XX4pL7pwl7w) - SLAC (2012)
- ["The Once and Future Higgs" by Prof. Nathaniel Craig (University of California, Santa Barbara)](https://youtu.be/GduJt1V2eNk) - TIFR (2021)

---

- Le modèle standard est constitué de 24 particules fondamentales.
- Le boson de Higgs joue un rôle fondamental dans le modèle standard
- Le boson de Higgs brise la symétrie électrofaible et donne sa masse aux bosons et aux fermions
- Les particules interagissent avec le champ de Higgs (qui remplit tout l'espace-temps), ce qui réduit leur vitesse. Et plus elles interagissent fortement, plus leur masse est grande (c'est proportionnel).
- Découvert en 2012 par le LHC, un accélérateur de particules de 27km de circonférence
- On peut sonder la composition de la matière de plus en plus profondément à mesure que l'on fait des collisions de plus en plus énergétiques
- Dans le modèle standard, le higgs est "le seul scalaire fondamental (tous les autres scalaires sont des états composites)"
- Un des problèmes les plus importants de la physique contemporaine
- coincidence spectaculaire
- Boson de Higgs prédit en 1954
- En une phrase : la masse observée du boson de Higgs est de 125 GeV (125 fois la masse du proton ?). Pourtant, des corrections quantiques issues des interactions avec d'autres particules du modèle standard prédisent une masse 17 ordres de grandeur plus grande (au niveau de la masse de Planck). C'est le problème de la hiérarchie électrofaible.
- The Higgs boson plays a key role in the Standard Model: it is related to the unification of the electromagnetic and weak forces, explains the origin of elementary particle masses
- Higgs—the last piece of the Standard Model
- Hierarchy problem. This is often ‘explained’ by saying that quantum corrections want to make the Higgs much heavier than we need it to be… say, 125-ish GeV. 
- The Higgs has a snowball’s chance in hell of having a mass in that ballpark.
- If you put a glass of water in a really hot place—you expect it to also become really hot, maybe even to off into steam.  It would be really surprising if we put an ice cube in a hot oven and 10 minutes later it had not melted. This is because the ambient thermal energy is expected to be transferred to the ice cube by the energetic air molecules bouncing off it. Sure, it is possible that the air molecules just happen to bounce in a way that doesn’t impart much thermal energy—but that would be ridiculously improbable, as we learn in thermodynamics.
- The Higgs is very similar: we expect its mass to be around 125 GeV (not too far from W and Z masses), but ambient quantum energy wants to make its mass much larger through interactions with virtual particles. While it is possible that the Higgs stays light without any additional help, it’s ridiculously improbable, as we learn from quantum physics.
- the Standard Model really, really wants the Higgs to be around the 100 GeV scale. This is because it needs something to “unitarize longitudinal vector boson scattering.” It needs to have some Higgs-like state accessible at low energies to explain why certain observed particle interactions are well behaved.
- The Hierarchy problem has been the main motivation for new physics at the TeV scale for over two decades. 
- it is possible that the Higgs mass is 125 GeV due to some miraculous almost-cancellation that set it to be in just the right ballpark to unitarize longitudinal vector boson scattering. But such miracles are rare in physics without any a priori explanation. The electron mass is an excellent example. There are some apparent (and somewhat controversial) counter-examples: the cosmological constant problem is a much more severe ‘fine-tuning’ problem which may be explained anthropically rather than through more fundamental principles.
-  What are the possible ways to solve the Hierarchy problem?
- There are two main directions that most people consider:
> * Supersymmetry. Recall in our electron analogy that the solution to the ‘electron mass hierarchy problem’ was that quantum mechanics doubled the number of particles: in addition to the electron, there was also a positron. The virtual electron–positron contributions solved the problem by smearing out the electric charge. Supersymmetry is an analogous idea where once again the set of particles is doubled, and in doing so the loop contributions of one particle to the Higgs are cancelled by the loop contributions of its super-partner. Supersymmetry has deep connections to an extension of space-time symmetry since it relates matter particles to force particles.
> * Compositeness/extra dimensions. The other solution is that maybe our description of physics breaks down much sooner than the Planck scale. In particular, maybe at the TeV scale the Higgs no longer behaves like a scalar particles, but rather as a bound state of two fermions. This is precisely what happens with the mesons: even though the pion is a scalar, there is no pion ‘hierarchy problem’ because as you probe smaller distances, you realize the pion is actually a bound state of two quarks and it starts behaving as such. One of the beautiful developments of theoretical physics in the 1990s and early 2000s was the realization that this is precisely what is being described by theories of extra dimensions through the so-called holographic principle.
- An important feature of nature that puzzles scientists like myself is known as the hierarchy, meaning the vast discrepancy between aspects of the weak nuclear force and gravity. There are several different ways to describe this hierarchy, each emphasizing a different feature of it. Here is one:
- The mass of the smallest possible black hole defines what is known as the Planck Mass. 
-     The masses of the W and Z particles, the force carriers of the weak nuclear force, are about 10,000,000,000,000,000 times smaller than the Planck Mass. Thus there is a huge hierarchy in the mass scales of weak nuclear forces and gravity.
- When faced with such a large number as 10,000,000,000,000,000, ten quadrillion, the question that physicists are naturally led to ask is: where did that number come from? It might have some sort of interesting explanation.
- But while trying to figure out a possible explanation, physicists in the 1970s realized there was actually a serious problem, even a paradox, behind this number. The issue, now called the hierarchy problem, has to do with the size of the non-zero Higgs field, which in turn determines the mass of the W and Z particles.
- The non-zero Higgs field has a size of about 250 GeV, and that gives us the W and Z particles with masses of about 100 GeV. But it turns out that quantum mechanics would lead us to expect that this size of a Higgs field is unstable, something like (warning: imperfect analogy ahead) a vase balanced precariously on the edge of a table. With the physics we know about so far, the tendency of quantum mechanics to jostle — those quantum fluctuations I’ve mentioned elsewhere — would seem to imply that there are two natural values for the Higgs field — in analogy to the two natural places for the vase, firmly placed on the table or smashed on the floor. Naively, the Higgs field should either be zero, or it should be as big as the Planck Energy, 10,000,000,000,000,000 times larger than it is observed to be. Why is it at a value that is non-zero and tiny, a value that seems, at least naively, so unnatural? This is the hierarchy problem.
- Many theoretical physicists have devoted significant fractions of their careers to trying to solve this problem. Some have argued that new particles and new forces are needed (and their theories go by names such as supersymmetry, technicolor , little Higgs, etc.) Some have argued that our understanding of gravity is mistaken and that there are new unknown dimensions (“extra dimensions”) of space that will become apparent to our experiments at the Large Hadron collider in the near future. Others have argued that there is nothing to explain, because of a selection effect: the universe is far larger and far more diverse than the part that we can see, and we live in an apparently unnatural part of the universe mainly because the rest of it is uninhabitable — much the way that although rocky planets are rare in the universe, we live on one because it’s the only place we could have evolved and survived. There may be other solutions to this problem that have not yet been invented.
- Many of these solutions — certainly all the ones with new particles and forces or with new dimensions — predict that new phenomena should be visible at the Large Hadron Collider. 
- By the way, you will often read the hierarchy problem stated as a problem with the Higgs particle mass.  This is incorrect.  The problem is with how big the non-zero Higgs field is.  (For experts — quantum mechanics corrects not the Higgs particle mass but the Higgs mass-squared parameter, changing the Higgs field potential energy and thus the field’s value, making it zero or immense.  That’s a disaster because the W and Z masses are known.  The Higgs mass is unknown, and therefore it could be very large — if the W and Z masses were very large too.  So it is the W and Z masses — and the size of the non-zero Higgs field — that are the problem, both logically and scientifically.)
- Either new particles are keeping the Higgs boson light, or the universe is oddly fine-tuned for our existence. 
- When Victor Weisskopf sat down in the early 1930s to compute the energy of a solitary electron, he had no way of knowing that he’d ultimately discover what is now known as the electroweak hierarchy problem. Revisiting a familiar puzzle from classical electrodynamics – that the energy stored in an electron’s own electric field diverges as the radius of the electron is taken to zero (equivalently, as the energy cutoff of the theory is taken to infinity) – in Dirac’s recently proposed theory of relativistic quantum mechanics, he made a remarkable discovery: the contribution from a new particle in Dirac’s theory, the positron, cancelled the divergence from the electron itself and left a quantum correction to the self-energy that was only logarithmically sensitive to the cutoff. 
-  the coupling between the Higgs boson and other particles of the Standard Model (SM) leads to yet another divergent self-energy, for which the logic of naturalness implied new physics at around the TeV scale. Thus the electroweak hierarchy problem was born – not as a new puzzle unique to the Higgs, but rather the latest application of Weisskopf’s wildly successful logic (albeit one for which the answer is not yet known). 
- History suggested two possibilities. As a scalar, the Higgs could only benefit from the sort of cancellation observed among fermions if there is a symmetry relating bosons and fermions, namely supersymmetry. Alternatively, it could be a light product of compositeness, just as the pions and kaons are light bound states of the strong interactions. These solutions to the hierarchy problem came to dominate expectations for physics beyond the SM, with a sharp target – the TeV scale – motivating successive generations of collider experiments. Indeed, when the physics case for the LHC was first developed in the mid-1980s, it was thought that new particles associated with supersymmetry or compositeness would be much easier to discover than the Higgs itself. But while the Higgs was discovered, no signs of supersymmetry or compositeness were to be found.
- In the meantime, other naturalness problems were brewing. The vacuum energy – Einstein’s infamous cosmological constant – suffers a divergence of its own, and even the finite contributions from the SM are many orders of magnitude larger than the observed value. Although natural expectations for the cosmological constant fail, an entirely different set of logic seems to succeed in its place. To observe a small cosmological constant requires observers, and observers can presumably arise only if gravitationally-bound structures are able to form. As Steven Weinberg and others observed in the 1980s, such anthropic reasoning leads to a prediction that is remarkably close to the value ultimately measured in 1998. To have predictive power, this requires a multitude of possible universes across which the cosmological constant varies; only the ones with sufficiently small values of the cosmological constant produce observers to bear witness.
- An analogous argument might apply to the electroweak hierarchy problem: the nuclear binding energy is no longer sufficient to stabilise the neutron within typical nuclei if the Higgs vacuum expectation value (VEV) is increased well above its observed value. If the Higgs VEV varies across a landscape of possible universes while its couplings to fermions are kept fixed, only universes with sufficiently small values of the Higgs VEV would lead to complex atoms and, presumably, observers. Although anthropic reasoning for the hierarchy problem requires stronger assumptions than for the cosmological-constant problem, its compatibility with null results at the LHC is enough to raise questions about the robustness of natural reasoning. 
- The success of the relaxion hypothesis in solving the hierarchy problem hinges on an array of other questions involving gravity. Whether the relaxion potential can remain sufficiently smooth over the vast trans-Planckian distances in field space required to set the value of the weak scale is an open question, one that is intimately connected to the fate of global symmetries in a theory of quantum gravity (itself the target of active study in what is known as the Swampland programme).
- the recognition that cosmology might play a role in solving the hierarchy problem has given rise to a plethora of new ideas. For instance, in Raffaele D’Agnolo and Daniele Teresi’s recent paradigm of “sliding naturalness”, the Higgs is coupled to a new scalar whose potential features two minima. In the true minimum, the cosmological constant is large and negative, and the universe would crunch away into oblivion if it ended up in this vacuum. In the second, local minimum, the cosmological constant is safely positive (and can be made compatible with the small observed value of the cosmological constant by Weinberg’s anthropic selection). The Higgs couples to this scalar in such a way that a large value of the Higgs VEV destabilises the “safe” minimum. During the inflationary epoch, only universes with suitably small values of the Higgs VEV can grow and expand, while those with large values of the Higgs VEV crunch away. A second scalar coupled analogously to the Higgs can explain why the VEV is small but non-zero. 
- Alternatively, in the paradigm of “Nnaturalness” proposed by Nima Arkani-Hamed and others, the multitude of SMs over which the Higgs mass varies occur in one universe, rather than many. The fact that the universe is predominantly composed of one copy of the SM with a small Higgs mass can be explained if inflation ends and reheats the universe through the decay of a single particle. If this particle is sufficiently light, it will preferentially reheat the copy of the SM with the smallest non-zero value of the Higgs VEV, even if it couples symmetrically to each copy. The sub-dominant energy density deposited in other copies of the SM leaves its mark in the form of dark radiation susceptible to detection by the Simons Observatory or upcoming CMB-S4 facility. 
- Finally, Gian Giudice, Matthew Mccullough and Tevong You have recently shown that inflation can help to understand the electroweak hierarchy problem by analogy with self-organised criticality. Just as adding individual grains of sand to a sandpile induces avalanches over diverse length scales – a hallmark of critical behaviour, obtained without tuning parameters – so too can inflation drive scalar fields close to critical points in their potential. This may help to understand why the observed Higgs mass lies so close to the boundary between the unbroken and broken phases of electroweak symmetry without fine tuning.
- Underlying Weisskopf’s natural reasoning is a long-standing assumption about relativistic theories of quantum mechanics: physics at short distances (the ultraviolet, or UV) is decoupled from physics at long distances (the infrared, or IR), making it challenging to apply a theory involving a large energy scale to a much smaller one without fine tuning. This suggests that loopholes may be found in theories that mix the UV and the IR, as is known to occur in quantum gravity. 
- While the connection between this type of UV/IR mixing and the mass of the Higgs remains tenuous, there are encouraging signs of progress.
- We have come a long way since Weisskopf first set out to understand the self-energy of the electron. The electroweak hierarchy problem is not the first of its kind, but rather the one that remains unresolved. The absence of supersymmetry or compositeness at the TeV scale beckons us to search for new solutions to the hierarchy problem, rather than turning our backs on it. In the decade since the discovery of the Higgs, this search has given rise to a plethora of novel approaches, building new bridges between particle physics, cosmology and gravity along the way. Despite the many differences among these new approaches, they share a common tendency to leave imprints on the Higgs boson. And so, as ever, we must look to experiment to show the way. 
-  The hierarchy problem, as the puzzle is called, asks why the Higgs boson is so lightweight — a hundred million billion times less massive than the highest energy scales that exist in nature. The Higgs mass seems unnaturally dialed down relative to these higher energies, as if huge numbers in the underlying equation that determines its value all miraculously cancel out.
-  The extra particles would have explained the tiny Higgs mass, restoring what physicists call “naturalness” to their equations. But after the LHC became the third and biggest collider to search in vain for them, it seemed that the very logic about what’s natural in nature might be wrong.
-  At first, the community despaired. “You could feel the pessimism,” said Isabel Garcia Garcia, a particle theorist at the Kavli Institute for Theoretical Physics at the University of California, Santa Barbara, who was a graduate student at the time. Not only had the $10 billion proton smasher failed to answer a 40-year-old question, but the very beliefs and strategies that had long guided particle physics could no longer be trusted. People wondered more loudly than before whether the universe is simply unnatural, the product of fine-tuned mathematical cancellations. Perhaps there’s a multiverse of universes, all with randomly dialed Higgs masses and other parameters, and we find ourselves here only because our universe’s peculiar properties foster the formation of atoms, stars and planets and therefore life. This “anthropic argument,” though possibly right, is frustratingly untestable.
-  Some of those who remained set to work scrutinizing decades-old assumptions. They started thinking anew about the striking features of nature that seem unnaturally fine-tuned — both the Higgs boson’s small mass, and a seemingly unrelated case, one that concerns the unnaturally low energy of space itself.
-  Their introspection is bearing fruit. Researchers are increasingly zeroing in on what they see as a weakness in the conventional reasoning about naturalness. It rests on a seemingly benign assumption, one that has been baked into scientific outlooks since ancient Greece: Big stuff consists of smaller, more fundamental stuff — an idea known as reductionism. “The reductionist paradigm … is hard-wired into the naturalness problems,” said Nima Arkani-Hamed, a theorist at the Institute for Advanced Study in Princeton, New Jersey.
- Now a growing number of particle physicists think naturalness problems and the null results at the Large Hadron Collider might be tied to reductionism’s breakdown. “Could it be that this changes the rules of the game?” Arkani-Hamed said. In a slew of recent papers, researchers have thrown reductionism to the wind. They’re exploring novel ways in which big and small distance scales might conspire, producing values of parameters that look unnaturally fine-tuned from a reductionist perspective.
- The Large Hadron Collider did make one critical discovery: In 2012, it finally struck upon the Higgs boson, the keystone of the 50-year-old set of equations known as the Standard Model of particle physics, which describes the 17 known elementary particles.
- The discovery of the Higgs confirmed a riveting story that’s written in the Standard Model equations. Moments after the Big Bang, an entity that permeates space called the Higgs field suddenly became infused with energy. This Higgs field crackles with Higgs bosons, particles that possess mass because of the field’s energy. As electrons, quarks and other particles move through space, they interact with Higgs bosons, and in this way they acquire mass as well.
- After the Standard Model was completed in 1975, its architects almost immediately noticed a problem.
- When the Higgs gives other particles mass, they give it right back; the particle masses shake out together. Physicists can write an equation for the Higgs boson’s mass that includes terms from each particle it interacts with. All the massive Standard Model particles contribute terms to the equation, but these aren’t the only contributions. The Higgs should also mathematically mingle with heavier particles, up to and including phenomena at the Planck scale, an energy level associated with the quantum nature of gravity, black holes and the Big Bang. Planck-scale phenomena should contribute terms to the Higgs mass that are huge — roughly a hundred million billion times larger than the actual Higgs mass. Naively, you would expect the Higgs boson to be as heavy as they are, thereby beefing up other elementary particles as well. Particles would be too heavy to form atoms, and the universe would be empty.
- For the Higgs to depend on enormous energies yet end up so light, you have to assume that some of the Planckian contributions to its mass are negative while others are positive, and that they’re all dialed to just the right amounts to exactly cancel out. Unless there’s some reason for this cancellation, it seems ludicrous — about as unlikely as air currents and table vibrations counteracting each other to keep a pencil balanced on its tip. This kind of fine-tuned cancellation physicists deem “unnatural.”
- Within a few years, physicists found a tidy solution: supersymmetry, a hypothesized doubling of nature’s elementary particles. Supersymmetry says that every boson (one of two types of particle) has a partner fermion (the other type), and vice versa. Bosons and fermions contribute positive and negative terms to the Higgs mass, respectively. So if these terms always come in pairs, they’ll always cancel.
- The search for supersymmetric partner particles began at the Large Electron-Positron Collider in the 1990s. Researchers assumed the particles were just a tad heavier than their Standard Model partners, requiring more raw energy to materialize, so they accelerated particles to nearly light speed, smashed them together, and looked for heavy apparitions among the debris.
- The fabric of space, even when devoid of matter, seems as if it should sizzle with energy — the net activity of all the quantum fields coursing through it. When particle physicists add up all the presumptive contributions to the energy of space, they find that, as with the Higgs mass, injections of energy coming from Planck-scale phenomena should blow it up. Albert Einstein showed that the energy of space, which he dubbed the cosmological constant, has a gravitationally repulsive effect; it causes space to expand faster and faster. If space were infused with a Planckian density of energy, the universe would have ripped itself apart moments after the Big Bang. But this hasn’t happened.
- Instead, cosmologists observe that space’s expansion is accelerating only slowly, indicating that the cosmological constant is small. Measurements in 1998 pegged its value as a million million million million million times lower than the Planck energy. Again, it seems all those enormous energy injections and extractions in the equation for the cosmological constant perfectly cancel out, leaving space eerily placid.
- In hindsight, the two naturalness problems seem more like symptoms of a deeper issue. “It’s useful to think about how these problems come about,” said Garcia Garcia in a Zoom call from Santa Barbara this winter. “The hierarchy problem and the cosmological constant problem are problems that arise in part because of the tools we’re using to try to answer questions — the way we’re trying to understand certain features of our universe.”
- 

# Le problème du lithium cosmique

*Sources*


- [Populating the periodic table: Nucleosynthesis of the elements](https://science.sciencemag.org/content/363/6426/474) - Johnson (2019)
- [The Primordial Lithium Problem](https://www.annualreviews.org/doi/10.1146/annurev-nucl-102010-130445) - Fields (2011)
- [The Cosmological Lithium Problem Revisited](https://arxiv.org/abs/1603.03864) - Bertulani et al (2016)
- [Etoile de population II](https://fr.wikipedia.org/wiki/%C3%89toile_de_population_II) - WIkipédia
- [Big-Bang Nucleosynthesis and the Baryon Density of the Universe](https://arxiv.org/pdf/astro-ph/9407006.pdf) - Copi et al (1995)
- [Big Bang Nucleosynthesis](https://pdg.lbl.gov/2019/reviews/rpp2019-rev-bbang-nucleosynthesis.pdf)  - Fields (2019)
- [Big Bang Nucleosynthesis (BBN)](https://www.astronomy.ohio-state.edu/weinberg.21/A5682/notes8.pdf) - Weinberg
- [The cosmological lithium problem](https://www.sciencedaily.com/releases/2018/10/181009102501.htm) - Science Daily (2018)

---

- La ***nucléosynthèse primordiale*** désigne le processus de formation des noyaux les plus légers dans les conditions ardentes de l'univers primordial, entre 1 s et 3 minutes (~180 s) environ après le Big Bang.
> À partir de 1 seconde après le Big Bang, la température du cosmos passe sous la barre des 10 milliards de degrés. Une séquence d'événements est alors initiée, qui mène à la synthèse de quelques éléments légers, comme le deutérium, l'hélium-3, l'hélium-4 et le lithium-7.
- La théorie de la nucléosynthèse primordiale standard (Big Bang Nucleosynthesis) repose sur le modèle standard de la physique des particules ainsi que du modèle standard de la cosmologie (ΛCDM), qui modélise un univershomogène et isotrope en expansion selon les règles de la relativité générale contenant de la matière noire et de l'énergie sombre.
- Dans la théorie de la nucléosynthèse primordiale standard, les abondances des éléments légers sont encapsulées dans un unique paramètre, la ***densité de baryons cosmique***, noté η (les baryons dans ce contexte sont les protons et les neutrons), qui est normalisée par rapport à la densité de photons du fond diffus cosmologique (baryon-photon ratio). Ce rapport est de l'ordre de 10^-9, ce qui veut dire que pour chaque baryon de l'univers, il y a environ 1 milliard de photons du CMB.
> La densité de photon du CMB est de 413 photons/cm3.
- Le ***problème du lithium cosmique*** désigne l'énorme différence entre nos théories et nos observations concernant l'abondance de lithium 7 dans l'univers.
> On observe 3 à 4 fois moins de lithium-7 dans l'univers que ce qui est prédit par la théorie. 
- Par contre, les abondances d'hélium et de deutérium sont reproduites avec beaucoup de succès
> Ces abondances constituent même l'un des quatre piliers observationnels du modèle du Big Bang chaud.
- Le problème du lithium est apparu lorsque les astronomes ont commencé à étudier les propriétés du rayonnement de fond diffus cosmologique à l'aide du satellite WMAP, et le désaccord entre théorie et observation est devenu de plus en plus plus important au fil des nouvelles observations (à l'époque de la première fournée de données de WMAP, il n'y avait qu'un facteur 2-3).

### Comment mesure t-on les abondances des éléments légers dans l'univers ?

- En analysant la carte du fond diffus cosmologique, on peut prédire la densité de baryons cosmiques avec une grande précision et ainsi tester la théorie de la nucléosynthèse primordiale.
> Les récentes données du satellite Planck donnent une prédiction de η = 6x10^-10, qui est en très bon accord avec les abondances observées de deutérium et d'hélium-4 de z=1000 à z=0.
- Les abondance en deutérium (un isotope de l'hydrogène constitué d'1 proton et 1 neutron) sont mesurées dans des nuages d'hydrogène très lointains (z~3) et très pauvres en éléments lourds qui sont observés sur la ligne de visée de quasars encore plus lointains. On ne peut pas mesurer les abondances en deutérium dans les étoiles, car il est entièrement détruit dans ces systèmes.
> À l'heure actuelle, ce sont les abondances qui sont (de loin) les mieux reproduites par la théorie.
- Les abondances en hélium-3 sont mesurées dans le milieu interstellaire dans la Voie Lactée, faute de pouvoir les mesurer dans des galaxies lointaines. Comme notre galaxie est riche en éléments lourds, on ne peut pas utiliser pour le moment les abondances en hélium-3 pour contraindre la période de nucléosynthèse primordiale.
- Les abondances en hélium-4 sont mesurées dans des régions de formation d'étoiles (régions HII) de galaxies voisines pauvres en éléments lourds (metal-poor)
- Les abondances en lithium sont principalement mesurées dans l'atmosphère (photosphère) de très vieilles étoiles (âgées de 11 à 13,5 milliards d'années) pauvres en éléments lourds présentes dans le halo stellaire de notre Galaxie. On connaît environ une centaine de ces étoiles dites de population II. Les abondances observées sont relativement faibles, notamment parce que le lithium de l'atmosphère de ces étoiles est en permanence emporté dans les profondeurs par les mouvements convectifs de leur enveloppe, où il est détruit par la chaleur intense.
> Ce qu'on mesure en pratique ce sont les raies d'absorption dans le spectre de ces étoiles qui correspondent à la signature du lithium

### Quelles sont les solutions possibles au problème du lithium cosmique ?

- Soit nos prédictions théoriques (cosmologie+physique des particules) sont correctes, mais ce sont les observations astrophysiques qui sont incomplètes
> Grâce au LSST, on aura bientôt accès à une population bien plus grande d'étoiles pauvres en éléments lourds dans des galaxies proches et lointaines qui permettra d'avoir des statistiques plus fiables sur les abondances de lithium.
- Il pourrait aussi exister des processus encore inconnus au niveau de la physique nucléaire qui pourrait altérer nos prédictions sur les abondances du lithium
> Ces processus pourraient en particulier amplifier la destruction du béryllium-7. On peut vérifier cette hypothèse à l'aide d'expériences de physique nucléaire comme celles menées par la collaboration n_TOF (neutron-Time Of Flight) au CERN depuis 2018.
- Il pourrait enfin exister des solutions au-delà du modèle standard (comme la supersymmétrie par exemple) qui pourrait impliquer de nouveaux processus au niveau cosmologique ou de la physique des particules
> Ces solutions sont testées dans les accélérateurs de particules comme le LHC et du côté des expériences de détection de matière noire.

# L'univers est-il isotrope ?

- [Probing cosmic isotropy with a new X-ray galaxy cluster sample through the LX–T scaling relation](https://www.aanda.org/articles/aa/full_html/2020/04/aa36602-19/aa36602-19.html) - Migkas et al (2020)
- [Observation d'une anisotropie de l'Univers !](https://www.ca-se-passe-la-haut.fr/2020/04/observation-dune-anisotropie-de-lunivers.html) - Ca se passe là-haut
- [The CMB Dipole: Eppur Si Muove](https://arxiv.org/abs/2111.12186) - Sullivan et Scott (2021)
- [Cosmic Microwave Background Dipole](https://astronomy.swin.edu.au/cosmos/c/Cosmic+Microwave+Background+Dipole) - SAO Encyclopedia of astronomy
- [The Motion of the Local Group with Respect to the 15,000 Kilometer per Second Abell Cluster Inertial Frame](https://adsabs.harvard.edu/full/1994ApJ...425..418L) - Lauer et Postman (1994)

---

- L'homogénéité (à grande échelle) et l'isotropie (statistiquement parlant) de l'univers est à la base du modèle standard de la cosmologie. Autrement dit, il n'y a pas d'observateur privilégié ou de direction privilégiée dans l'univers. Où que l'on soit dans l'univers, et quelque soit la direction où l'on regarde, on devrait observer la même chose à grande échelle. Cette idée constitue le ***principe cosmologique***.
- Lorsque l'on cartographie le fond diffus cosmologique, on remarque qu'il n'est pas directement isotrope. La plus grande anisotropie de température qu'il présente est un dipole (CMB dipole). L'amplitude de ce dipole (ΔT/T\~10-3) est plus grande que les autres fluctuations de température (ΔT/T\~10-5).
- On interprète généralement ce dipôle comme le résultat du mouvement du Groupe Local vers un ***grand attracteur*** dans le référentiel du CMB (modèle proposé par Lynden Bell et al en 1988), qui génère un ***effet Doppler***.
> * Notre Galaxie se déplace à environ 620 km/s par rapport au CMB dans la direction de la constellation du Centaure, où se trouve le grand attracteur
> * On sait aujourd'hui que ce grand attracteur se trouve au coeur du super-amas Laniakea
> * Le grand attracteur est difficile à observer dans le domaine du visible parce qu'il est situé directement derrière le plan Galactique.
- Lorsque ce mouvement global est corrigé, le CMB est remarquablement isotrope.
- The isotropy of the late Universe and consequently of the X-ray galaxy cluster scaling relations is an assumption greatly used in astronomy. However, within the last decade, many studies have reported deviations from isotropy when using various cosmological probes; a definitive conclusion has yet to be made. New, effective and independent methods to robustly test the cosmic isotropy are of crucial importance.
